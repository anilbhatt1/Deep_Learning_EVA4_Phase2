<!-- PROJECT SHIELDS -->
<!--
*** I'm using markdown "reference style" links for readability.
*** Reference links are enclosed in brackets [ ] instead of parentheses ( ).
*** See the bottom of this document for the declaration of the reference variables
*** for contributors-url, forks-url, etc. This is an optional, concise syntax you may use.
*** https://www.markdownguide.org/basic-syntax/#reference-style-links
-->
[![Mentor][mentor-shield]][mentor-url]
[![Issues][issues-shield]][issues-url]
[![MIT License][license-shield]][license-url]

# Human Pose Estimation(HPE)
________

# [Link to Web page(HPE Image and HPE Video)](https://neural-eyes.herokuapp.com/)

<!-- TABLE OF CONTENTS -->
## Table of Contents

* [Prerequisites](#prerequisites)
* [HPE Working](#gan-working)
* [Data Preparation](#Data-Preparation)
* [Colab Notebook References](#Colab-Notebook-References)
* [Model Weight references for future training](#model-weights)
* [License](#license)
* [Group Members](#group-members)
* [Mentor](#mentor)

## Prerequisites

* [Linux](https://www.tutorialspoint.com/ubuntu/index.htm)
* [Python 3.8](https://www.python.org/downloads/) or Above
* [Pytorch 1.5.1](https://pytorch.org/) ** Higher versions will cause storage issues while deploying to AWS Lambda 
* [torchvision 0.6.1](https://pytorch.org/docs/stable/torchvision/index.html) ** Higher versions will cause storage issues while deploying to AWS Lambda
* [AWS Account](https://aws.amazon.com/free/?all-free-tier.sort-by=item.additionalFields.SortRank&all-free-tier.sort-order=asc)
* [Serverless](https://www.serverless.com/) 
* [Google Colab](https://colab.research.google.com/)
* [Open-CV](https://pypi.org/project/opencv-python/)
* [Html](https://www.w3schools.com/html/)
* [Jquery](https://jquery.com/)

<!-- HPE Working -->
## HPE Working
- Human pose estimation (HPE) is the process of estimating the configuration of the body (pose) from a single, typically monocular, image. 
- It can be applied to many applications such as action/activity recognition, action detection, human tracking, in movies and animation, virtual reality, human-computer interaction, video surveillance, medical assistance, self-driving, sports motion analysis, etc.
- There are various methods through which HPE is done as shown below.

![HPE Catgories](https://github.com/anilbhatt1/Deep_Learning_EVA4_Phase2/blob/master/S5_Human_Pose_Estimation/Readme_Contents/HPE%20Method%20Categories.png)

- Here, we are implementing the HPE based on **Simple Baseline for HPE and tracking** paper. Paper can be referred here - https://arxiv.org/pdf/1804.06208.pdf
- Github reference for the same is https://github.com/microsoft/human-pose-estimation.pytorch/blob/master/README.md
- Openpose and Densepose are two other methods but these methods are complex compared to Simaple Baseline method that is followed here. **Simple Baseline for HPE and tracking** is aimed to ease the HPE problem. 
- This approach involves a few deconvolutional layers added on a backbone network, ResNet. This approach adds a few deconvolutional layers over the last convolution stage in the ResNet, called C5.
- By default, 3 Deconv layers with BN and ReLU as used. Each layer has 256 filters with 4x4 kernels. The stride is 2.
- A 1x1 convolutional layer is added at the last to generate predicted heatmaps for all k key points. MSE is used as the loss function.
- The targeted heatmap for joint k is generated by applying a 2D gaussian centered on the kth joint's ground truth location.
- As can be seen, this approach is based on combination of Discriminative, Detection, bottom-up and Multistage methods.
- 3D Keypoints are as shown in the skeleton below.

 ![Keypoints](https://github.com/anilbhatt1/Deep_Learning_EVA4_Phase2/blob/master/S5_Human_Pose_Estimation/Readme_Contents/Keyjoints.jpg)

- Architecture is as below.  
 ![HPE Architecture](https://github.com/anilbhatt1/Deep_Learning_EVA4_Phase2/blob/master/S5_Human_Pose_Estimation/Readme_Contents/HPE_DNN.jpg)

<!-- Data Preparation -->
## Data Preparation
- Input image used is as below

 ![Input Sample Image](https://github.com/anilbhatt1/Deep_Learning_EVA4_Phase2/blob/master/S5_Human_Pose_Estimation/Readme_Contents/Messi_Kick.jpg)

<!-- Colab Notebook References -->
## Colab Notebook References
- Colab notebook reference is as below. Pretrained weight is used. Refer model weight reference section listed below for the same.
https://github.com/anilbhatt1/Deep_Learning_EVA4_Phase2/blob/master/S5_Human_Pose_Estimation/EVA4P2_S5_HPE_V2.ipynb
- HPE generated is as below
![HPE Generated](https://github.com/anilbhatt1/Deep_Learning_EVA4_Phase2/blob/master/S5_Human_Pose_Estimation/Readme_Contents/Messi_Connected.jpg)

<!-- Model weight References -->
## Model Weight references for future training
- Refer below locations to download pretrained weights for future. ONNX quantized version and yaml file is also available.
https://drive.google.com/drive/folders/1OJeWfBSVtMVHPqdW0ytlZ3Fsveb8Lx-x?usp=sharing

<!-- LICENSE -->
## License

Distributed under the MIT License. See `LICENSE` for more information.

<!-- GROUP MEMBERS -->
## Group Members
  - [Gajanana Ganjigatti](https://github.com/gaju27) , [Gaju_on_LinkedIn](https://www.linkedin.com/in/gajanana-ganjigatti/)
  - [Anilkumar N Bhatt](https://github.com/anilbhatt1) , [Anil_on_LinkedIn](https://www.linkedin.com/in/anilkumar-n-bhatt/)
  - [Sridevi B](https://github.com/sridevibonthu) , [Sridevi_on_LinkedIn](https://www.linkedin.com/in/sridevi-bonthu/)
  - [SMAG TEAM](https://github.com/SMAGEVA4/session1/tree/master/Session1) :performing_arts: team github account

<!-- MENTOR -->
## Mentor

* [Rohan Shravan](https://www.linkedin.com/in/rohanshravan/) , [The School of A.I.](https://theschoolof.ai/)

<!-- MARKDOWN LINKS & IMAGES -->
<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->
[mentor-shield]: https://img.shields.io/badge/Mentor-mentor-yellowgreen
[mentor-url]: https://www.linkedin.com/in/rohanshravan/
[forks-shield]: https://img.shields.io/github/forks/othneildrew/Best-README-Template.svg?style=flat-square
[forks-url]: https://github.com/othneildrew/Best-README-Template/network/members
[stars-shield]: https://img.shields.io/github/stars/othneildrew/Best-README-Template.svg?style=flat-square
[stars-url]: https://github.com/othneildrew/Best-README-Template/stargazers
[issues-shield]: https://img.shields.io/github/issues/othneildrew/Best-README-Template.svg?style=flat-square
[issues-url]: https://github.com/othneildrew/Best-README-Template/issues
[license-shield]: https://img.shields.io/github/license/othneildrew/Best-README-Template.svg?style=flat-square
[license-url]: https://github.com/anilbhatt1/Deep_Learning_EVA4_Phase2/blob/master/LICENSE.txt
[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=flat-square&logo=linkedin&colorB=555



