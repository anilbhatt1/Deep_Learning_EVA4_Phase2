{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "EVA4P2_S12_ImageCaption_V2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anilbhatt1/Deep_Learning_EVA4_Phase2/blob/master/EVA4P2_S12_ImageCaption_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlGk2nuq4ZCr",
        "outputId": "3b80189a-70c2-4409-9f2f-5e396862e7ee"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon May 17 14:55:44 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8    33W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiAN4b6Qtrne"
      },
      "source": [
        "### Reference : https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning\n",
        "This notebook is based on above reference. Code below is heavily commented at each line. This code is built with a focus on deploying to cloud thereby addressing space limitations of models.Encoder is using pre-trained RESNET-18."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95gzlYvniAPz",
        "outputId": "4ab0b036-f085-4a05-d8ad-7fc7b0d89b7e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_5J-2DRZ8Co",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e4949f0-ef2b-4df2-f9a3-da032f89faf3"
      },
      "source": [
        "!pip install torch==1.5.0+cu92 torchvision==0.6.0+cu92 torchtext==0.6.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.0+cu92\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu92/torch-1.5.0%2Bcu92-cp37-cp37m-linux_x86_64.whl (603.7MB)\n",
            "\u001b[K     |████████████████████████████████| 603.7MB 30kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.0+cu92\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu92/torchvision-0.6.0%2Bcu92-cp37-cp37m-linux_x86_64.whl (6.5MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5MB 114kB/s \n",
            "\u001b[?25hCollecting torchtext==0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/17/e7c588245aece7aa93f360894179374830daf60d7ed0bbb59332de3b3b61/torchtext-0.6.0-py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.5.0+cu92) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5.0+cu92) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.6.0+cu92) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 17.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Installing collected packages: torch, torchvision, sentencepiece, torchtext\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "  Found existing installation: torchvision 0.9.1+cu101\n",
            "    Uninstalling torchvision-0.9.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.1+cu101\n",
            "  Found existing installation: torchtext 0.9.1\n",
            "    Uninstalling torchtext-0.9.1:\n",
            "      Successfully uninstalled torchtext-0.9.1\n",
            "Successfully installed sentencepiece-0.1.95 torch-1.5.0+cu92 torchtext-0.6.0 torchvision-0.6.0+cu92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_3Y4xuqZ_8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99f799c2-2caf-4398-f725-d1cad244b105"
      },
      "source": [
        "pip install scipy==1.1.0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scipy==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/de/0c22c6754370ba6b1fa8e53bd6e514d4a41a181125d405a501c215cbdbd6/scipy-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (31.2MB)\n",
            "\u001b[K     |████████████████████████████████| 31.2MB 100kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.1.0) (1.19.5)\n",
            "\u001b[31mERROR: pymc3 3.11.2 has requirement scipy>=1.2.0, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement scipy>=1.2.0, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "Successfully installed scipy-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teyHqGVbqdfJ",
        "outputId": "a61a4c35-d18f-4e98-85b1-14c7f4b84a7f"
      },
      "source": [
        "import zipfile\n",
        "from zipfile import ZipFile\n",
        "from time import time\n",
        "from datetime import datetime \n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torch.utils.data\n",
        "import torchvision.utils as utils\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import numpy as np\n",
        "print('Pytorch version:', torch.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pytorch version: 1.5.0+cu92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUny_bqyoWiw"
      },
      "source": [
        "!unzip -q '/content/gdrive/MyDrive/EVA4P2_S12_ImageCaptioning/Flickr8k_Dataset.zip'\n",
        "!unzip -q '/content/gdrive/MyDrive/EVA4P2_S12_ImageCaptioning/Flickr8k_text.zip'\n",
        "!unzip -q '/content/gdrive/MyDrive/EVA4P2_S12_ImageCaptioning/caption_datasets.zip'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-PwtuyEy857"
      },
      "source": [
        "import h5py\n",
        "import json\n",
        "from scipy.misc import imread, imresize\n",
        "from collections import Counter\n",
        "from random import seed, choice, sample\n",
        "\n",
        "def create_input_files(dataset, karpathy_json_path, image_folder, captions_per_image, min_word_freq, output_folder,\n",
        "                       max_len=100):\n",
        "    \"\"\"\n",
        "    Creates input files for training, validation, and test data.\n",
        "    :param dataset: name of dataset, one of 'coco', 'flickr8k', 'flickr30k'\n",
        "    :param karpathy_json_path: path of Karpathy JSON file with splits and captions\n",
        "    :param image_folder: folder with downloaded images\n",
        "    :param captions_per_image: number of captions to sample per image\n",
        "    :param min_word_freq: words occuring less frequently than this threshold are binned as <unk>s\n",
        "    :param output_folder: folder to save files\n",
        "    :param max_len: don't sample captions longer than this length\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    Sample format of flickr8k json file is as follows:\n",
        "    {\"images\": [{\"sentids\": [0, 1, 2, 3, 4], \n",
        "     \"imgid\": 0, \"sentences\": [{\"tokens\": [\"a\", \"black\", \"dog\", \"is\", \"running\", \"after\", \"a\", \"white\", \"dog\", \"in\", \"the\", \"snow\"], \"raw\": \"A black dog is running after a white dog in the snow .\", \"imgid\": 0, \"sentid\": 0}, \n",
        "                               {\"tokens\": [\"black\", \"dog\", \"chasing\", \"brown\", \"dog\", \"through\", \"snow\"], \"raw\": \"Black dog chasing brown dog through snow\", \"imgid\": 0, \"sentid\": 1}, \n",
        "\t\t\t\t\t\t                   {\"tokens\": [\"two\", \"dogs\", \"chase\", \"each\", \"other\", \"across\", \"the\", \"snowy\", \"ground\"], \"raw\": \"Two dogs chase each other across the snowy ground .\", \"imgid\": 0, \"sentid\": 2}, \n",
        "\t\t\t\t\t\t                   {\"tokens\": [\"two\", \"dogs\", \"play\", \"together\", \"in\", \"the\", \"snow\"], \"raw\": \"Two dogs play together in the snow .\", \"imgid\": 0, \"sentid\": 3}, \n",
        "\t\t\t\t\t\t                   {\"tokens\": [\"two\", \"dogs\", \"running\", \"through\", \"a\", \"low\", \"lying\", \"body\", \"of\", \"water\"], \"raw\": \"Two dogs running through a low lying body of water .\", \"imgid\": 0, \"sentid\": 4}], \n",
        "\t\t\t\t\t\t                    \"split\": \"train\", \"filename\": \"2513260012_03d33305cf.jpg\"}, \n",
        "                {\"sentids\": [5, 6, 7, 8, 9], \n",
        "     \"imgid\": 1, \"sentences\": [{\"tokens\": [\"a\", \"little\", \"baby\", \"plays\", \"croquet\"], \"raw\": \"A little baby plays croquet .\", \"imgid\": 1, \"sentid\": 5},\n",
        "                               {\"tokens\": [\"a\", \"little\", \"girl\", \"plays\", \"croquet\", \"next\", \"to\", \"a\", \"truck\"], \"raw\": \"A little girl plays croquet next to a truck .\", \"imgid\": 1, \"sentid\": 6},\n",
        "                               {\"tokens\": [\"the\", \"child\", \"is\", \"playing\", \"croquette\", \"by\", \"the\", \"truck\"], \"raw\": \"The child is playing croquette by the truck .\", \"imgid\": 1, \"sentid\": 7}, \n",
        "                               {\"tokens\": [\"the\", \"kid\", \"is\", \"in\", \"front\", \"of\", \"a\", \"car\", \"with\", \"a\", \"put\", \"and\", \"a\", \"ball\"], \"raw\": \"The kid is in front of a car with a put and a ball .\", \"imgid\": 1, \"sentid\": 8}, \n",
        "                               {\"tokens\": [\"the\", \"little\", \"boy\", \"is\", \"playing\", \"with\", \"a\", \"croquet\", \"hammer\", \"and\", \"ball\", \"beside\", \"the\", \"car\"], \"raw\": \"The little boy is playing with a croquet hammer and ball beside the car .\", \"imgid\": 1, \"sentid\": 9}],\n",
        "                                \"split\": \"train\", \"filename\": \"2903617548_d3e38d7f88.jpg\"}\t\t\t\n",
        "    \"\"\"                      \n",
        "\n",
        "    assert dataset in {'coco', 'flickr8k', 'flickr30k'}\n",
        "\n",
        "    # Read Karpathy JSON\n",
        "    with open(karpathy_json_path, 'r') as j:\n",
        "        data = json.load(j)\n",
        "\n",
        "    # Read image paths and captions for each image\n",
        "    train_image_paths = []\n",
        "    train_image_captions = []\n",
        "    val_image_paths = []\n",
        "    val_image_captions = []\n",
        "    test_image_paths = []\n",
        "    test_image_captions = []\n",
        "    word_freq = Counter()   # list1 = ['x','y','z','x','x','x','y', 'z']. The output if counter is used on list1 should be something like :\n",
        "                            # Counter({'x': 4, 'y': 2, 'z': 2})\n",
        "\n",
        "    for img in data['images']:\n",
        "        captions = []\n",
        "        for c in img['sentences']:\n",
        "            # Update word frequency\n",
        "            word_freq.update(c['tokens'])\n",
        "            if len(c['tokens']) <= max_len:\n",
        "                captions.append(c['tokens'])      \n",
        "\n",
        "        if len(captions) == 0:\n",
        "            continue\n",
        "\n",
        "        path = os.path.join(image_folder, img['filepath'], img['filename']) if dataset == 'coco' else os.path.join(\n",
        "            image_folder, img['filename'])\n",
        "        \n",
        "        if img['split'] in {'train', 'restval'}:\n",
        "            train_image_paths.append(path)\n",
        "            train_image_captions.append(captions)\n",
        "        elif img['split'] in {'val'}:\n",
        "            val_image_paths.append(path)\n",
        "            val_image_captions.append(captions)\n",
        "        elif img['split'] in {'test'}:\n",
        "            test_image_paths.append(path)\n",
        "            test_image_captions.append(captions)\n",
        "\n",
        "    print ('# of train_images, val_images, test_images : ', len(train_image_paths), len(val_image_paths), len(test_image_paths)) \n",
        "    # Sanity check\n",
        "    assert len(train_image_paths) == len(train_image_captions)\n",
        "    assert len(val_image_paths) == len(val_image_captions)\n",
        "    assert len(test_image_paths) == len(test_image_captions)\n",
        "\n",
        "    # Create word map\n",
        "    words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq] # words occuring less frequently than min_word_freq are binned as <unk>s\n",
        "    word_map = {k: v + 1 for v, k in enumerate(words)} # v+1 to reserve first index 0 for <pad>. word_map is a dict like {<pad>:0, 'a':1, 'an':2,..}\n",
        "    word_map['<unk>'] = len(word_map) + 1\n",
        "    word_map['<start>'] = len(word_map) + 1\n",
        "    word_map['<end>'] = len(word_map) + 1\n",
        "    word_map['<pad>'] = 0\n",
        "\n",
        "    # Create a base/root name for all output files\n",
        "    base_filename = dataset + '_' + str(captions_per_image) + '_cap_per_img_' + str(min_word_freq) + '_min_word_freq'\n",
        "    print('base_filename : ', base_filename)\n",
        "\n",
        "    # Save word map to a JSON\n",
        "    with open(os.path.join(output_folder, 'WORDMAP_' + base_filename + '.json'), 'w') as j:\n",
        "        json.dump(word_map, j)\n",
        "\n",
        "    \n",
        "    # Sample captions for each image, save images to HDF5 file, and captions and their lengths to JSON files\n",
        "    seed(123)\n",
        "    for impaths, imcaps, split in [(train_image_paths, train_image_captions, 'TRAIN'),\n",
        "                                   (val_image_paths, val_image_captions, 'VAL'),\n",
        "                                   (test_image_paths, test_image_captions, 'TEST')]:\n",
        "\n",
        "        print(' *** Split Change *** :', split)\n",
        "        cnt = 0\n",
        "        with h5py.File(os.path.join(output_folder, split + '_IMAGES_' + base_filename + '.hdf5'), 'a') as h:\n",
        "            # Make a note of the number of captions we are sampling per image\n",
        "            h.attrs['captions_per_image'] = captions_per_image\n",
        "\n",
        "            # Create dataset inside HDF5 file to store images\n",
        "            images = h.create_dataset('images', (len(impaths), 3, 256, 256), dtype='uint8')\n",
        "\n",
        "            print(\"\\nReading %s images and captions, storing to file...\\n\" % split)\n",
        "\n",
        "            enc_captions = []\n",
        "            caplens = []\n",
        "            \n",
        "            for i, path in enumerate(tqdm(impaths)):\n",
        "                cnt +=1 \n",
        "                # Sample captions\n",
        "                if len(imcaps[i]) < captions_per_image:\n",
        "     \n",
        "                    captions = imcaps[i] + [choice(imcaps[i]) for _ in range(captions_per_image - len(imcaps[i]))]  \n",
        "                    # choice() returns a random item. mylist = [\"apple\", \"banana\", \"cherry\"], print(choice(mylist)) --> banana\n",
        "                    # We are filling the gap between captions_per_image and length of caption by available words randomly chosen from that specific caption itself\n",
        "                    # Let us say len(imcaps[i]) = 3 & captions_per_image = 5, then 4th & 5th captions will be generated by above logic\n",
        "                else:                    \n",
        "                    captions = sample(imcaps[i], k=captions_per_image) \n",
        "                    # Prints list of random items of given length. list1 = [1, 2, 3, 4, 5] , print(sample(list1,3))  --> [2,3,5]\n",
        "                    # If len(imcaps[i]) = 5 and captions_per_image = 5, then all 5 captions will be written\n",
        "                    # If len(imcaps[i]) = 7 and captions_per_image = 5, then 5 captions out of 7 will be randomly chosen via random() and written\n",
        "\n",
        "                # Sanity check\n",
        "                assert len(captions) == captions_per_image\n",
        "\n",
        "                # Read images\n",
        "                img = imread(impaths[i])\n",
        "                if len(img.shape) == 2:           # If gray scale, add 1 more axis to bring to [256, 256, 3] format\n",
        "                    img = img[:, :, np.newaxis]   # An empty axis created on 3rd dimension i.e. axis = 2 (0 & 1 are existing axes)\n",
        "                    img = np.concatenate([img, img, img], axis=2)  # Filling values of 0 & 1 axis (2d frame), three times to make it 3D\n",
        "                                                                   # eg: (256, 256) is filled 3 times to give RGB i.e. (256, 256, 3) \n",
        "                img = imresize(img, (256, 256))   # eg: (128, 128, 3) -> (256, 256, 3)\n",
        "                img = img.transpose(2, 0, 1)      # changes (256,256,3) to (3, 256, 256)\n",
        "                assert img.shape == (3, 256, 256)\n",
        "                assert np.max(img) <= 255\n",
        "\n",
        "                # Save image to HDF5 file\n",
        "                images[i] = img\n",
        "\n",
        "                for j, c in enumerate(captions):\n",
        "                    # Encode captions\n",
        "                    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in c] + [\n",
        "                        word_map['<end>']] + [word_map['<pad>']] * (max_len - len(c))\n",
        "                    # Create encoding for each caption. Use index of <unk> if a particular word from caption is not found in word_map.\n",
        "                    # Pad the vacant space with index belonging to <pad>\n",
        "                    # If max_len we are passing as argument is 50 & caption length from dataset is 14, then 50-14 = 36 will be padded as <pad>\n",
        "\n",
        "                    # Find caption lengths\n",
        "                    c_len = len(c) + 2   # Plus 2 as we are adding <start> and <end> indexes\n",
        "\n",
        "                    enc_captions.append(enc_c)\n",
        "                    caplens.append(c_len)\n",
        "\n",
        "            # Sanity check\n",
        "            assert images.shape[0] * captions_per_image == len(enc_captions) == len(caplens)\n",
        "            # Save encoded captions and their lengths to JSON files\n",
        "            with open(os.path.join(output_folder, split + '_CAPTIONS_' + base_filename + '.json'), 'w') as j:\n",
        "                json.dump(enc_captions, j)\n",
        "\n",
        "            with open(os.path.join(output_folder, split + '_CAPLENS_' + base_filename + '.json'), 'w') as j:\n",
        "                json.dump(caplens, j)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-BG7D_2zpl_",
        "outputId": "7d4957ef-b433-4a4a-beac-4b0ef24f143d"
      },
      "source": [
        "#### MANUALLY Create a folder '/content/data_output' before executing this cell\n",
        "\n",
        "create_input_files(dataset='flickr8k',\n",
        "                       karpathy_json_path='/content/dataset_flickr8k.json',\n",
        "                       image_folder='/content/Flicker8k_Dataset/',\n",
        "                       captions_per_image=5,\n",
        "                       min_word_freq=5,\n",
        "                       output_folder='/content/data_output/',\n",
        "                       max_len=50)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/6000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:140: DeprecationWarning:     `imread` is deprecated!\n",
            "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "    Use ``imageio.imread`` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:145: DeprecationWarning:     `imresize` is deprecated!\n",
            "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "    Use ``skimage.transform.resize`` instead.\n",
            "  0%|          | 7/6000 [00:00<01:29, 66.92it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "# of train_images, val_images, test_images :  6000 1000 1000\n",
            "base_filename :  flickr8k_5_cap_per_img_5_min_word_freq\n",
            " *** Split Change *** : TRAIN\n",
            "\n",
            "Reading TRAIN images and captions, storing to file...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6000/6000 [00:54<00:00, 110.65it/s]\n",
            "  1%|          | 11/1000 [00:00<00:09, 106.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " *** Split Change *** : VAL\n",
            "\n",
            "Reading VAL images and captions, storing to file...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:09<00:00, 102.88it/s]\n",
            "  1%|          | 12/1000 [00:00<00:08, 116.86it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " *** Split Change *** : TEST\n",
            "\n",
            "Reading TEST images and captions, storing to file...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:09<00:00, 109.77it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9EKZp1E-4D9"
      },
      "source": [
        "class CaptionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_folder, data_name, split, transform=None):\n",
        "        \"\"\"\n",
        "        :param data_folder: folder where data files are stored\n",
        "        :param data_name: base name of processed datasets\n",
        "        :param split: split, one of 'TRAIN', 'VAL', or 'TEST'\n",
        "        :param transform: image transform pipeline\n",
        "        \"\"\"\n",
        "        self.split = split\n",
        "        assert self.split in {'TRAIN', 'VAL', 'TEST'}\n",
        "\n",
        "        # Open hdf5 file where images are stored\n",
        "        self.h = h5py.File(os.path.join(data_folder, self.split + '_IMAGES_' + data_name + '.hdf5'), 'r')\n",
        "        self.imgs = self.h['images']\n",
        "\n",
        "        # Captions per image\n",
        "        self.cpi = self.h.attrs['captions_per_image']  ## If there are 5 different captions for an image, then cpi = 5\n",
        "\n",
        "        # Load encoded captions (completely into memory)\n",
        "        with open(os.path.join(data_folder, self.split + '_CAPTIONS_' + data_name + '.json'), 'r') as j:\n",
        "            self.captions = json.load(j)\n",
        "\n",
        "        # Load caption lengths (completely into memory)\n",
        "        with open(os.path.join(data_folder, self.split + '_CAPLENS_' + data_name + '.json'), 'r') as j:\n",
        "            self.caplens = json.load(j)\n",
        "\n",
        "        # PyTorch transformation pipeline for the image (normalizing, etc.)\n",
        "        self.transform = transform\n",
        "\n",
        "        # Total number of datapoints\n",
        "        self.dataset_size = len(self.captions)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # Remember, the Nth caption corresponds to the (N // captions_per_image)th image\n",
        "        # i indicates captions..For every image there are 5 captions (cpi)...so total images = i / self.cpi\n",
        "        img = torch.FloatTensor(self.imgs[i // self.cpi] / 255.)\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        caption = torch.LongTensor(self.captions[i])\n",
        "\n",
        "        caplen = torch.LongTensor([self.caplens[i]])\n",
        "\n",
        "        if self.split is 'TRAIN':\n",
        "            return img, caption, caplen\n",
        "        else:\n",
        "            # For validation of testing, also return all 'captions_per_image' captions to find BLEU-4 score\n",
        "            # eg: all_captions belonging to 2nd image.\n",
        "            # Here i will be 5,6,7,8,9 (assuming index starts at 0). Let us take i = 8\n",
        "            # so [((8//5)*5):(((8//5)*5) + 5)]  = [(1*5):((1*5)+5)] = [5:10] ie return all captions with idx 5,6,7,8,9 for 2nd image\n",
        "            all_captions = torch.LongTensor(\n",
        "                self.captions[((i // self.cpi) * self.cpi):(((i // self.cpi) * self.cpi) + self.cpi)])\n",
        "            return img, caption, caplen, all_captions\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGM3CRH_-7uW"
      },
      "source": [
        "def init_embedding(embeddings):\n",
        "    \"\"\"\n",
        "    Fills embedding tensor with values from the uniform distribution.\n",
        "    :param embeddings: embedding tensor\n",
        "    \"\"\"\n",
        "    bias = np.sqrt(3.0 / embeddings.size(1))\n",
        "    torch.nn.init.uniform_(embeddings, -bias, bias)\n",
        "\n",
        "'''\n",
        "def load_embeddings(emb_file, word_map):\n",
        "    \"\"\"\n",
        "    Creates an embedding tensor for the specified word map, for loading into the model.\n",
        "    :param emb_file: file containing embeddings (stored in GloVe format)\n",
        "    :param word_map: word map (word map is what we get based on captions dataset)\n",
        "    :return: embeddings in the same order as the words in the word map, dimension of embeddings\n",
        "    \"\"\"\n",
        "\n",
        "    # Find embedding dimension\n",
        "    with open(emb_file, 'r') as f:\n",
        "        emb_dim = len(f.readline().split(' ')) - 1\n",
        "\n",
        "    vocab = set(word_map.keys())\n",
        "\n",
        "    # Create tensor to hold embeddings, initialize\n",
        "    embeddings = torch.FloatTensor(len(vocab), emb_dim)\n",
        "    init_embedding(embeddings)\n",
        "\n",
        "    # Read embedding file\n",
        "    print(\"\\nLoading embeddings...\")\n",
        "    for line in open(emb_file, 'r'):\n",
        "        line = line.split(' ')\n",
        "\n",
        "        emb_word = line[0]\n",
        "        embedding = list(map(lambda t: float(t), filter(lambda n: n and not n.isspace(), line[1:])))\n",
        "\n",
        "        # Ignore word if not in train_vocab\n",
        "        if emb_word not in vocab:\n",
        "            continue\n",
        "\n",
        "        embeddings[word_map[emb_word]] = torch.FloatTensor(embedding)\n",
        "\n",
        "    return embeddings, emb_dim\n",
        "'''\n",
        "\n",
        "def clip_gradient(optimizer, grad_clip):\n",
        "    \"\"\"\n",
        "    Clips gradients computed during backpropagation to avoid explosion of gradients.\n",
        "    :param optimizer: optimizer with the gradients to be clipped\n",
        "    :param grad_clip: clip value\n",
        "    \"\"\"\n",
        "    for group in optimizer.param_groups:\n",
        "        for param in group['params']:\n",
        "            if param.grad is not None:\n",
        "                param.grad.data.clamp_(-grad_clip, grad_clip)\n",
        "\n",
        "\n",
        "def save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "                    bleu4, is_best):\n",
        "    \"\"\"\n",
        "    Saves model checkpoint.\n",
        "    :param data_name: base name of processed dataset\n",
        "    :param epoch: epoch number\n",
        "    :param epochs_since_improvement: number of epochs since last improvement in BLEU-4 score\n",
        "    :param encoder: encoder model\n",
        "    :param decoder: decoder model\n",
        "    :param encoder_optimizer: optimizer to update encoder's weights, if fine-tuning\n",
        "    :param decoder_optimizer: optimizer to update decoder's weights\n",
        "    :param bleu4: validation BLEU-4 score for this epoch\n",
        "    :param is_best: is this checkpoint the best so far?\n",
        "    \"\"\"\n",
        "    state = {'epoch': epoch,\n",
        "             'epochs_since_improvement': epochs_since_improvement,\n",
        "             'bleu-4': bleu4,\n",
        "             'encoder': encoder,\n",
        "             'decoder': decoder,\n",
        "             'encoder_optimizer': encoder_optimizer,\n",
        "             'decoder_optimizer': decoder_optimizer}\n",
        "    filename    = 'checkpoint_' + data_name + '.pth.tar'\n",
        "    #pt_filename = 'checkpoint_' + data_name + '.pt'\n",
        "    \n",
        "    '''\n",
        "    In PyTorch, the learnable parameters (i.e. weights and biases) of a torch.nn.Module model are contained in the model’s parameters (accessed with model.parameters()). \n",
        "    A state_dict is simply a Python dictionary object that maps each layer to its parameter tensor.\n",
        "    '''\n",
        "    torch.save(state, filename)       \n",
        "    #torch.save({\n",
        "    #            \"encoder\": encoder.state_dict(),\n",
        "    #            \"decoder\": decoder.state_dict()\n",
        "    #           }, pt_filename)\n",
        "    \n",
        "    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n",
        "    print('epoch :', epoch, 'Checkpoint saved :', filename)\n",
        "    if is_best:\n",
        "        torch.save(state, 'BEST_' + filename)\n",
        "        #torch.save({\n",
        "        #        \"encoder\": encoder.state_dict(),\n",
        "        #        \"decoder\": decoder.state_dict()\n",
        "        #           }, 'BEST_' + pt_filename)  \n",
        "        print('Best checkpoint encountered & saved in epoch :', epoch)      \n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Keeps track of most recent, average, sum, and count of a metric. This class is used for below objects while training & validating:\n",
        "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
        "    data_time = AverageMeter()  # data loading time\n",
        "    losses = AverageMeter()  # loss (per word decoded)\n",
        "    top5accs = AverageMeter()  # top5 accuracy\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, shrink_factor):\n",
        "    \"\"\"\n",
        "    Shrinks learning rate by a specified factor.\n",
        "    :param optimizer: optimizer whose learning rate must be shrunk.\n",
        "    :param shrink_factor: factor in interval (0, 1) to multiply learning rate with.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nDECAYING learning rate.\")\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
        "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n",
        "\n",
        "\n",
        "def accuracy(scores, targets, k):\n",
        "    \"\"\"\n",
        "    Computes top-k accuracy, from predicted and true labels.\n",
        "    :param scores: scores from the model\n",
        "    :param targets: true labels\n",
        "    :param k: k in top-k accuracy\n",
        "    :return: top-k accuracy\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size = targets.size(0)\n",
        "    _, ind = scores.topk(k, 1, True, True)\n",
        "    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n",
        "    correct_total = correct.view(-1).float().sum()  # 0D tensor\n",
        "    return correct_total.item() * (100.0 / batch_size)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWm4jqGx9G9y"
      },
      "source": [
        "#### Necessary imports for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-397Jrl-No0"
      },
      "source": [
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "#from models import Encoder, DecoderWithAttention\n",
        "#from datasets import *\n",
        "#from utils import *\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22HMt_r9THF6"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoded_image_size=14):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_image_size = encoded_image_size\n",
        "\n",
        "        resnet = torchvision.models.resnet18(pretrained=True)  # pretrained ImageNet ResNet-18\n",
        "\n",
        "        # Remove linear and pool layers (since we're not doing classification)\n",
        "        modules = list(resnet.children())[:-2]\n",
        "                 \n",
        "        '''\n",
        "        nn.Sequential is a construction which is used when you want to run certain layers sequentially.\n",
        "        Let us say modules = [layer1,layer2,layer3]\n",
        "        x = ... # our input\n",
        "\n",
        "        x = layer1(x)\n",
        "        x = layer2(x)\n",
        "        x = layer3(x)\n",
        "        '''\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "\n",
        "        # Resize image to fixed size to allow input images of variable size\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
        "\n",
        "        self.fine_tune()\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n",
        "        :return: encoded images\n",
        "        \"\"\"\n",
        "        out = self.resnet(images)  # (batch_size, 2048, image_size/32, image_size/32)\n",
        "        out = self.adaptive_pool(out)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
        "        out = out.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 2048) -> (32, 14, 14, 2048)\n",
        "        #print('Encoder Ouput size:', out.size())\n",
        "        return out\n",
        "\n",
        "    def fine_tune(self, fine_tune=True):\n",
        "        \"\"\"\n",
        "        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n",
        "        :param fine_tune: Allow?\n",
        "        \"\"\"\n",
        "        for p in self.resnet.parameters():\n",
        "            p.requires_grad = False\n",
        "        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n",
        "        for c in list(self.resnet.children())[5:]:\n",
        "            for p in c.parameters():\n",
        "                p.requires_grad = fine_tune"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssYVnAd6TR4v"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention Network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
        "        \"\"\"\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param attention_dim: size of the attention network\n",
        "        \"\"\"\n",
        "        super(Attention, self).__init__()\n",
        "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
        "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
        "        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
        "\n",
        "    def forward(self, encoder_out, decoder_hidden):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim) [32, 196, 2048]\n",
        "        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim) [32, 512]\n",
        "        :return: attention weighted encoding, weights\n",
        "        \"\"\"\n",
        "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim) \n",
        "                                              # [32, 196, 2048] -> [32, 196, 512] makes encoder_out compatible with attention_dim via nn.linear\n",
        "                                              # for t = 17, batch_size_t = 3 in example, [3, 196, 2048] -> [3, 196, 512]\n",
        "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
        "                                                 # for t = 17, batch_size_t = 3 in example, [3, 512] -> [3, 512] as both attention_dim & decoder_dim = 512\n",
        "        att_temp0 = self.relu(att1 + att2.unsqueeze(1))         \n",
        "\n",
        "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
        "        # for t = 17,  batch_size_t = 3 in example,\n",
        "        #   att2.unsqueeze(1)                                             --> Adding dimension to make [3, 512] --> [3, 1, 512]\n",
        "        #   self.relu(att1 + att2.unsqueeze(1))                           --> [3, 196, 512] + [3, 1, 512] -> [3, 196, 512]\n",
        "        #   self.full_att(self.relu(att1 + att2.unsqueeze(1)))            --> [3, 196, 512] -> [3, 196, 1]\n",
        "        #   self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2) --> Removing dim =2 to give [3, 196]\n",
        "        \n",
        "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
        "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
        "        # for t = 17,  batch_size_t = 3 in example,\n",
        "        #    alpha = self.softmax(att)                       --> [3, 196]. These are weights to be multiplied with encodings to give attention.\n",
        "        #    alpha.unsqueeze(2)                              --> [3, 196, 1]. Adding dimension on dim =2 to enable multiplication with encodings.\n",
        "        #    encoder_out                                     --> [3, 196, 2048]\n",
        "        #    (encoder_out * alpha.unsqueeze(2))              --> [3, 196, 2048]. Multipled alpha. Equilant to multiplying a scalar weight.\n",
        "        #    (encoder_out * alpha.unsqueeze(2)).sum(dim=1)   --> Sums up 196 pixels in dim = 1 to give [3, 2048] \n",
        "\n",
        "        return attention_weighted_encoding, alpha"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a-8ok8qTZAG"
      },
      "source": [
        "class DecoderWithAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n",
        "        \"\"\"\n",
        "        :param attention_dim: size of attention network\n",
        "        :param embed_dim: embedding size\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param vocab_size: size of vocabulary\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param dropout: dropout\n",
        "        \"\"\"\n",
        "        super(DecoderWithAttention, self).__init__()\n",
        "\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.attention_dim = attention_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
        "        self.dropout = nn.Dropout(p=self.dropout)\n",
        "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
        "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
        "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
        "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
        "        self.init_weights()  # initialize some layers with the uniform distribution\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
        "        \"\"\"\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def load_pretrained_embeddings(self, embeddings):\n",
        "        \"\"\"\n",
        "        Loads embedding layer with pre-trained embeddings.\n",
        "        :param embeddings: pre-trained embeddings\n",
        "        \"\"\"\n",
        "        self.embedding.weight = nn.Parameter(embeddings)\n",
        "\n",
        "    def fine_tune_embeddings(self, fine_tune=True):\n",
        "        \"\"\"\n",
        "        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n",
        "        :param fine_tune: Allow?\n",
        "        \"\"\"\n",
        "        for p in self.embedding.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "\n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        \"\"\"\n",
        "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :return: hidden state, cell state\n",
        "        \"\"\"\n",
        "        mean_encoder_out = encoder_out.mean(dim=1) # will take mean of [32, 196, 2048] against dim=1 giving mean_encoder_out of size [32, 2048]\n",
        "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
        "        # An example how nn.linear works\n",
        "        # >>> m = nn.Linear(20, 30) --> Here self.init_h = nn.Linear(encoder_dim, decoder_dim) where encoder_dim = 2048 & decoder_dim = 512\n",
        "        # >>> input = torch.randn(128, 20)\n",
        "        # >>> output = m(input)     --> Here h = self.init_h(mean_encoder_out) --> we will get h of size [32, 512] i.e. 2048 converted to 512 for batch-size of 32 \n",
        "        # >>> print(output.size())\n",
        "        # torch.Size([128, 30])\n",
        "\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n",
        "        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
        "        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
        "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
        "        \"\"\"\n",
        "        batch_size = encoder_out.size(0)  # torch.Size([32, 14, 14, 2048])...batch_size 32\n",
        "        encoder_dim = encoder_out.size(-1) # encoder_dim = 2048..last dimension of encoder_out\n",
        "        vocab_size = self.vocab_size\n",
        "       \n",
        "        # Flatten image\n",
        "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim) -> (32, 196, 2048)\n",
        "        # enoder_out size is [32, 14, 14, 2048]. While using encoder_out.view, we kept batch_size =32 & encoder_dim = 2048 as such and\n",
        "        # gave -1 inplace of 14, 14. So pytorch converts this -1 to 14*14 = 196 which is num_pixels\n",
        "        # We are converting encoder output to this format because we need to know which pixel/s out of 196 to focus using attention mechanism\n",
        "        num_pixels = encoder_out.size(1) # 196 pixels (14*14)\n",
        "\n",
        "        # Sort input data by decreasing lengths; why? apparent below\n",
        "        # torch.sort will give 2 outputs \n",
        "        # A namedtuple of (values, indices) is returned, where the values are the sorted values and indices are the indices of the elements \n",
        "        # in the original input tensor. Here values -> caption_lengths, indices -> sort_ind\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True) # caption_lengths.size() -> 32 , sort_ind.size() -> 32\n",
        "        encoder_out = encoder_out[sort_ind]  # Sorting corresponding input image encodings based on sorted indices [32, 196, 2048]\n",
        "        encoded_captions = encoded_captions[sort_ind] # Sorting corresponding input caption encodings based on sorted indices [32, 52]\n",
        "                                                      # [32, 52] because 32 is batch size and 52 is the fixed caption length we choose to create above in input dataset.\n",
        "\n",
        "        # Embedding\n",
        "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim) --> [32, 52, 512]\n",
        "\n",
        "        # Initialize LSTM state\n",
        "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim) --> h & c of size [32, 512]\n",
        "\n",
        "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
        "        # So, decoding lengths are actual lengths - 1\n",
        "        # Let us say caption_lengths are tensor([20, 19, 19, 18, 16, 15, 14, 14, 13, 13, 13, 13, 13, 13, 13, 12, 12, 12, 12, 11, 11, 11, 10, 10, 10,  9,  9,  9,  8,  8,  8,  7],\n",
        "        # then decode_lengths will be           [19, 18, 18, 17, 15, 14, 13, 13, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 10, 10, 10,  9,  9,  9,  8,  8,  8,  7,  7,  7,  6]\n",
        "        decode_lengths = (caption_lengths - 1).tolist()\n",
        "\n",
        "        # Create tensors to hold word predicion scores and alphas\n",
        "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device) \n",
        "        # predictions are words --> an instance could be [32, 19, 2633]. Here 19 can change based on max caption_length in a particular batch.\n",
        "        # vocab_size -> 2633 -> len(word_map). Word_map we created early while creating input data \n",
        "\n",
        "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device) \n",
        "        # alphas are weights that decide which pixel to focus\n",
        "        # an instance could be [32, 19, 196]. Here 19 can change based on max caption_length in a particular batch.\n",
        "\n",
        "\n",
        "        # At each time-step, decode by\n",
        "        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n",
        "        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
        "        \n",
        "        # This 'for' loop is to ensure that we process images by decreasing caption lengths. This will help to avoid processing <pad>.\n",
        "        # Let us say decode_lengths = [19, 18, 18, 17, 15, 14, 13, 13, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 10, 10, 10, 9, 9, 9, 8, 8, 8, 7, 7, 7, 6]\n",
        "        # Here max(decode_lengths) = 19 so 'for' loop will execute from 0 to 18 & batch_size_t at each time-step 't' will be as follows:\n",
        "        '''\n",
        "        batch_size_t = sum([l > t for l in decode_lengths])\n",
        "        t = 0 till 5 , batch_size_t = 32, all 32 images processed & 32 outputs from previous step is used. Caption_length 19, 18, 18, 17, 15, 14, 13, 13, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 10, 10, 10, 9, 9, 9, 8, 8, 8, 7, 7, 7, 6\n",
        "        t = 6, batch_size_t = 31, only 31 images processed & 31 top outputs from previous step is used. Caption_length 19, 18, 18, 17, 15, 14, 13, 13, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 10, 10, 10, 9, 9, 9, 8, 8, 8, 7, 7, 7\n",
        "        t = 7, batch_size_t = 28, only 28 images processed & 23 top outputs from previous step is used. Caption_length 19, 18, 18, 17, 15, 14, 13, 13, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 10, 10, 10, 9, 9, 9, 8, 8, 8\n",
        "        t = 8, batch_size_t = 25, only 25 images processed & 25 top outputs from previous step is used. Caption_length 19, 18, 18, 17, 15, 14, 13, 13, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 10, 10, 10, 9, 9, 9\n",
        "        t = 9, batch_size_t = 22, only 22 images processed & 22 top outputs from previous step is used. Caption_length 19, 18, 18, 17, 15, 14, 13, 13, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 10, 10, 10\n",
        "        t = 10, batch_size_t = 19, only 19 images processed & 19 top outputs from previous step is used. Caption_length 19, 18, 18, 17, 15, 14, 13, 13, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11\n",
        "        t = 11, batch_size_t = 15, only 15 images processed & 15 top outputs from previous step is used. Caption_length 19, 18, 18, 17, 15, 14, 13, 13, 12, 12, 12, 12, 12, 12, 12\n",
        "        t = 12, batch_size_t = 8, only 8 images processed & 8 top outputs from previous step is used. Caption_length 19, 18, 18, 17, 15, 14, 13\n",
        "        t = 13, batch_size_t = 6, only 6 images processed & 6 top outputs from previous step is used. Caption_length 19, 18, 18, 17, 15, 14\n",
        "        t = 14, batch_size_t = 5, only 5 images processed & 5 top outputs from previous step is used. Caption_length 19, 18, 18, 17, 15\n",
        "        t = 15, batch_size_t = 4, only 4 images processed & 4 top outputs from previous step is used. Caption_length 19, 18, 18, 17\n",
        "        t = 16, batch_size_t = 4, only 4 images processed & 4 top outputs from previous step is used. Caption_length 19, 18, 18, 17\n",
        "        t = 17, batch_size_t = 3, only 3 images processed & 3 top outputs from previous step is used. Caption_length 19, 18, 18\n",
        "        t = 18, batch_size_t = 1, only 1 image processed & 1 top output from previous step is used. Caption_length 19\n",
        "        '''\n",
        "        for t in range(max(decode_lengths)):\n",
        "            \n",
        "            batch_size_t = sum([l > t for l in decode_lengths])\n",
        "          \n",
        "            '''\n",
        "                        attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
        "                                                                h[:batch_size_t], px) \n",
        "            Passing only those image encodings to attention network based on decreasing caption lengths\n",
        "            for example at t = 16, batch_size_t = 4 \n",
        "                           t = 17, batch_size_t = 3. Let us consider 17th time-step.\n",
        "            then encoder_out[:3] which means only 3 images out of 32 that are passed to attention on 17th step Which also means out of \n",
        "            [32, 196, 2048] only [3,196,2048] is passed to self.attention.\n",
        "            Also h[:3] means only first 3 entries from h belonging to 16th time-step will be passed on 17th step for attention processing.\n",
        "            which means out of [32, 512], only [3,512] is passed to self.attention\n",
        "            '''\n",
        "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
        "                                                                h[:batch_size_t]) \n",
        "            \n",
        "            # for t = 17 in above example, attention_weighted_encoding.size() will be [3, 2048] and alpha will be [3, 196]\n",
        "            # Check Attention(nn.module) to get better sense of dimensions and operation\n",
        "\n",
        "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim) \n",
        "                            # for t = 17 in above example, \n",
        "                            # h[:batch_size_t]                             --> [3, 512], h --> [4, 512] bcoz t = 16 has batch_size_t = 4  \n",
        "                            # self.f_beta(h[:batch_size_t])                --> [3, 2048]\n",
        "                            # self.sigmoid(self.f_beta(h[:batch_size_t]))  --> [3, 2048]\n",
        "                            # This dimension change is to enable gating scalar multiplication with attention_weighted_encoding [3, 2048]\n",
        "            attention_weighted_encoding = gate * attention_weighted_encoding  # for t =17 in above example, [3, 2048]      \n",
        "            h, c = self.decode_step(\n",
        "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
        "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
        "            '''\n",
        "            for t = 17 in above example, \n",
        "               [embeddings[:batch_size_t, t, :] --> embeddings[:3, 17, :] --> [3, 512] where embeddings.size() is [32, 52, 512]\n",
        "               Refer pytorch slicing example given below for more clarity\n",
        "               :3        -> 0,1,2 out of 32 selected from [32,52,512]\n",
        "               17        -> Only 17th row out of 52 rows selected from [32,52,512]\n",
        "               :         -> All 512 columns selected from [32,52,512]\n",
        "               17, :     -> 17th row element for all 512 columns selected. This will give a tensor of size [1, 512]\n",
        "               :3, 17, : -> Means 3 such entries out of 32 is selected giving a tensor of size [3, 512]\n",
        "               torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1)\n",
        "                                                --> torch.cat([[3, 512], [3, 2048]], dim =1) --> [3, 2560]\n",
        "               h[:batch_size_t] that is fed to LSTM will be from t = 16. So h.size() will be [4, 512] bcoz t = 16 has batch_size_t = 4 &\n",
        "               h passed to LSTM in 17th time-step i.e. h[:batch_size_t] will be [3, 512] bcoz t = 17 has batch_size_t = 3.\n",
        "               \n",
        "               Similarly c[:batch_size_t] that is fed to LSTM will be from t = 16. So c.size() will be [4, 512] bcoz t = 16 has \n",
        "               batch_size_t = 4 & c passed to LSTM  in 17th time-step i.e. c[:batch_size_t] will be [3, 512] bcoz t = 17 has batch_size_t = 3.\n",
        "               \n",
        "               To summarize h,c = self.decode_step([3,2560], [3, 512], [3, 512])\n",
        "\n",
        "               self.decode_step is an LSTM cell that will accept weighted encodings, prev prediction from decoder (h) and prev cell state (c).\n",
        "               Output of LSTM cell is h and c.\n",
        "\n",
        "            '''   \n",
        "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size) \n",
        "            # preds are scores over vocabulary. \n",
        "            # for t = 17 in above example, h.size = [3, 512]. After passing through self.fc it will become [3, 2633]     \n",
        "            predictions[:batch_size_t, t, :] = preds\n",
        "            # Here predictions.size() that we created before 'for' loop is [32, 19, 2633]. \n",
        "            # Out of this [0:3, 19, 2633] is replaced by preds we got above\n",
        "            alphas[:batch_size_t, t, :] = alpha\n",
        "            # Here alphas.size() that we created before 'for' loop is [32, 19, 196]. \n",
        "            # Out of this [0:2, 19, 196] is replaced by alpha we got from self.attention           \n",
        "       \n",
        "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfC8KCh4HK-l"
      },
      "source": [
        "# Data parameters\n",
        "data_folder = '/content/data_output'                  # folder with data files saved by create_input_files.py...Create this MANUALLY in colab\n",
        "data_name = 'flickr8k_5_cap_per_img_5_min_word_freq'  # base name shared by data files"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P55kTHd5KNBz"
      },
      "source": [
        "# Model parameters\n",
        "emb_dim = 512  # dimension of word embeddings\n",
        "attention_dim = 512  # dimension of attention linear layers\n",
        "decoder_dim = 512  # dimension of decoder RNN\n",
        "encoder_dim = 512 # dimension from the resnet18\n",
        "dropout = 0.5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
        "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_YpEEzgTAlL"
      },
      "source": [
        "# Training parameters\n",
        "start_epoch = 0\n",
        "epochs = 50  # number of epochs to train for (if early stopping is not triggered)\n",
        "epochs_since_improvement = 0  # keeps track of number of epochs since there's been an improvement in validation BLEU\n",
        "batch_size = 32\n",
        "workers = 1  # for data-loading; right now, only 1 works with h5py\n",
        "encoder_lr = 1e-4  # learning rate for encoder if fine-tuning\n",
        "decoder_lr = 4e-4  # learning rate for decoder\n",
        "grad_clip = 5.  # clip gradients at an absolute value of\n",
        "alpha_c = 1.  # regularization parameter for 'doubly stochastic attention', as in the paper\n",
        "best_bleu4 = 0.  # BLEU-4 score right now\n",
        "print_freq = 100  # print training/validation stats every __ batches\n",
        "fine_tune_encoder = False  # fine-tune encoder?\n",
        "checkpoint = None  # path to checkpoint, None if none"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovVJnP4UTjEK"
      },
      "source": [
        "def train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch):\n",
        "    \"\"\"\n",
        "    Performs one epoch's training.\n",
        "    :param train_loader: DataLoader for training data\n",
        "    :param encoder: encoder model\n",
        "    :param decoder: decoder model\n",
        "    :param criterion: loss layer\n",
        "    :param encoder_optimizer: optimizer to update encoder's weights (if fine-tuning)\n",
        "    :param decoder_optimizer: optimizer to update decoder's weights\n",
        "    :param epoch: epoch number\n",
        "    \"\"\"  \n",
        "\n",
        "    decoder.train()  # train mode (dropout and batchnorm is used)\n",
        "    encoder.train()\n",
        "\n",
        "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
        "    data_time = AverageMeter()  # data loading time\n",
        "    losses = AverageMeter()  # loss (per word decoded)\n",
        "    top5accs = AverageMeter()  # top5 accuracy\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    print(' *** Epoch :',epoch)    \n",
        "\n",
        "    # Batches\n",
        "    for i, (imgs, caps, caplens) in enumerate(train_loader):\n",
        "        data_time.update(time.time() - start)\n",
        "\n",
        "        # Move to GPU, if available\n",
        "        imgs = imgs.to(device)\n",
        "        caps = caps.to(device)\n",
        "        caplens = caplens.to(device)\n",
        "\n",
        "        # Forward prop.\n",
        "        imgs = encoder(imgs)\n",
        "        scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n",
        "        # Output of decoder -> predictions [32, 20, 2633], encoded_captions [32,52], decode_lengths 32, alphas [32,20,196], sort_ind [32]  \n",
        "        # Here 20 could change based on max(decode_lengths)     \n",
        "        \n",
        "        # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
        "        targets = caps_sorted[:, 1:]  # caps_sorted.size -> [32, 52] This means there are 32 rows each having captions of length 52.\n",
        "                                      # So caps_sorted[:, 1:] will give us [32, 51] excluding the first column that represents <start>\n",
        "       \n",
        "        # Remove timesteps that we didn't decode at, or are pads\n",
        "        # pack_padded_sequence is an easy trick to do this. \n",
        "        \n",
        "        # Refer https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch\n",
        "        # Let us say decode_lengths = [22, 21, 19, 19, 18, 16, 14, 14, 13, 13, 13, 12, 12, 12, 12, 12, 12, 11, 11, 11, 10, 10, 10, 9, 9, 8, 8, 7, 7, 7, 6, 5]\n",
        "        # sum of decode_lengths = 383\n",
        "        # Before applying \"pack_padded_sequence\":scores.size() -> [32, 22, 2633], targets.size() -> [32, 51] \n",
        "        scores = pack_padded_sequence(scores, decode_lengths, batch_first=True).data    # ([32, 22, 2633], 383, batch_first=True )\n",
        "        targets = pack_padded_sequence(targets, decode_lengths, batch_first=True).data  # ([32, 51],       383, batch_first=True )\n",
        "        # After applying \"pack_padded_sequence\":\n",
        "        # scores.size() -> [383, 2633]  32*22 = 704 but not all 32 rows need 22 characters, many are pads. So after applying pack-padding,\n",
        "        # we reduced this 704 to 383. You can understand how it works better from stackoverflow link given above.\n",
        "        # targets.size()-> [383] 1632 reduced to 383 via pack-padding.\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(scores, targets)  # loss coming out from nn.CrossEntropyLoss criterion is a single value tensor\n",
        "        \n",
        "        # Add doubly stochastic attention regularization\n",
        "        loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
        "        \n",
        "        # Back prop.\n",
        "        decoder_optimizer.zero_grad()\n",
        "        if encoder_optimizer is not None:\n",
        "            encoder_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients\n",
        "        if grad_clip is not None:\n",
        "            clip_gradient(decoder_optimizer, grad_clip)\n",
        "            if encoder_optimizer is not None:\n",
        "                clip_gradient(encoder_optimizer, grad_clip)\n",
        "\n",
        "        # Update weights\n",
        "        decoder_optimizer.step()\n",
        "        if encoder_optimizer is not None:\n",
        "            encoder_optimizer.step()\n",
        "\n",
        "        # Keep track of metrics\n",
        "        top5 = accuracy(scores, targets, 5)\n",
        "        losses.update(loss.item(), sum(decode_lengths))\n",
        "        top5accs.update(top5, sum(decode_lengths))\n",
        "        batch_time.update(time.time() - start)\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        # Print status\n",
        "        if i % print_freq == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'.format(epoch, i, len(train_loader),\n",
        "                                                                          batch_time=batch_time,\n",
        "                                                                          data_time=data_time, loss=losses,\n",
        "                                                                          top5=top5accs))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaLPD8-pj3Tb"
      },
      "source": [
        "## Slicing pytorch example\n",
        "#### torch.rand(3,3,2) --> Means 3 pages each having (4,2) tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzkLRPN_c4A8",
        "outputId": "a0d77233-75dd-458e-db43-4e16341cc4bb"
      },
      "source": [
        "b = torch.rand(3,4,2)\n",
        "print(b)\n",
        "b_ = b[:2, 3, :]  # :2 -> Means 3rd page omitted. Only 1st & 2nd page will be represented\n",
        "                  #  3 -> Means 3rd index row only selected out of 4 indexes i.e. [0,1,2,3]\n",
        "                  # : -> Means all available out of selected 2 columns [0,1] are taken\n",
        "print(b_)\n",
        "print(b.size(), b_.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0.4292, 0.9800],\n",
            "         [0.6874, 0.2835],\n",
            "         [0.3971, 0.9038],\n",
            "         [0.4445, 0.3100]],\n",
            "\n",
            "        [[0.9836, 0.5401],\n",
            "         [0.4187, 0.3589],\n",
            "         [0.1402, 0.5209],\n",
            "         [0.8631, 0.8726]],\n",
            "\n",
            "        [[0.2677, 0.5601],\n",
            "         [0.7570, 0.7538],\n",
            "         [0.1663, 0.0958],\n",
            "         [0.7970, 0.4050]]])\n",
            "tensor([[0.4445, 0.3100],\n",
            "        [0.8631, 0.8726]])\n",
            "torch.Size([3, 4, 2]) torch.Size([2, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwlTgKk9T7l1"
      },
      "source": [
        "def validate(val_loader, encoder, decoder, criterion):\n",
        "    \"\"\"\n",
        "    Performs one epoch's validation.\n",
        "    :param val_loader: DataLoader for validation data.\n",
        "    :param encoder: encoder model\n",
        "    :param decoder: decoder model\n",
        "    :param criterion: loss layer\n",
        "    :return: BLEU-4 score\n",
        "    \"\"\"\n",
        "    decoder.eval()  # eval mode (no dropout or batchnorm)\n",
        "    if encoder is not None:\n",
        "        encoder.eval()\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top5accs = AverageMeter()\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    references = list()  # references (true captions) for calculating BLEU-4 score\n",
        "    hypotheses = list()  # hypotheses (predictions)\n",
        "\n",
        "    # explicitly disable gradient calculation to avoid CUDA memory error\n",
        "    # solves the issue #57\n",
        "    with torch.no_grad():\n",
        "        # Batches\n",
        "        for i, (imgs, caps, caplens, allcaps) in enumerate(val_loader):\n",
        "            \n",
        "            # Move to device, if available\n",
        "            imgs = imgs.to(device)\n",
        "            caps = caps.to(device)\n",
        "            caplens = caplens.to(device)\n",
        "\n",
        "            # Forward prop.\n",
        "            if encoder is not None:\n",
        "                imgs = encoder(imgs)\n",
        "            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n",
        "           # scores [32, 19, 2633], caps_sorted [32, 52], decode_lengths 32, alphas [32, 19, 196], sort_ind 32 \n",
        "           # Here 19 could change based on max(decode_lengths)\n",
        " \n",
        "            # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
        "            targets = caps_sorted[:, 1:] # [32, 51]\n",
        "\n",
        "            # Remove timesteps that we didn't decode at, or are pads\n",
        "            # pack_padded_sequence is an easy trick to do this\n",
        "            scores_copy = scores.clone()\n",
        "            scores = pack_padded_sequence(scores, decode_lengths, batch_first=True).data     # size -> [sum of decode_lengths, 2633]\n",
        "            targets = pack_padded_sequence(targets, decode_lengths, batch_first=True).data   # size -> [sum of decode_lengths]\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(scores, targets)\n",
        "\n",
        "            # Add doubly stochastic attention regularization\n",
        "            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
        "\n",
        "            # Keep track of metrics\n",
        "            losses.update(loss.item(), sum(decode_lengths))\n",
        "            top5 = accuracy(scores, targets, 5)\n",
        "            top5accs.update(top5, sum(decode_lengths))\n",
        "            batch_time.update(time.time() - start)\n",
        "\n",
        "            start = time.time()\n",
        "\n",
        "            if i % print_freq == 0:\n",
        "                print('Validation: [{0}/{1}]\\t'\n",
        "                      'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                      'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(i, len(val_loader), batch_time=batch_time,\n",
        "                                                                                loss=losses, top5=top5accs))\n",
        "\n",
        "            # Store references (true captions), and hypothesis (prediction) for each image\n",
        "            # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n",
        "            # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n",
        "\n",
        "            # References\n",
        "            allcaps = allcaps[sort_ind]  # because images were sorted in the decoder \n",
        "            # allcaps -> [32, 5, 52], there are 5 captions for each image. So 32 images in a batch, each having 5 captions of length 52                             \n",
        "\n",
        "            '''\n",
        "            Below logic is to remove <start> -> index 2631 & <pad> -> index 0 from allcaps.\n",
        "            Result is 'references' which is a list having 32 lists inside it. Each of the 32 lists will have 5 captions against it.\n",
        "            Eg. of 'references' as follows:\n",
        "            References need to be in this format for BLEU metrics.This evaluates a generated caption against reference caption(s). \n",
        "            For each generated caption, we will use all N_c captions (here 5) available for that image as the reference captions.\n",
        "\n",
        "            [[[7, 301, 825, 34, 717, 45, 246, 2, 301, 67, 918, 1477, 2632], \n",
        "              [1, 918, 301, 8, 7, 4, 602, 34, 2630, 374, 9, 301, 8, 2, 248, 4, 2251, 9, 45, 2632], \n",
        "              [14, 1090, 288, 37, 918, 92, 239, 99, 4, 404, 472, 9, 369, 2632], \n",
        "              [198, 274, 275, 117, 42, 1, 918, 92, 1, 2630, 306, 34, 714, 9, 45, 246, 107, 2632],  ---> 1st list\n",
        "              [610, 8, 2, 657, 2251, 918, 473, 246, 610, 8, 7, 657, 2632]], \n",
        "              \n",
        "              [[14, 256, 226, 59, 8, 1, 169, 2632], \n",
        "              [14, 256, 226, 180, 473, 8, 1, 169, 2632], \n",
        "              [14, 120, 226, 288, 59, 19, 198, 745, 169, 2632], \n",
        "              [9, 29, 46, 44, 29, 32, 288, 59, 212, 38, 212, 2632],                   ---> 2nd list\n",
        "              [1, 46, 8, 72, 44, 1, 32, 51, 288, 59, 13, 1, 24, 1454, 169, 2632]], \n",
        "              \n",
        "              ....\n",
        "\n",
        "              [[1, 89, 90, 1, 1214, 56, 44, 1, 1666, 27, 1053, 2632], \n",
        "               [9, 89, 42, 1, 2630, 1312, 350, 1, 927, 27, 1191, 8, 166, 343, 329, 2632], \n",
        "               [1, 89, 64, 1053, 350, 1, 927, 27, 1191, 44, 132, 88, 2632],                       ---> 32nd list\n",
        "               [1, 32, 42, 1015, 439, 67, 44, 2630, 67, 166, 329, 94, 13, 2496, 2632], \n",
        "               [9, 165, 8, 9, 1278, 590, 56, 48, 1, 1887, 67, 1, 7, 195, 8, 166, 329, 2632]]] \n",
        "\n",
        "            Length of 'references' keeps increasing by 32 with each iteration. If there are 100 iterations (i = 100), then progression of\n",
        "            'references' will be 0, 32, 64, 96,.....3200.\n",
        "            '''\n",
        "            \n",
        "            for j in range(allcaps.shape[0]):\n",
        "                img_caps = allcaps[j].tolist()\n",
        "                img_captions = list(\n",
        "                    map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<pad>']}],\n",
        "                        img_caps))  # remove <start> and pads                    \n",
        "                references.append(img_captions) \n",
        "\n",
        "            # Hypotheses\n",
        "            _, preds = torch.max(scores_copy, dim=2) \n",
        "            # Returns (value, indices). We are interested in indices here which is \"preds\"\n",
        "            # preds -> [32, 19]...We are taking index corresponding to maximum value out of 2633 for 18 timesteps belonging to each of 32 images. \n",
        "            # Remember that Scores_copy dimension in this example was -> [32, 19, 2633] & 19 can change based on max(decode_lengths)\n",
        "\n",
        "            '''\n",
        "            Let us say preds -> [32, 19]. Here 19 is the max(decode_length). However all the 32 entries won't be having 19 values.\n",
        "            Decode_lengths [19, 17, 17, 17, 16, 16, 15, 14, 14, 14, 13, 13, 13, 13, 12, 12, 12, 11, 11, 11, 10, 10, 10, 10, 9, 9, 9, 8, 8, 7, 7, 7]\n",
        "            From decode_lengths, it is evident that only 1st image has 19 caption length, 2nd, 3rd & 4th has 17 and so on..\n",
        "            So preds value will be as below for 1st (preds[0]), 2nd(preds[1]) & 5th image(preds[4]).\n",
        "            preds[0] -> [1, 99, 8, 1, 89, 2630, 1, 4, 2632, 1, 2630, 2632, 1, 366, 2632, 1, 2630, 1, 2632] -> 19 elements\n",
        "            preds[1] -> [ 1, 99, 4, 8, 1, 44, 67, 1, 7, 3, 7, 3, 4, 9, 169, 169, 2632, 0, 0] -> 19 elements but only 17 has values, 2 are padded with zeroes\n",
        "            preds[4] -> [ 1, 46, 8, 368, 1, 2630, 67, 1, 2632, 1, 2630, 2632, 9, 78, 2632, 2632, 0, 0, 0]\n",
        "                                                                          -> 19 elements but only 16 has values, rest 3 are padded with zeroes\n",
        "            Below logic removes pad i.e. removes 0s and write to temp_preds which will later be written to 'hypotheses'.\n",
        "            temp_preds for 1st, 2nd & 5th will look as below:\n",
        "            temp_preds = [[1, 99, 8, 1, 89, 2630, 1, 4, 2632, 1, 2630, 2632, 1, 366, 2632, 1, 2630, 1, 2632],\n",
        "                          [ 1, 99, 4, 8, 1, 44, 67, 1, 7, 3, 7, 3, 4, 9, 169, 169, 2632],\n",
        "                          ..\n",
        "                          ..\n",
        "                          [ 1, 46, 8, 368, 1, 2630, 67, 1, 2632, 1, 2630, 2632, 9, 78, 2632, 2632]\n",
        "                          ..\n",
        "                          ..]\n",
        "            Length of 'hypotheses' like 'references' before keeps increasing by 32 with each iteration. If there are 100 iterations (i = 100), \n",
        "            then progression of 'hypotheses' will be 0, 32, 64, 96,.....3200.              \n",
        "            '''\n",
        "            preds = preds.tolist()\n",
        "            temp_preds = list()\n",
        "            for j, p in enumerate(preds):\n",
        "                temp_preds.append(preds[j][:decode_lengths[j]])  # remove pads          \n",
        "            preds = temp_preds\n",
        "            hypotheses.extend(preds)\n",
        "            \n",
        "            assert len(references) == len(hypotheses)\n",
        "\n",
        "        # Calculate BLEU-4 scores. BLEU score is a metric designed for comparing \n",
        "        # naturally generated captions to ground-truth captions of differing length. \n",
        "        bleu4 = corpus_bleu(references, hypotheses)\n",
        "\n",
        "        print(\n",
        "            '\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}, BLEU-4 - {bleu}\\n'.format(\n",
        "                loss=losses,\n",
        "                top5=top5accs,\n",
        "                bleu=bleu4))\n",
        "\n",
        "    return bleu4"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_i2ypp_UHwv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14f4bbf4-360c-4bba-86ad-3e4de7cb9084"
      },
      "source": [
        "    \"\"\"\n",
        "    Training and validation.\n",
        "    \"\"\"\n",
        "\n",
        "    global best_bleu4, epochs_since_improvement, checkpoint, start_epoch, fine_tune_encoder, data_name, word_map\n",
        "    import time\n",
        "\n",
        "    # Read word map\n",
        "    word_map_file = os.path.join(data_folder, 'WORDMAP_' + data_name + '.json')\n",
        "    with open(word_map_file, 'r') as j:\n",
        "        word_map = json.load(j)\n",
        "\n",
        "    # Initialize / load checkpoint\n",
        "    if checkpoint is None:\n",
        "        print('checkpoint')\n",
        "        decoder = DecoderWithAttention(attention_dim=attention_dim,\n",
        "                                       embed_dim=emb_dim,\n",
        "                                       decoder_dim=decoder_dim,                                  \n",
        "                                       vocab_size=len(word_map),\n",
        "                                       dropout=dropout,\n",
        "                                       encoder_dim=encoder_dim\n",
        "                                       )\n",
        "        decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n",
        "                                             lr=decoder_lr)\n",
        "        encoder = Encoder()\n",
        "        encoder.fine_tune(fine_tune_encoder)\n",
        "        encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
        "                                             lr=encoder_lr) if fine_tune_encoder else None\n",
        "\n",
        "    else:\n",
        "        checkpoint = torch.load(checkpoint)\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
        "        best_bleu4 = checkpoint['bleu-4']\n",
        "        decoder = checkpoint['decoder']\n",
        "        decoder_optimizer = checkpoint['decoder_optimizer']\n",
        "        encoder = checkpoint['encoder']\n",
        "        encoder_optimizer = checkpoint['encoder_optimizer']\n",
        "        if fine_tune_encoder is True and encoder_optimizer is None:\n",
        "            encoder.fine_tune(fine_tune_encoder)\n",
        "            encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
        "                                                 lr=encoder_lr)\n",
        "\n",
        "    # Move to GPU, if available\n",
        "    decoder = decoder.to(device)\n",
        "    encoder = encoder.to(device)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    # Custom dataloaders\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        CaptionDataset(data_folder, data_name, 'TRAIN', transform=transforms.Compose([normalize])),\n",
        "        batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        CaptionDataset(data_folder, data_name, 'VAL', transform=transforms.Compose([normalize])),\n",
        "        batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
        "\n",
        "    # Epochs\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "\n",
        "        # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n",
        "        if epochs_since_improvement == 20:\n",
        "            break\n",
        "        if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n",
        "            adjust_learning_rate(decoder_optimizer, 0.5)\n",
        "            if fine_tune_encoder:\n",
        "                adjust_learning_rate(encoder_optimizer, 0.5)\n",
        "\n",
        "        # One epoch's training\n",
        "        train(train_loader=train_loader,\n",
        "              encoder=encoder,\n",
        "              decoder=decoder,\n",
        "              criterion=criterion,\n",
        "              encoder_optimizer=encoder_optimizer,\n",
        "              decoder_optimizer=decoder_optimizer,\n",
        "              epoch=epoch)\n",
        "\n",
        "        # One epoch's validation\n",
        "        recent_bleu4 = validate(val_loader=val_loader,\n",
        "                                encoder=encoder,\n",
        "                                decoder=decoder,\n",
        "                                criterion=criterion)\n",
        "\n",
        "        # Check if there was an improvement\n",
        "        is_best = recent_bleu4 > best_bleu4\n",
        "        best_bleu4 = max(recent_bleu4, best_bleu4)\n",
        "        if not is_best:\n",
        "            epochs_since_improvement += 1\n",
        "            print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
        "        else:\n",
        "            epochs_since_improvement = 0\n",
        "\n",
        "        # Save checkpoint\n",
        "        save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer,\n",
        "                        decoder_optimizer, recent_bleu4, is_best)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint\n",
            " *** Epoch : 0\n",
            "Epoch: [0][0/938]\tBatch Time 0.450 (0.450)\tData Load Time 0.165 (0.165)\tLoss 8.8308 (8.8308)\tTop-5 Accuracy 0.000 (0.000)\n",
            "Epoch: [0][100/938]\tBatch Time 0.232 (0.239)\tData Load Time 0.000 (0.002)\tLoss 5.3835 (6.1727)\tTop-5 Accuracy 43.299 (34.989)\n",
            "Epoch: [0][200/938]\tBatch Time 0.233 (0.238)\tData Load Time 0.000 (0.001)\tLoss 5.1607 (5.7796)\tTop-5 Accuracy 48.958 (39.687)\n",
            "Epoch: [0][300/938]\tBatch Time 0.236 (0.238)\tData Load Time 0.000 (0.001)\tLoss 5.0246 (5.5212)\tTop-5 Accuracy 49.062 (43.402)\n",
            "Epoch: [0][400/938]\tBatch Time 0.234 (0.238)\tData Load Time 0.000 (0.001)\tLoss 4.2631 (5.3365)\tTop-5 Accuracy 62.363 (46.023)\n",
            "Epoch: [0][500/938]\tBatch Time 0.241 (0.238)\tData Load Time 0.000 (0.001)\tLoss 4.6139 (5.1995)\tTop-5 Accuracy 55.586 (47.969)\n",
            "Epoch: [0][600/938]\tBatch Time 0.242 (0.237)\tData Load Time 0.000 (0.000)\tLoss 4.4642 (5.0951)\tTop-5 Accuracy 58.933 (49.332)\n",
            "Epoch: [0][700/938]\tBatch Time 0.222 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.9747 (5.0058)\tTop-5 Accuracy 62.637 (50.511)\n",
            "Epoch: [0][800/938]\tBatch Time 0.211 (0.237)\tData Load Time 0.000 (0.000)\tLoss 4.1865 (4.9284)\tTop-5 Accuracy 59.714 (51.520)\n",
            "Epoch: [0][900/938]\tBatch Time 0.233 (0.237)\tData Load Time 0.000 (0.000)\tLoss 4.3232 (4.8650)\tTop-5 Accuracy 60.317 (52.372)\n",
            "Validation: [0/157]\tBatch Time 0.312 (0.312)\tLoss 4.1431 (4.1431)\tTop-5 Accuracy 65.730 (65.730)\t\n",
            "Validation: [100/157]\tBatch Time 0.142 (0.151)\tLoss 4.0913 (4.2278)\tTop-5 Accuracy 64.035 (61.054)\t\n",
            "\n",
            " * LOSS - 4.214, TOP-5 ACCURACY - 61.113, BLEU-4 - 0.1194555358727753\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type DecoderWithAttention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Attention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch : 0 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            "Best checkpoint encountered & saved in epoch : 0\n",
            " *** Epoch : 1\n",
            "Epoch: [1][0/938]\tBatch Time 0.476 (0.476)\tData Load Time 0.179 (0.179)\tLoss 4.2756 (4.2756)\tTop-5 Accuracy 60.511 (60.511)\n",
            "Epoch: [1][100/938]\tBatch Time 0.217 (0.239)\tData Load Time 0.000 (0.002)\tLoss 4.2791 (4.1861)\tTop-5 Accuracy 60.388 (60.958)\n",
            "Epoch: [1][200/938]\tBatch Time 0.225 (0.237)\tData Load Time 0.000 (0.001)\tLoss 4.6435 (4.1882)\tTop-5 Accuracy 53.186 (61.010)\n",
            "Epoch: [1][300/938]\tBatch Time 0.229 (0.237)\tData Load Time 0.000 (0.001)\tLoss 4.1495 (4.1776)\tTop-5 Accuracy 60.963 (61.199)\n",
            "Epoch: [1][400/938]\tBatch Time 0.228 (0.237)\tData Load Time 0.000 (0.001)\tLoss 4.1925 (4.1622)\tTop-5 Accuracy 60.204 (61.418)\n",
            "Epoch: [1][500/938]\tBatch Time 0.239 (0.237)\tData Load Time 0.000 (0.001)\tLoss 3.6495 (4.1537)\tTop-5 Accuracy 69.890 (61.556)\n",
            "Epoch: [1][600/938]\tBatch Time 0.238 (0.237)\tData Load Time 0.000 (0.000)\tLoss 4.1688 (4.1387)\tTop-5 Accuracy 59.517 (61.751)\n",
            "Epoch: [1][700/938]\tBatch Time 0.244 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.9502 (4.1239)\tTop-5 Accuracy 66.760 (61.977)\n",
            "Epoch: [1][800/938]\tBatch Time 0.225 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.8609 (4.1103)\tTop-5 Accuracy 64.611 (62.156)\n",
            "Epoch: [1][900/938]\tBatch Time 0.241 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.8272 (4.0961)\tTop-5 Accuracy 65.707 (62.357)\n",
            "Validation: [0/157]\tBatch Time 0.303 (0.303)\tLoss 4.0022 (4.0022)\tTop-5 Accuracy 64.211 (64.211)\t\n",
            "Validation: [100/157]\tBatch Time 0.144 (0.151)\tLoss 3.6986 (3.9695)\tTop-5 Accuracy 67.945 (64.339)\t\n",
            "\n",
            " * LOSS - 3.965, TOP-5 ACCURACY - 64.368, BLEU-4 - 0.12438223044811099\n",
            "\n",
            "epoch : 1 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            "Best checkpoint encountered & saved in epoch : 1\n",
            " *** Epoch : 2\n",
            "Epoch: [2][0/938]\tBatch Time 0.463 (0.463)\tData Load Time 0.166 (0.166)\tLoss 3.8758 (3.8758)\tTop-5 Accuracy 64.491 (64.491)\n",
            "Epoch: [2][100/938]\tBatch Time 0.233 (0.240)\tData Load Time 0.000 (0.002)\tLoss 4.0994 (3.8584)\tTop-5 Accuracy 64.607 (65.420)\n",
            "Epoch: [2][200/938]\tBatch Time 0.245 (0.238)\tData Load Time 0.000 (0.001)\tLoss 3.8740 (3.8698)\tTop-5 Accuracy 65.816 (65.284)\n",
            "Epoch: [2][300/938]\tBatch Time 0.221 (0.238)\tData Load Time 0.000 (0.001)\tLoss 3.8039 (3.8670)\tTop-5 Accuracy 66.400 (65.262)\n",
            "Epoch: [2][400/938]\tBatch Time 0.236 (0.238)\tData Load Time 0.000 (0.001)\tLoss 3.7202 (3.8654)\tTop-5 Accuracy 66.491 (65.242)\n",
            "Epoch: [2][500/938]\tBatch Time 0.268 (0.238)\tData Load Time 0.000 (0.001)\tLoss 3.9770 (3.8604)\tTop-5 Accuracy 65.025 (65.348)\n",
            "Epoch: [2][600/938]\tBatch Time 0.233 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.7787 (3.8566)\tTop-5 Accuracy 67.341 (65.467)\n",
            "Epoch: [2][700/938]\tBatch Time 0.215 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.7607 (3.8486)\tTop-5 Accuracy 67.568 (65.624)\n",
            "Epoch: [2][800/938]\tBatch Time 0.249 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.7838 (3.8432)\tTop-5 Accuracy 65.672 (65.717)\n",
            "Epoch: [2][900/938]\tBatch Time 0.232 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.7075 (3.8351)\tTop-5 Accuracy 68.033 (65.834)\n",
            "Validation: [0/157]\tBatch Time 0.314 (0.314)\tLoss 3.6873 (3.6873)\tTop-5 Accuracy 67.989 (67.989)\t\n",
            "Validation: [100/157]\tBatch Time 0.162 (0.152)\tLoss 3.7853 (3.8159)\tTop-5 Accuracy 69.740 (66.530)\t\n",
            "\n",
            " * LOSS - 3.828, TOP-5 ACCURACY - 66.327, BLEU-4 - 0.14039087071434578\n",
            "\n",
            "epoch : 2 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            "Best checkpoint encountered & saved in epoch : 2\n",
            " *** Epoch : 3\n",
            "Epoch: [3][0/938]\tBatch Time 0.489 (0.489)\tData Load Time 0.174 (0.174)\tLoss 3.7502 (3.7502)\tTop-5 Accuracy 65.037 (65.037)\n",
            "Epoch: [3][100/938]\tBatch Time 0.249 (0.238)\tData Load Time 0.000 (0.002)\tLoss 3.6979 (3.6676)\tTop-5 Accuracy 65.962 (68.245)\n",
            "Epoch: [3][200/938]\tBatch Time 0.238 (0.237)\tData Load Time 0.000 (0.001)\tLoss 3.7783 (3.6695)\tTop-5 Accuracy 67.574 (68.085)\n",
            "Epoch: [3][300/938]\tBatch Time 0.267 (0.238)\tData Load Time 0.000 (0.001)\tLoss 3.5221 (3.6716)\tTop-5 Accuracy 69.388 (67.993)\n",
            "Epoch: [3][400/938]\tBatch Time 0.244 (0.238)\tData Load Time 0.000 (0.001)\tLoss 3.8723 (3.6693)\tTop-5 Accuracy 62.032 (68.081)\n",
            "Epoch: [3][500/938]\tBatch Time 0.227 (0.238)\tData Load Time 0.000 (0.001)\tLoss 3.7164 (3.6681)\tTop-5 Accuracy 65.160 (68.121)\n",
            "Epoch: [3][600/938]\tBatch Time 0.234 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.6905 (3.6671)\tTop-5 Accuracy 66.316 (68.138)\n",
            "Epoch: [3][700/938]\tBatch Time 0.241 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.5513 (3.6682)\tTop-5 Accuracy 69.797 (68.122)\n",
            "Epoch: [3][800/938]\tBatch Time 0.231 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.5450 (3.6668)\tTop-5 Accuracy 66.935 (68.154)\n",
            "Epoch: [3][900/938]\tBatch Time 0.231 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.4643 (3.6633)\tTop-5 Accuracy 70.250 (68.215)\n",
            "Validation: [0/157]\tBatch Time 0.310 (0.310)\tLoss 3.7149 (3.7149)\tTop-5 Accuracy 68.232 (68.232)\t\n",
            "Validation: [100/157]\tBatch Time 0.144 (0.151)\tLoss 3.9073 (3.7613)\tTop-5 Accuracy 66.027 (67.461)\t\n",
            "\n",
            " * LOSS - 3.769, TOP-5 ACCURACY - 67.160, BLEU-4 - 0.14196532591866518\n",
            "\n",
            "epoch : 3 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            "Best checkpoint encountered & saved in epoch : 3\n",
            " *** Epoch : 4\n",
            "Epoch: [4][0/938]\tBatch Time 0.472 (0.472)\tData Load Time 0.191 (0.191)\tLoss 3.3730 (3.3730)\tTop-5 Accuracy 74.799 (74.799)\n",
            "Epoch: [4][100/938]\tBatch Time 0.228 (0.239)\tData Load Time 0.000 (0.002)\tLoss 3.8048 (3.5179)\tTop-5 Accuracy 65.934 (69.955)\n",
            "Epoch: [4][200/938]\tBatch Time 0.239 (0.237)\tData Load Time 0.000 (0.001)\tLoss 3.4958 (3.5338)\tTop-5 Accuracy 70.171 (69.861)\n",
            "Epoch: [4][300/938]\tBatch Time 0.242 (0.238)\tData Load Time 0.000 (0.001)\tLoss 3.4807 (3.5342)\tTop-5 Accuracy 69.890 (69.919)\n",
            "Epoch: [4][400/938]\tBatch Time 0.233 (0.237)\tData Load Time 0.000 (0.001)\tLoss 3.5842 (3.5334)\tTop-5 Accuracy 70.361 (70.000)\n",
            "Epoch: [4][500/938]\tBatch Time 0.228 (0.237)\tData Load Time 0.000 (0.001)\tLoss 3.3362 (3.5322)\tTop-5 Accuracy 72.559 (70.032)\n",
            "Epoch: [4][600/938]\tBatch Time 0.226 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.5173 (3.5311)\tTop-5 Accuracy 70.028 (70.068)\n",
            "Epoch: [4][700/938]\tBatch Time 0.250 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.5183 (3.5317)\tTop-5 Accuracy 68.069 (70.099)\n",
            "Epoch: [4][800/938]\tBatch Time 0.226 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.5379 (3.5325)\tTop-5 Accuracy 69.867 (70.090)\n",
            "Epoch: [4][900/938]\tBatch Time 0.247 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.4505 (3.5302)\tTop-5 Accuracy 70.833 (70.137)\n",
            "Validation: [0/157]\tBatch Time 0.310 (0.310)\tLoss 3.4968 (3.4968)\tTop-5 Accuracy 71.823 (71.823)\t\n",
            "Validation: [100/157]\tBatch Time 0.149 (0.152)\tLoss 3.9612 (3.7290)\tTop-5 Accuracy 65.482 (67.522)\t\n",
            "\n",
            " * LOSS - 3.722, TOP-5 ACCURACY - 67.620, BLEU-4 - 0.14430730688455706\n",
            "\n",
            "epoch : 4 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            "Best checkpoint encountered & saved in epoch : 4\n",
            " *** Epoch : 5\n",
            "Epoch: [5][0/938]\tBatch Time 0.509 (0.509)\tData Load Time 0.162 (0.162)\tLoss 3.3020 (3.3020)\tTop-5 Accuracy 75.397 (75.397)\n",
            "Epoch: [5][100/938]\tBatch Time 0.223 (0.241)\tData Load Time 0.000 (0.002)\tLoss 3.5159 (3.4103)\tTop-5 Accuracy 72.404 (71.708)\n",
            "Epoch: [5][200/938]\tBatch Time 0.245 (0.240)\tData Load Time 0.000 (0.001)\tLoss 3.4128 (3.4150)\tTop-5 Accuracy 71.642 (71.739)\n",
            "Epoch: [5][300/938]\tBatch Time 0.229 (0.239)\tData Load Time 0.000 (0.001)\tLoss 3.2851 (3.4199)\tTop-5 Accuracy 72.606 (71.646)\n",
            "Epoch: [5][400/938]\tBatch Time 0.241 (0.238)\tData Load Time 0.000 (0.001)\tLoss 3.6527 (3.4207)\tTop-5 Accuracy 70.081 (71.693)\n",
            "Epoch: [5][500/938]\tBatch Time 0.230 (0.238)\tData Load Time 0.000 (0.000)\tLoss 3.5762 (3.4177)\tTop-5 Accuracy 70.248 (71.725)\n",
            "Epoch: [5][600/938]\tBatch Time 0.239 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.3708 (3.4191)\tTop-5 Accuracy 72.324 (71.718)\n",
            "Epoch: [5][700/938]\tBatch Time 0.252 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.6695 (3.4174)\tTop-5 Accuracy 67.506 (71.718)\n",
            "Epoch: [5][800/938]\tBatch Time 0.232 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.3961 (3.4193)\tTop-5 Accuracy 73.130 (71.692)\n",
            "Epoch: [5][900/938]\tBatch Time 0.258 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.4104 (3.4239)\tTop-5 Accuracy 72.346 (71.635)\n",
            "Validation: [0/157]\tBatch Time 0.323 (0.323)\tLoss 3.6809 (3.6809)\tTop-5 Accuracy 68.922 (68.922)\t\n",
            "Validation: [100/157]\tBatch Time 0.148 (0.151)\tLoss 3.6999 (3.7113)\tTop-5 Accuracy 65.544 (67.754)\t\n",
            "\n",
            " * LOSS - 3.700, TOP-5 ACCURACY - 67.977, BLEU-4 - 0.1439447879622303\n",
            "\n",
            "\n",
            "Epochs since last improvement: 1\n",
            "\n",
            "epoch : 5 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            " *** Epoch : 6\n",
            "Epoch: [6][0/938]\tBatch Time 0.444 (0.444)\tData Load Time 0.165 (0.165)\tLoss 3.0890 (3.0890)\tTop-5 Accuracy 76.901 (76.901)\n",
            "Epoch: [6][100/938]\tBatch Time 0.247 (0.238)\tData Load Time 0.000 (0.002)\tLoss 3.1596 (3.2927)\tTop-5 Accuracy 74.677 (73.498)\n",
            "Epoch: [6][200/938]\tBatch Time 0.245 (0.237)\tData Load Time 0.000 (0.001)\tLoss 3.2440 (3.3049)\tTop-5 Accuracy 74.935 (73.315)\n",
            "Epoch: [6][300/938]\tBatch Time 0.220 (0.237)\tData Load Time 0.000 (0.001)\tLoss 3.1684 (3.3092)\tTop-5 Accuracy 75.926 (73.287)\n",
            "Epoch: [6][400/938]\tBatch Time 0.226 (0.237)\tData Load Time 0.000 (0.001)\tLoss 3.1932 (3.3076)\tTop-5 Accuracy 73.797 (73.343)\n",
            "Epoch: [6][500/938]\tBatch Time 0.223 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.2511 (3.3169)\tTop-5 Accuracy 71.698 (73.244)\n",
            "Epoch: [6][600/938]\tBatch Time 0.224 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.2370 (3.3231)\tTop-5 Accuracy 75.900 (73.133)\n",
            "Epoch: [6][700/938]\tBatch Time 0.221 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.1743 (3.3237)\tTop-5 Accuracy 74.785 (73.120)\n",
            "Epoch: [6][800/938]\tBatch Time 0.257 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.4495 (3.3274)\tTop-5 Accuracy 72.048 (73.059)\n",
            "Epoch: [6][900/938]\tBatch Time 0.239 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.4811 (3.3256)\tTop-5 Accuracy 71.675 (73.087)\n",
            "Validation: [0/157]\tBatch Time 0.304 (0.304)\tLoss 3.7411 (3.7411)\tTop-5 Accuracy 67.302 (67.302)\t\n",
            "Validation: [100/157]\tBatch Time 0.155 (0.151)\tLoss 3.2785 (3.6874)\tTop-5 Accuracy 74.659 (68.322)\t\n",
            "\n",
            " * LOSS - 3.689, TOP-5 ACCURACY - 68.222, BLEU-4 - 0.14837694894621695\n",
            "\n",
            "epoch : 6 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            "Best checkpoint encountered & saved in epoch : 6\n",
            " *** Epoch : 7\n",
            "Epoch: [7][0/938]\tBatch Time 0.489 (0.489)\tData Load Time 0.160 (0.160)\tLoss 3.2400 (3.2400)\tTop-5 Accuracy 74.307 (74.307)\n",
            "Epoch: [7][100/938]\tBatch Time 0.234 (0.239)\tData Load Time 0.000 (0.002)\tLoss 2.9927 (3.2158)\tTop-5 Accuracy 77.989 (74.588)\n",
            "Epoch: [7][200/938]\tBatch Time 0.233 (0.238)\tData Load Time 0.000 (0.001)\tLoss 3.1404 (3.2180)\tTop-5 Accuracy 77.099 (74.631)\n",
            "Epoch: [7][300/938]\tBatch Time 0.235 (0.238)\tData Load Time 0.000 (0.001)\tLoss 3.4556 (3.2308)\tTop-5 Accuracy 70.694 (74.446)\n",
            "Epoch: [7][400/938]\tBatch Time 0.224 (0.238)\tData Load Time 0.000 (0.001)\tLoss 3.2121 (3.2386)\tTop-5 Accuracy 75.202 (74.371)\n",
            "Epoch: [7][500/938]\tBatch Time 0.269 (0.238)\tData Load Time 0.000 (0.000)\tLoss 3.2541 (3.2367)\tTop-5 Accuracy 74.555 (74.373)\n",
            "Epoch: [7][600/938]\tBatch Time 0.225 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.0469 (3.2352)\tTop-5 Accuracy 76.944 (74.425)\n",
            "Epoch: [7][700/938]\tBatch Time 0.247 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.1932 (3.2408)\tTop-5 Accuracy 76.903 (74.361)\n",
            "Epoch: [7][800/938]\tBatch Time 0.238 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.4052 (3.2418)\tTop-5 Accuracy 75.000 (74.373)\n",
            "Epoch: [7][900/938]\tBatch Time 0.251 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.0686 (3.2409)\tTop-5 Accuracy 77.454 (74.403)\n",
            "Validation: [0/157]\tBatch Time 0.320 (0.320)\tLoss 3.4635 (3.4635)\tTop-5 Accuracy 70.725 (70.725)\t\n",
            "Validation: [100/157]\tBatch Time 0.147 (0.152)\tLoss 3.8629 (3.6588)\tTop-5 Accuracy 65.940 (68.594)\t\n",
            "\n",
            " * LOSS - 3.678, TOP-5 ACCURACY - 68.455, BLEU-4 - 0.1490767821806858\n",
            "\n",
            "epoch : 7 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            "Best checkpoint encountered & saved in epoch : 7\n",
            " *** Epoch : 8\n",
            "Epoch: [8][0/938]\tBatch Time 0.479 (0.479)\tData Load Time 0.164 (0.164)\tLoss 3.5654 (3.5654)\tTop-5 Accuracy 70.096 (70.096)\n",
            "Epoch: [8][100/938]\tBatch Time 0.244 (0.240)\tData Load Time 0.000 (0.002)\tLoss 3.1414 (3.1389)\tTop-5 Accuracy 77.309 (75.829)\n",
            "Epoch: [8][200/938]\tBatch Time 0.228 (0.240)\tData Load Time 0.000 (0.001)\tLoss 3.3542 (3.1440)\tTop-5 Accuracy 74.677 (75.669)\n",
            "Epoch: [8][300/938]\tBatch Time 0.231 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.9782 (3.1482)\tTop-5 Accuracy 76.676 (75.657)\n",
            "Epoch: [8][400/938]\tBatch Time 0.235 (0.238)\tData Load Time 0.000 (0.001)\tLoss 3.0623 (3.1488)\tTop-5 Accuracy 75.443 (75.670)\n",
            "Epoch: [8][500/938]\tBatch Time 0.223 (0.238)\tData Load Time 0.000 (0.000)\tLoss 3.0201 (3.1510)\tTop-5 Accuracy 77.049 (75.654)\n",
            "Epoch: [8][600/938]\tBatch Time 0.239 (0.238)\tData Load Time 0.000 (0.000)\tLoss 3.2453 (3.1562)\tTop-5 Accuracy 75.916 (75.610)\n",
            "Epoch: [8][700/938]\tBatch Time 0.254 (0.238)\tData Load Time 0.000 (0.000)\tLoss 3.2376 (3.1586)\tTop-5 Accuracy 72.613 (75.618)\n",
            "Epoch: [8][800/938]\tBatch Time 0.210 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.3415 (3.1586)\tTop-5 Accuracy 70.904 (75.621)\n",
            "Epoch: [8][900/938]\tBatch Time 0.237 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.2047 (3.1612)\tTop-5 Accuracy 75.194 (75.584)\n",
            "Validation: [0/157]\tBatch Time 0.315 (0.315)\tLoss 3.4527 (3.4527)\tTop-5 Accuracy 71.429 (71.429)\t\n",
            "Validation: [100/157]\tBatch Time 0.148 (0.151)\tLoss 3.8834 (3.6839)\tTop-5 Accuracy 64.800 (68.546)\t\n",
            "\n",
            " * LOSS - 3.673, TOP-5 ACCURACY - 68.835, BLEU-4 - 0.1519682547136852\n",
            "\n",
            "epoch : 8 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            "Best checkpoint encountered & saved in epoch : 8\n",
            " *** Epoch : 9\n",
            "Epoch: [9][0/938]\tBatch Time 0.464 (0.464)\tData Load Time 0.172 (0.172)\tLoss 2.9122 (2.9122)\tTop-5 Accuracy 80.902 (80.902)\n",
            "Epoch: [9][100/938]\tBatch Time 0.228 (0.239)\tData Load Time 0.000 (0.002)\tLoss 2.9907 (3.0375)\tTop-5 Accuracy 77.260 (77.410)\n",
            "Epoch: [9][200/938]\tBatch Time 0.234 (0.238)\tData Load Time 0.000 (0.001)\tLoss 3.2484 (3.0466)\tTop-5 Accuracy 73.280 (77.313)\n",
            "Epoch: [9][300/938]\tBatch Time 0.249 (0.237)\tData Load Time 0.000 (0.001)\tLoss 3.2223 (3.0534)\tTop-5 Accuracy 73.747 (77.181)\n",
            "Epoch: [9][400/938]\tBatch Time 0.248 (0.237)\tData Load Time 0.000 (0.001)\tLoss 3.0323 (3.0686)\tTop-5 Accuracy 77.913 (76.966)\n",
            "Epoch: [9][500/938]\tBatch Time 0.247 (0.238)\tData Load Time 0.000 (0.001)\tLoss 3.1378 (3.0730)\tTop-5 Accuracy 76.943 (76.933)\n",
            "Epoch: [9][600/938]\tBatch Time 0.228 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.3985 (3.0777)\tTop-5 Accuracy 69.837 (76.830)\n",
            "Epoch: [9][700/938]\tBatch Time 0.222 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.1804 (3.0800)\tTop-5 Accuracy 75.135 (76.830)\n",
            "Epoch: [9][800/938]\tBatch Time 0.244 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.0930 (3.0824)\tTop-5 Accuracy 74.931 (76.791)\n",
            "Epoch: [9][900/938]\tBatch Time 0.226 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.1871 (3.0863)\tTop-5 Accuracy 74.656 (76.696)\n",
            "Validation: [0/157]\tBatch Time 0.307 (0.307)\tLoss 3.6634 (3.6634)\tTop-5 Accuracy 66.307 (66.307)\t\n",
            "Validation: [100/157]\tBatch Time 0.143 (0.152)\tLoss 3.4618 (3.6733)\tTop-5 Accuracy 72.162 (68.940)\t\n",
            "\n",
            " * LOSS - 3.678, TOP-5 ACCURACY - 68.775, BLEU-4 - 0.14942422974711705\n",
            "\n",
            "\n",
            "Epochs since last improvement: 1\n",
            "\n",
            "epoch : 9 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            " *** Epoch : 10\n",
            "Epoch: [10][0/938]\tBatch Time 0.429 (0.429)\tData Load Time 0.160 (0.160)\tLoss 2.9397 (2.9397)\tTop-5 Accuracy 79.293 (79.293)\n",
            "Epoch: [10][100/938]\tBatch Time 0.221 (0.238)\tData Load Time 0.000 (0.002)\tLoss 3.1127 (3.0202)\tTop-5 Accuracy 75.953 (77.805)\n",
            "Epoch: [10][200/938]\tBatch Time 0.219 (0.238)\tData Load Time 0.000 (0.001)\tLoss 2.9687 (3.0085)\tTop-5 Accuracy 81.593 (78.007)\n",
            "Epoch: [10][300/938]\tBatch Time 0.240 (0.237)\tData Load Time 0.000 (0.001)\tLoss 2.9886 (2.9996)\tTop-5 Accuracy 79.703 (78.188)\n",
            "Epoch: [10][400/938]\tBatch Time 0.252 (0.237)\tData Load Time 0.000 (0.001)\tLoss 3.0345 (3.0013)\tTop-5 Accuracy 78.718 (78.177)\n",
            "Epoch: [10][500/938]\tBatch Time 0.239 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.1422 (3.0074)\tTop-5 Accuracy 74.684 (78.030)\n",
            "Epoch: [10][600/938]\tBatch Time 0.241 (0.237)\tData Load Time 0.000 (0.000)\tLoss 2.9217 (3.0119)\tTop-5 Accuracy 78.100 (77.977)\n",
            "Epoch: [10][700/938]\tBatch Time 0.234 (0.237)\tData Load Time 0.000 (0.000)\tLoss 2.8253 (3.0173)\tTop-5 Accuracy 81.383 (77.868)\n",
            "Epoch: [10][800/938]\tBatch Time 0.223 (0.237)\tData Load Time 0.000 (0.000)\tLoss 2.9853 (3.0205)\tTop-5 Accuracy 80.112 (77.776)\n",
            "Epoch: [10][900/938]\tBatch Time 0.222 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.0488 (3.0219)\tTop-5 Accuracy 78.386 (77.743)\n",
            "Validation: [0/157]\tBatch Time 0.312 (0.312)\tLoss 3.6228 (3.6228)\tTop-5 Accuracy 68.901 (68.901)\t\n",
            "Validation: [100/157]\tBatch Time 0.148 (0.151)\tLoss 3.8680 (3.6967)\tTop-5 Accuracy 66.150 (68.509)\t\n",
            "\n",
            " * LOSS - 3.688, TOP-5 ACCURACY - 68.624, BLEU-4 - 0.15102465333208615\n",
            "\n",
            "\n",
            "Epochs since last improvement: 2\n",
            "\n",
            "epoch : 10 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            " *** Epoch : 11\n",
            "Epoch: [11][0/938]\tBatch Time 0.438 (0.438)\tData Load Time 0.160 (0.160)\tLoss 2.8804 (2.8804)\tTop-5 Accuracy 80.332 (80.332)\n",
            "Epoch: [11][100/938]\tBatch Time 0.225 (0.239)\tData Load Time 0.000 (0.002)\tLoss 2.9169 (2.9162)\tTop-5 Accuracy 80.620 (79.489)\n",
            "Epoch: [11][200/938]\tBatch Time 0.240 (0.238)\tData Load Time 0.000 (0.001)\tLoss 2.7012 (2.9300)\tTop-5 Accuracy 83.465 (79.299)\n",
            "Epoch: [11][300/938]\tBatch Time 0.237 (0.237)\tData Load Time 0.000 (0.001)\tLoss 3.0200 (2.9316)\tTop-5 Accuracy 78.478 (79.184)\n",
            "Epoch: [11][400/938]\tBatch Time 0.235 (0.237)\tData Load Time 0.000 (0.001)\tLoss 2.8177 (2.9398)\tTop-5 Accuracy 81.285 (79.066)\n",
            "Epoch: [11][500/938]\tBatch Time 0.243 (0.237)\tData Load Time 0.000 (0.000)\tLoss 2.9136 (2.9444)\tTop-5 Accuracy 80.159 (78.982)\n",
            "Epoch: [11][600/938]\tBatch Time 0.235 (0.237)\tData Load Time 0.000 (0.000)\tLoss 2.9946 (2.9507)\tTop-5 Accuracy 78.273 (78.877)\n",
            "Epoch: [11][700/938]\tBatch Time 0.222 (0.237)\tData Load Time 0.000 (0.000)\tLoss 2.8225 (2.9535)\tTop-5 Accuracy 81.892 (78.817)\n",
            "Epoch: [11][800/938]\tBatch Time 0.265 (0.237)\tData Load Time 0.000 (0.000)\tLoss 2.9161 (2.9567)\tTop-5 Accuracy 80.893 (78.759)\n",
            "Epoch: [11][900/938]\tBatch Time 0.251 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.0721 (2.9606)\tTop-5 Accuracy 75.174 (78.653)\n",
            "Validation: [0/157]\tBatch Time 0.300 (0.300)\tLoss 3.6530 (3.6530)\tTop-5 Accuracy 71.512 (71.512)\t\n",
            "Validation: [100/157]\tBatch Time 0.145 (0.151)\tLoss 3.6969 (3.7084)\tTop-5 Accuracy 71.658 (68.416)\t\n",
            "\n",
            " * LOSS - 3.714, TOP-5 ACCURACY - 68.364, BLEU-4 - 0.14637629109033673\n",
            "\n",
            "\n",
            "Epochs since last improvement: 3\n",
            "\n",
            "epoch : 11 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            " *** Epoch : 12\n",
            "Epoch: [12][0/938]\tBatch Time 0.412 (0.412)\tData Load Time 0.159 (0.159)\tLoss 2.7023 (2.7023)\tTop-5 Accuracy 84.211 (84.211)\n",
            "Epoch: [12][100/938]\tBatch Time 0.237 (0.239)\tData Load Time 0.000 (0.002)\tLoss 2.9445 (2.8524)\tTop-5 Accuracy 77.810 (80.421)\n",
            "Epoch: [12][200/938]\tBatch Time 0.251 (0.238)\tData Load Time 0.000 (0.001)\tLoss 2.7866 (2.8618)\tTop-5 Accuracy 83.727 (80.227)\n",
            "Epoch: [12][300/938]\tBatch Time 0.224 (0.238)\tData Load Time 0.000 (0.001)\tLoss 3.0266 (2.8649)\tTop-5 Accuracy 79.570 (80.233)\n",
            "Epoch: [12][400/938]\tBatch Time 0.235 (0.238)\tData Load Time 0.000 (0.001)\tLoss 2.9486 (2.8726)\tTop-5 Accuracy 79.241 (80.146)\n",
            "Epoch: [12][500/938]\tBatch Time 0.257 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.9781 (2.8793)\tTop-5 Accuracy 79.897 (80.055)\n",
            "Epoch: [12][600/938]\tBatch Time 0.229 (0.237)\tData Load Time 0.000 (0.000)\tLoss 2.9157 (2.8845)\tTop-5 Accuracy 79.620 (79.948)\n",
            "Epoch: [12][700/938]\tBatch Time 0.234 (0.237)\tData Load Time 0.000 (0.000)\tLoss 3.0589 (2.8893)\tTop-5 Accuracy 77.612 (79.880)\n",
            "Epoch: [12][800/938]\tBatch Time 0.248 (0.237)\tData Load Time 0.000 (0.000)\tLoss 2.8213 (2.8918)\tTop-5 Accuracy 81.030 (79.829)\n",
            "Epoch: [12][900/938]\tBatch Time 0.233 (0.237)\tData Load Time 0.000 (0.000)\tLoss 2.9084 (2.8962)\tTop-5 Accuracy 77.714 (79.764)\n",
            "Validation: [0/157]\tBatch Time 0.325 (0.325)\tLoss 3.9615 (3.9615)\tTop-5 Accuracy 66.489 (66.489)\t\n",
            "Validation: [100/157]\tBatch Time 0.164 (0.153)\tLoss 3.7679 (3.7370)\tTop-5 Accuracy 69.524 (68.248)\t\n",
            "\n",
            " * LOSS - 3.724, TOP-5 ACCURACY - 68.423, BLEU-4 - 0.15102058672491872\n",
            "\n",
            "\n",
            "Epochs since last improvement: 4\n",
            "\n",
            "epoch : 12 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            " *** Epoch : 13\n",
            "Epoch: [13][0/938]\tBatch Time 0.439 (0.439)\tData Load Time 0.164 (0.164)\tLoss 2.8402 (2.8402)\tTop-5 Accuracy 79.810 (79.810)\n",
            "Epoch: [13][100/938]\tBatch Time 0.247 (0.238)\tData Load Time 0.000 (0.002)\tLoss 2.9044 (2.7958)\tTop-5 Accuracy 79.714 (81.322)\n",
            "Epoch: [13][200/938]\tBatch Time 0.230 (0.238)\tData Load Time 0.000 (0.001)\tLoss 2.8038 (2.8072)\tTop-5 Accuracy 82.749 (81.269)\n",
            "Epoch: [13][300/938]\tBatch Time 0.236 (0.238)\tData Load Time 0.000 (0.001)\tLoss 2.7678 (2.8114)\tTop-5 Accuracy 82.653 (81.192)\n",
            "Epoch: [13][400/938]\tBatch Time 0.210 (0.238)\tData Load Time 0.000 (0.001)\tLoss 2.6945 (2.8170)\tTop-5 Accuracy 81.928 (81.061)\n",
            "Epoch: [13][500/938]\tBatch Time 0.253 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.9800 (2.8175)\tTop-5 Accuracy 80.506 (81.036)\n",
            "Epoch: [13][600/938]\tBatch Time 0.234 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.9444 (2.8271)\tTop-5 Accuracy 79.679 (80.855)\n",
            "Epoch: [13][700/938]\tBatch Time 0.230 (0.237)\tData Load Time 0.000 (0.000)\tLoss 2.8454 (2.8308)\tTop-5 Accuracy 80.585 (80.789)\n",
            "Epoch: [13][800/938]\tBatch Time 0.230 (0.237)\tData Load Time 0.000 (0.000)\tLoss 2.9610 (2.8342)\tTop-5 Accuracy 77.027 (80.734)\n",
            "Epoch: [13][900/938]\tBatch Time 0.248 (0.237)\tData Load Time 0.000 (0.000)\tLoss 2.9172 (2.8409)\tTop-5 Accuracy 75.979 (80.603)\n",
            "Validation: [0/157]\tBatch Time 0.303 (0.303)\tLoss 3.4347 (3.4347)\tTop-5 Accuracy 70.000 (70.000)\t\n",
            "Validation: [100/157]\tBatch Time 0.146 (0.152)\tLoss 3.5184 (3.7377)\tTop-5 Accuracy 70.994 (68.182)\t\n",
            "\n",
            " * LOSS - 3.735, TOP-5 ACCURACY - 68.299, BLEU-4 - 0.15044528296746212\n",
            "\n",
            "\n",
            "Epochs since last improvement: 5\n",
            "\n",
            "epoch : 13 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            " *** Epoch : 14\n",
            "Epoch: [14][0/938]\tBatch Time 0.406 (0.406)\tData Load Time 0.170 (0.170)\tLoss 2.7033 (2.7033)\tTop-5 Accuracy 83.651 (83.651)\n",
            "Epoch: [14][100/938]\tBatch Time 0.257 (0.238)\tData Load Time 0.000 (0.002)\tLoss 2.9806 (2.7316)\tTop-5 Accuracy 78.502 (82.501)\n",
            "Epoch: [14][200/938]\tBatch Time 0.222 (0.238)\tData Load Time 0.000 (0.001)\tLoss 2.7372 (2.7437)\tTop-5 Accuracy 82.133 (82.166)\n",
            "Epoch: [14][300/938]\tBatch Time 0.223 (0.238)\tData Load Time 0.000 (0.001)\tLoss 2.7823 (2.7552)\tTop-5 Accuracy 80.161 (81.951)\n",
            "Epoch: [14][400/938]\tBatch Time 0.224 (0.237)\tData Load Time 0.000 (0.001)\tLoss 2.7674 (2.7595)\tTop-5 Accuracy 80.223 (81.881)\n",
            "Epoch: [14][500/938]\tBatch Time 0.254 (0.237)\tData Load Time 0.000 (0.000)\tLoss 2.8390 (2.7680)\tTop-5 Accuracy 81.748 (81.745)\n",
            "Epoch: [14][600/938]\tBatch Time 0.231 (0.237)\tData Load Time 0.000 (0.000)\tLoss 2.8063 (2.7750)\tTop-5 Accuracy 81.771 (81.627)\n",
            "Epoch: [14][700/938]\tBatch Time 0.245 (0.237)\tData Load Time 0.000 (0.000)\tLoss 2.9207 (2.7798)\tTop-5 Accuracy 81.633 (81.567)\n",
            "Epoch: [14][800/938]\tBatch Time 0.226 (0.237)\tData Load Time 0.000 (0.000)\tLoss 2.8238 (2.7854)\tTop-5 Accuracy 82.817 (81.476)\n",
            "Epoch: [14][900/938]\tBatch Time 0.273 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.8937 (2.7901)\tTop-5 Accuracy 79.469 (81.383)\n",
            "Validation: [0/157]\tBatch Time 0.317 (0.317)\tLoss 3.6059 (3.6059)\tTop-5 Accuracy 71.875 (71.875)\t\n",
            "Validation: [100/157]\tBatch Time 0.156 (0.151)\tLoss 3.9191 (3.7412)\tTop-5 Accuracy 65.152 (68.571)\t\n",
            "\n",
            " * LOSS - 3.745, TOP-5 ACCURACY - 68.361, BLEU-4 - 0.14344739418467276\n",
            "\n",
            "\n",
            "Epochs since last improvement: 6\n",
            "\n",
            "epoch : 14 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            " *** Epoch : 15\n",
            "Epoch: [15][0/938]\tBatch Time 0.435 (0.435)\tData Load Time 0.162 (0.162)\tLoss 2.6027 (2.6027)\tTop-5 Accuracy 85.359 (85.359)\n",
            "Epoch: [15][100/938]\tBatch Time 0.236 (0.239)\tData Load Time 0.000 (0.002)\tLoss 2.7411 (2.6807)\tTop-5 Accuracy 83.554 (83.104)\n",
            "Epoch: [15][200/938]\tBatch Time 0.262 (0.238)\tData Load Time 0.000 (0.001)\tLoss 2.7980 (2.6916)\tTop-5 Accuracy 83.753 (82.850)\n",
            "Epoch: [15][300/938]\tBatch Time 0.243 (0.237)\tData Load Time 0.000 (0.001)\tLoss 2.8872 (2.7087)\tTop-5 Accuracy 79.115 (82.633)\n",
            "Epoch: [15][400/938]\tBatch Time 0.239 (0.237)\tData Load Time 0.000 (0.001)\tLoss 2.7308 (2.7159)\tTop-5 Accuracy 82.275 (82.591)\n",
            "Epoch: [15][500/938]\tBatch Time 0.229 (0.237)\tData Load Time 0.000 (0.000)\tLoss 2.6623 (2.7221)\tTop-5 Accuracy 85.450 (82.526)\n",
            "Epoch: [15][600/938]\tBatch Time 0.239 (0.237)\tData Load Time 0.000 (0.000)\tLoss 2.5714 (2.7276)\tTop-5 Accuracy 85.753 (82.408)\n",
            "Epoch: [15][700/938]\tBatch Time 0.226 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.6906 (2.7294)\tTop-5 Accuracy 84.319 (82.398)\n",
            "Epoch: [15][800/938]\tBatch Time 0.248 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.6328 (2.7349)\tTop-5 Accuracy 82.005 (82.303)\n",
            "Epoch: [15][900/938]\tBatch Time 0.241 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.7103 (2.7399)\tTop-5 Accuracy 82.940 (82.211)\n",
            "Validation: [0/157]\tBatch Time 0.321 (0.321)\tLoss 3.8618 (3.8618)\tTop-5 Accuracy 67.920 (67.920)\t\n",
            "Validation: [100/157]\tBatch Time 0.150 (0.152)\tLoss 3.7698 (3.7819)\tTop-5 Accuracy 68.123 (68.190)\t\n",
            "\n",
            " * LOSS - 3.773, TOP-5 ACCURACY - 68.212, BLEU-4 - 0.1442160103886197\n",
            "\n",
            "\n",
            "Epochs since last improvement: 7\n",
            "\n",
            "epoch : 15 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            " *** Epoch : 16\n",
            "Epoch: [16][0/938]\tBatch Time 0.430 (0.430)\tData Load Time 0.172 (0.172)\tLoss 2.6180 (2.6180)\tTop-5 Accuracy 84.289 (84.289)\n",
            "Epoch: [16][100/938]\tBatch Time 0.227 (0.238)\tData Load Time 0.000 (0.002)\tLoss 2.7602 (2.6443)\tTop-5 Accuracy 80.000 (83.788)\n",
            "Epoch: [16][200/938]\tBatch Time 0.228 (0.237)\tData Load Time 0.000 (0.001)\tLoss 2.7924 (2.6516)\tTop-5 Accuracy 79.620 (83.622)\n",
            "Epoch: [16][300/938]\tBatch Time 0.251 (0.237)\tData Load Time 0.000 (0.001)\tLoss 2.5293 (2.6611)\tTop-5 Accuracy 88.689 (83.427)\n",
            "Epoch: [16][400/938]\tBatch Time 0.242 (0.238)\tData Load Time 0.000 (0.001)\tLoss 2.6354 (2.6685)\tTop-5 Accuracy 83.029 (83.276)\n",
            "Epoch: [16][500/938]\tBatch Time 0.228 (0.237)\tData Load Time 0.000 (0.000)\tLoss 2.7463 (2.6716)\tTop-5 Accuracy 80.593 (83.230)\n",
            "Epoch: [16][600/938]\tBatch Time 0.231 (0.237)\tData Load Time 0.000 (0.000)\tLoss 2.7293 (2.6770)\tTop-5 Accuracy 81.361 (83.101)\n",
            "Epoch: [16][700/938]\tBatch Time 0.254 (0.237)\tData Load Time 0.000 (0.000)\tLoss 2.6586 (2.6823)\tTop-5 Accuracy 83.033 (83.040)\n",
            "Epoch: [16][800/938]\tBatch Time 0.267 (0.237)\tData Load Time 0.000 (0.000)\tLoss 2.8326 (2.6882)\tTop-5 Accuracy 80.533 (82.927)\n",
            "Epoch: [16][900/938]\tBatch Time 0.248 (0.237)\tData Load Time 0.000 (0.000)\tLoss 2.8910 (2.6924)\tTop-5 Accuracy 80.977 (82.872)\n",
            "Validation: [0/157]\tBatch Time 0.325 (0.325)\tLoss 3.5778 (3.5778)\tTop-5 Accuracy 71.939 (71.939)\t\n",
            "Validation: [100/157]\tBatch Time 0.154 (0.151)\tLoss 3.9968 (3.7839)\tTop-5 Accuracy 64.938 (68.080)\t\n",
            "\n",
            " * LOSS - 3.778, TOP-5 ACCURACY - 68.173, BLEU-4 - 0.14358158077469307\n",
            "\n",
            "\n",
            "Epochs since last improvement: 8\n",
            "\n",
            "epoch : 16 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000200\n",
            "\n",
            " *** Epoch : 17\n",
            "Epoch: [17][0/938]\tBatch Time 0.426 (0.426)\tData Load Time 0.179 (0.179)\tLoss 2.5544 (2.5544)\tTop-5 Accuracy 86.066 (86.066)\n",
            "Epoch: [17][100/938]\tBatch Time 0.226 (0.239)\tData Load Time 0.000 (0.002)\tLoss 2.7612 (2.5787)\tTop-5 Accuracy 81.793 (84.678)\n",
            "Epoch: [17][200/938]\tBatch Time 0.248 (0.238)\tData Load Time 0.000 (0.001)\tLoss 2.5856 (2.5659)\tTop-5 Accuracy 85.638 (84.712)\n",
            "Epoch: [17][300/938]\tBatch Time 0.237 (0.237)\tData Load Time 0.000 (0.001)\tLoss 2.5476 (2.5525)\tTop-5 Accuracy 85.507 (84.966)\n",
            "Epoch: [17][400/938]\tBatch Time 0.227 (0.238)\tData Load Time 0.000 (0.001)\tLoss 2.5704 (2.5564)\tTop-5 Accuracy 83.158 (84.925)\n",
            "Epoch: [17][500/938]\tBatch Time 0.228 (0.238)\tData Load Time 0.000 (0.001)\tLoss 2.6624 (2.5583)\tTop-5 Accuracy 82.698 (84.896)\n",
            "Epoch: [17][600/938]\tBatch Time 0.237 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.5687 (2.5567)\tTop-5 Accuracy 86.053 (84.895)\n",
            "Epoch: [17][700/938]\tBatch Time 0.240 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.5720 (2.5598)\tTop-5 Accuracy 83.245 (84.891)\n",
            "Epoch: [17][800/938]\tBatch Time 0.233 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.7403 (2.5607)\tTop-5 Accuracy 83.957 (84.884)\n",
            "Epoch: [17][900/938]\tBatch Time 0.234 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.5086 (2.5619)\tTop-5 Accuracy 86.016 (84.868)\n",
            "Validation: [0/157]\tBatch Time 0.313 (0.313)\tLoss 3.7726 (3.7726)\tTop-5 Accuracy 68.782 (68.782)\t\n",
            "Validation: [100/157]\tBatch Time 0.148 (0.153)\tLoss 3.6999 (3.7830)\tTop-5 Accuracy 69.689 (68.277)\t\n",
            "\n",
            " * LOSS - 3.800, TOP-5 ACCURACY - 68.036, BLEU-4 - 0.14237267183724245\n",
            "\n",
            "\n",
            "Epochs since last improvement: 9\n",
            "\n",
            "epoch : 17 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            " *** Epoch : 18\n",
            "Epoch: [18][0/938]\tBatch Time 0.441 (0.441)\tData Load Time 0.174 (0.174)\tLoss 2.4579 (2.4579)\tTop-5 Accuracy 87.798 (87.798)\n",
            "Epoch: [18][100/938]\tBatch Time 0.242 (0.236)\tData Load Time 0.000 (0.002)\tLoss 2.4834 (2.4750)\tTop-5 Accuracy 87.158 (86.469)\n",
            "Epoch: [18][200/938]\tBatch Time 0.257 (0.237)\tData Load Time 0.000 (0.001)\tLoss 2.5374 (2.4942)\tTop-5 Accuracy 85.427 (86.079)\n",
            "Epoch: [18][300/938]\tBatch Time 0.244 (0.238)\tData Load Time 0.000 (0.001)\tLoss 2.5148 (2.4986)\tTop-5 Accuracy 86.308 (85.897)\n",
            "Epoch: [18][400/938]\tBatch Time 0.232 (0.238)\tData Load Time 0.000 (0.001)\tLoss 2.4069 (2.5031)\tTop-5 Accuracy 86.096 (85.849)\n",
            "Epoch: [18][500/938]\tBatch Time 0.228 (0.238)\tData Load Time 0.000 (0.001)\tLoss 2.5412 (2.5064)\tTop-5 Accuracy 84.615 (85.814)\n",
            "Epoch: [18][600/938]\tBatch Time 0.261 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.4366 (2.5089)\tTop-5 Accuracy 86.957 (85.770)\n",
            "Epoch: [18][700/938]\tBatch Time 0.257 (0.239)\tData Load Time 0.000 (0.000)\tLoss 2.5002 (2.5117)\tTop-5 Accuracy 86.047 (85.734)\n",
            "Epoch: [18][800/938]\tBatch Time 0.229 (0.239)\tData Load Time 0.000 (0.000)\tLoss 2.5130 (2.5161)\tTop-5 Accuracy 86.753 (85.651)\n",
            "Epoch: [18][900/938]\tBatch Time 0.237 (0.239)\tData Load Time 0.000 (0.000)\tLoss 2.5510 (2.5170)\tTop-5 Accuracy 85.912 (85.666)\n",
            "Validation: [0/157]\tBatch Time 0.306 (0.306)\tLoss 3.7786 (3.7786)\tTop-5 Accuracy 64.820 (64.820)\t\n",
            "Validation: [100/157]\tBatch Time 0.148 (0.152)\tLoss 3.8932 (3.8155)\tTop-5 Accuracy 67.109 (68.083)\t\n",
            "\n",
            " * LOSS - 3.810, TOP-5 ACCURACY - 68.071, BLEU-4 - 0.14188785204014137\n",
            "\n",
            "\n",
            "Epochs since last improvement: 10\n",
            "\n",
            "epoch : 18 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            " *** Epoch : 19\n",
            "Epoch: [19][0/938]\tBatch Time 0.437 (0.437)\tData Load Time 0.172 (0.172)\tLoss 2.4398 (2.4398)\tTop-5 Accuracy 87.105 (87.105)\n",
            "Epoch: [19][100/938]\tBatch Time 0.220 (0.243)\tData Load Time 0.000 (0.002)\tLoss 2.4784 (2.4571)\tTop-5 Accuracy 87.989 (86.591)\n",
            "Epoch: [19][200/938]\tBatch Time 0.237 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.5302 (2.4573)\tTop-5 Accuracy 85.101 (86.568)\n",
            "Epoch: [19][300/938]\tBatch Time 0.222 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.4116 (2.4621)\tTop-5 Accuracy 87.887 (86.533)\n",
            "Epoch: [19][400/938]\tBatch Time 0.236 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.5454 (2.4703)\tTop-5 Accuracy 84.091 (86.392)\n",
            "Epoch: [19][500/938]\tBatch Time 0.237 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.3998 (2.4730)\tTop-5 Accuracy 87.277 (86.336)\n",
            "Epoch: [19][600/938]\tBatch Time 0.264 (0.239)\tData Load Time 0.000 (0.000)\tLoss 2.5506 (2.4754)\tTop-5 Accuracy 85.677 (86.288)\n",
            "Epoch: [19][700/938]\tBatch Time 0.226 (0.239)\tData Load Time 0.000 (0.000)\tLoss 2.5353 (2.4784)\tTop-5 Accuracy 85.028 (86.230)\n",
            "Epoch: [19][800/938]\tBatch Time 0.266 (0.239)\tData Load Time 0.000 (0.000)\tLoss 2.6419 (2.4787)\tTop-5 Accuracy 84.541 (86.224)\n",
            "Epoch: [19][900/938]\tBatch Time 0.226 (0.239)\tData Load Time 0.000 (0.000)\tLoss 2.6078 (2.4806)\tTop-5 Accuracy 85.946 (86.164)\n",
            "Validation: [0/157]\tBatch Time 0.314 (0.314)\tLoss 3.6098 (3.6098)\tTop-5 Accuracy 69.697 (69.697)\t\n",
            "Validation: [100/157]\tBatch Time 0.145 (0.152)\tLoss 3.9816 (3.8159)\tTop-5 Accuracy 66.758 (68.189)\t\n",
            "\n",
            " * LOSS - 3.840, TOP-5 ACCURACY - 67.870, BLEU-4 - 0.14247077997199883\n",
            "\n",
            "\n",
            "Epochs since last improvement: 11\n",
            "\n",
            "epoch : 19 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            " *** Epoch : 20\n",
            "Epoch: [20][0/938]\tBatch Time 0.406 (0.406)\tData Load Time 0.166 (0.166)\tLoss 2.4446 (2.4446)\tTop-5 Accuracy 85.492 (85.492)\n",
            "Epoch: [20][100/938]\tBatch Time 0.223 (0.241)\tData Load Time 0.000 (0.002)\tLoss 2.3878 (2.4110)\tTop-5 Accuracy 87.569 (87.218)\n",
            "Epoch: [20][200/938]\tBatch Time 0.223 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.3628 (2.4136)\tTop-5 Accuracy 87.263 (87.138)\n",
            "Epoch: [20][300/938]\tBatch Time 0.234 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.3407 (2.4190)\tTop-5 Accuracy 91.304 (87.096)\n",
            "Epoch: [20][400/938]\tBatch Time 0.248 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.4101 (2.4280)\tTop-5 Accuracy 86.842 (86.956)\n",
            "Epoch: [20][500/938]\tBatch Time 0.227 (0.238)\tData Load Time 0.000 (0.001)\tLoss 2.6887 (2.4308)\tTop-5 Accuracy 82.540 (86.923)\n",
            "Epoch: [20][600/938]\tBatch Time 0.252 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.4736 (2.4369)\tTop-5 Accuracy 87.471 (86.834)\n",
            "Epoch: [20][700/938]\tBatch Time 0.225 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.4854 (2.4433)\tTop-5 Accuracy 85.714 (86.761)\n",
            "Epoch: [20][800/938]\tBatch Time 0.271 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.5096 (2.4464)\tTop-5 Accuracy 88.315 (86.722)\n",
            "Epoch: [20][900/938]\tBatch Time 0.247 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.3363 (2.4515)\tTop-5 Accuracy 87.113 (86.642)\n",
            "Validation: [0/157]\tBatch Time 0.319 (0.319)\tLoss 4.0981 (4.0981)\tTop-5 Accuracy 66.756 (66.756)\t\n",
            "Validation: [100/157]\tBatch Time 0.141 (0.152)\tLoss 3.7549 (3.8409)\tTop-5 Accuracy 70.231 (67.998)\t\n",
            "\n",
            " * LOSS - 3.854, TOP-5 ACCURACY - 67.871, BLEU-4 - 0.14363784285400868\n",
            "\n",
            "\n",
            "Epochs since last improvement: 12\n",
            "\n",
            "epoch : 20 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            " *** Epoch : 21\n",
            "Epoch: [21][0/938]\tBatch Time 0.443 (0.443)\tData Load Time 0.169 (0.169)\tLoss 2.4913 (2.4913)\tTop-5 Accuracy 87.634 (87.634)\n",
            "Epoch: [21][100/938]\tBatch Time 0.253 (0.241)\tData Load Time 0.000 (0.002)\tLoss 2.5300 (2.4174)\tTop-5 Accuracy 84.804 (87.317)\n",
            "Epoch: [21][200/938]\tBatch Time 0.251 (0.240)\tData Load Time 0.000 (0.001)\tLoss 2.4437 (2.4109)\tTop-5 Accuracy 85.027 (87.383)\n",
            "Epoch: [21][300/938]\tBatch Time 0.223 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.4510 (2.4124)\tTop-5 Accuracy 87.252 (87.304)\n",
            "Epoch: [21][400/938]\tBatch Time 0.230 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.5389 (2.4122)\tTop-5 Accuracy 85.714 (87.267)\n",
            "Epoch: [21][500/938]\tBatch Time 0.243 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.4646 (2.4148)\tTop-5 Accuracy 85.825 (87.202)\n",
            "Epoch: [21][600/938]\tBatch Time 0.246 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.4370 (2.4164)\tTop-5 Accuracy 86.133 (87.175)\n",
            "Epoch: [21][700/938]\tBatch Time 0.254 (0.239)\tData Load Time 0.000 (0.000)\tLoss 2.3805 (2.4211)\tTop-5 Accuracy 88.489 (87.085)\n",
            "Epoch: [21][800/938]\tBatch Time 0.260 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.4142 (2.4218)\tTop-5 Accuracy 88.636 (87.070)\n",
            "Epoch: [21][900/938]\tBatch Time 0.233 (0.239)\tData Load Time 0.000 (0.000)\tLoss 2.5211 (2.4243)\tTop-5 Accuracy 86.701 (87.053)\n",
            "Validation: [0/157]\tBatch Time 0.327 (0.327)\tLoss 3.8553 (3.8553)\tTop-5 Accuracy 66.667 (66.667)\t\n",
            "Validation: [100/157]\tBatch Time 0.146 (0.152)\tLoss 3.9045 (3.8793)\tTop-5 Accuracy 67.797 (67.198)\t\n",
            "\n",
            " * LOSS - 3.864, TOP-5 ACCURACY - 67.536, BLEU-4 - 0.13852621121473657\n",
            "\n",
            "\n",
            "Epochs since last improvement: 13\n",
            "\n",
            "epoch : 21 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            " *** Epoch : 22\n",
            "Epoch: [22][0/938]\tBatch Time 0.447 (0.447)\tData Load Time 0.166 (0.166)\tLoss 2.6582 (2.6582)\tTop-5 Accuracy 82.741 (82.741)\n",
            "Epoch: [22][100/938]\tBatch Time 0.238 (0.240)\tData Load Time 0.000 (0.002)\tLoss 2.4532 (2.3808)\tTop-5 Accuracy 85.577 (87.617)\n",
            "Epoch: [22][200/938]\tBatch Time 0.241 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.4221 (2.3711)\tTop-5 Accuracy 86.598 (87.808)\n",
            "Epoch: [22][300/938]\tBatch Time 0.241 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.3668 (2.3778)\tTop-5 Accuracy 86.016 (87.691)\n",
            "Epoch: [22][400/938]\tBatch Time 0.257 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.4157 (2.3795)\tTop-5 Accuracy 88.060 (87.707)\n",
            "Epoch: [22][500/938]\tBatch Time 0.216 (0.239)\tData Load Time 0.000 (0.000)\tLoss 2.5375 (2.3859)\tTop-5 Accuracy 85.714 (87.621)\n",
            "Epoch: [22][600/938]\tBatch Time 0.223 (0.239)\tData Load Time 0.000 (0.000)\tLoss 2.4302 (2.3918)\tTop-5 Accuracy 87.052 (87.528)\n",
            "Epoch: [22][700/938]\tBatch Time 0.230 (0.239)\tData Load Time 0.000 (0.000)\tLoss 2.3073 (2.3938)\tTop-5 Accuracy 88.919 (87.490)\n",
            "Epoch: [22][800/938]\tBatch Time 0.244 (0.239)\tData Load Time 0.000 (0.000)\tLoss 2.4940 (2.3972)\tTop-5 Accuracy 84.800 (87.425)\n",
            "Epoch: [22][900/938]\tBatch Time 0.235 (0.239)\tData Load Time 0.000 (0.000)\tLoss 2.4984 (2.3995)\tTop-5 Accuracy 87.863 (87.416)\n",
            "Validation: [0/157]\tBatch Time 0.322 (0.322)\tLoss 4.0485 (4.0485)\tTop-5 Accuracy 65.909 (65.909)\t\n",
            "Validation: [100/157]\tBatch Time 0.143 (0.154)\tLoss 3.7473 (3.8583)\tTop-5 Accuracy 69.795 (67.268)\t\n",
            "\n",
            " * LOSS - 3.869, TOP-5 ACCURACY - 67.318, BLEU-4 - 0.14134628862966978\n",
            "\n",
            "\n",
            "Epochs since last improvement: 14\n",
            "\n",
            "epoch : 22 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            " *** Epoch : 23\n",
            "Epoch: [23][0/938]\tBatch Time 0.432 (0.432)\tData Load Time 0.162 (0.162)\tLoss 2.5247 (2.5247)\tTop-5 Accuracy 85.279 (85.279)\n",
            "Epoch: [23][100/938]\tBatch Time 0.249 (0.239)\tData Load Time 0.000 (0.002)\tLoss 2.3149 (2.3362)\tTop-5 Accuracy 87.192 (88.152)\n",
            "Epoch: [23][200/938]\tBatch Time 0.246 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.1482 (2.3424)\tTop-5 Accuracy 91.436 (88.105)\n",
            "Epoch: [23][300/938]\tBatch Time 0.248 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.5476 (2.3463)\tTop-5 Accuracy 85.536 (88.105)\n",
            "Epoch: [23][400/938]\tBatch Time 0.245 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.3585 (2.3517)\tTop-5 Accuracy 87.198 (88.023)\n",
            "Epoch: [23][500/938]\tBatch Time 0.259 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.3039 (2.3565)\tTop-5 Accuracy 87.348 (87.991)\n",
            "Epoch: [23][600/938]\tBatch Time 0.238 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.4835 (2.3590)\tTop-5 Accuracy 87.500 (87.968)\n",
            "Epoch: [23][700/938]\tBatch Time 0.245 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.4660 (2.3660)\tTop-5 Accuracy 87.268 (87.868)\n",
            "Epoch: [23][800/938]\tBatch Time 0.221 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.2922 (2.3698)\tTop-5 Accuracy 89.429 (87.797)\n",
            "Epoch: [23][900/938]\tBatch Time 0.231 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.3357 (2.3724)\tTop-5 Accuracy 89.315 (87.764)\n",
            "Validation: [0/157]\tBatch Time 0.310 (0.310)\tLoss 3.8450 (3.8450)\tTop-5 Accuracy 69.576 (69.576)\t\n",
            "Validation: [100/157]\tBatch Time 0.151 (0.152)\tLoss 3.9863 (3.8804)\tTop-5 Accuracy 67.838 (67.631)\t\n",
            "\n",
            " * LOSS - 3.900, TOP-5 ACCURACY - 67.430, BLEU-4 - 0.14215122783764164\n",
            "\n",
            "\n",
            "Epochs since last improvement: 15\n",
            "\n",
            "epoch : 23 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            " *** Epoch : 24\n",
            "Epoch: [24][0/938]\tBatch Time 0.408 (0.408)\tData Load Time 0.169 (0.169)\tLoss 2.2676 (2.2676)\tTop-5 Accuracy 89.014 (89.014)\n",
            "Epoch: [24][100/938]\tBatch Time 0.238 (0.240)\tData Load Time 0.000 (0.002)\tLoss 2.4810 (2.3153)\tTop-5 Accuracy 85.979 (88.775)\n",
            "Epoch: [24][200/938]\tBatch Time 0.258 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.3526 (2.3189)\tTop-5 Accuracy 87.685 (88.667)\n",
            "Epoch: [24][300/938]\tBatch Time 0.261 (0.238)\tData Load Time 0.000 (0.001)\tLoss 2.3510 (2.3222)\tTop-5 Accuracy 90.094 (88.518)\n",
            "Epoch: [24][400/938]\tBatch Time 0.239 (0.238)\tData Load Time 0.000 (0.001)\tLoss 2.2625 (2.3260)\tTop-5 Accuracy 89.147 (88.487)\n",
            "Epoch: [24][500/938]\tBatch Time 0.229 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.2536 (2.3317)\tTop-5 Accuracy 89.351 (88.409)\n",
            "Epoch: [24][600/938]\tBatch Time 0.240 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.3310 (2.3359)\tTop-5 Accuracy 86.582 (88.352)\n",
            "Epoch: [24][700/938]\tBatch Time 0.263 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.3810 (2.3421)\tTop-5 Accuracy 86.173 (88.278)\n",
            "Epoch: [24][800/938]\tBatch Time 0.231 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.3562 (2.3449)\tTop-5 Accuracy 87.071 (88.240)\n",
            "Epoch: [24][900/938]\tBatch Time 0.248 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.4486 (2.3482)\tTop-5 Accuracy 86.957 (88.204)\n",
            "Validation: [0/157]\tBatch Time 0.317 (0.317)\tLoss 3.8834 (3.8834)\tTop-5 Accuracy 67.486 (67.486)\t\n",
            "Validation: [100/157]\tBatch Time 0.153 (0.151)\tLoss 3.7899 (3.9221)\tTop-5 Accuracy 69.271 (67.336)\t\n",
            "\n",
            " * LOSS - 3.925, TOP-5 ACCURACY - 67.303, BLEU-4 - 0.1392320917354225\n",
            "\n",
            "\n",
            "Epochs since last improvement: 16\n",
            "\n",
            "epoch : 24 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000100\n",
            "\n",
            " *** Epoch : 25\n",
            "Epoch: [25][0/938]\tBatch Time 0.448 (0.448)\tData Load Time 0.167 (0.167)\tLoss 2.2532 (2.2532)\tTop-5 Accuracy 90.358 (90.358)\n",
            "Epoch: [25][100/938]\tBatch Time 0.232 (0.240)\tData Load Time 0.000 (0.002)\tLoss 2.3101 (2.2742)\tTop-5 Accuracy 89.831 (89.239)\n",
            "Epoch: [25][200/938]\tBatch Time 0.247 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.2568 (2.2636)\tTop-5 Accuracy 90.176 (89.421)\n",
            "Epoch: [25][300/938]\tBatch Time 0.259 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.2882 (2.2687)\tTop-5 Accuracy 89.851 (89.305)\n",
            "Epoch: [25][400/938]\tBatch Time 0.255 (0.238)\tData Load Time 0.000 (0.001)\tLoss 2.2290 (2.2684)\tTop-5 Accuracy 90.120 (89.301)\n",
            "Epoch: [25][500/938]\tBatch Time 0.232 (0.239)\tData Load Time 0.000 (0.000)\tLoss 2.4066 (2.2692)\tTop-5 Accuracy 86.563 (89.322)\n",
            "Epoch: [25][600/938]\tBatch Time 0.222 (0.239)\tData Load Time 0.000 (0.000)\tLoss 2.3038 (2.2695)\tTop-5 Accuracy 87.826 (89.306)\n",
            "Epoch: [25][700/938]\tBatch Time 0.219 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.2642 (2.2705)\tTop-5 Accuracy 90.529 (89.295)\n",
            "Epoch: [25][800/938]\tBatch Time 0.219 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.1784 (2.2732)\tTop-5 Accuracy 89.583 (89.246)\n",
            "Epoch: [25][900/938]\tBatch Time 0.235 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.3299 (2.2752)\tTop-5 Accuracy 89.863 (89.240)\n",
            "Validation: [0/157]\tBatch Time 0.314 (0.314)\tLoss 3.9183 (3.9183)\tTop-5 Accuracy 65.750 (65.750)\t\n",
            "Validation: [100/157]\tBatch Time 0.149 (0.152)\tLoss 3.8468 (3.9319)\tTop-5 Accuracy 69.210 (67.328)\t\n",
            "\n",
            " * LOSS - 3.937, TOP-5 ACCURACY - 67.261, BLEU-4 - 0.1402015459350494\n",
            "\n",
            "\n",
            "Epochs since last improvement: 17\n",
            "\n",
            "epoch : 25 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            " *** Epoch : 26\n",
            "Epoch: [26][0/938]\tBatch Time 0.408 (0.408)\tData Load Time 0.163 (0.163)\tLoss 2.1445 (2.1445)\tTop-5 Accuracy 90.503 (90.503)\n",
            "Epoch: [26][100/938]\tBatch Time 0.233 (0.241)\tData Load Time 0.000 (0.002)\tLoss 2.5022 (2.2385)\tTop-5 Accuracy 85.271 (89.792)\n",
            "Epoch: [26][200/938]\tBatch Time 0.226 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.2266 (2.2405)\tTop-5 Accuracy 88.406 (89.799)\n",
            "Epoch: [26][300/938]\tBatch Time 0.237 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.2922 (2.2429)\tTop-5 Accuracy 89.779 (89.736)\n",
            "Epoch: [26][400/938]\tBatch Time 0.231 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.2573 (2.2415)\tTop-5 Accuracy 88.920 (89.699)\n",
            "Epoch: [26][500/938]\tBatch Time 0.257 (0.239)\tData Load Time 0.000 (0.000)\tLoss 2.1962 (2.2461)\tTop-5 Accuracy 90.864 (89.651)\n",
            "Epoch: [26][600/938]\tBatch Time 0.242 (0.239)\tData Load Time 0.000 (0.000)\tLoss 2.2721 (2.2501)\tTop-5 Accuracy 91.294 (89.582)\n",
            "Epoch: [26][700/938]\tBatch Time 0.242 (0.239)\tData Load Time 0.000 (0.000)\tLoss 2.2957 (2.2513)\tTop-5 Accuracy 89.673 (89.565)\n",
            "Epoch: [26][800/938]\tBatch Time 0.257 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.3469 (2.2537)\tTop-5 Accuracy 88.413 (89.525)\n",
            "Epoch: [26][900/938]\tBatch Time 0.243 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.2281 (2.2550)\tTop-5 Accuracy 90.515 (89.487)\n",
            "Validation: [0/157]\tBatch Time 0.324 (0.324)\tLoss 3.8201 (3.8201)\tTop-5 Accuracy 69.509 (69.509)\t\n",
            "Validation: [100/157]\tBatch Time 0.149 (0.152)\tLoss 4.2390 (3.9415)\tTop-5 Accuracy 63.612 (67.301)\t\n",
            "\n",
            " * LOSS - 3.949, TOP-5 ACCURACY - 67.268, BLEU-4 - 0.1382109163735919\n",
            "\n",
            "\n",
            "Epochs since last improvement: 18\n",
            "\n",
            "epoch : 26 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            " *** Epoch : 27\n",
            "Epoch: [27][0/938]\tBatch Time 0.422 (0.422)\tData Load Time 0.165 (0.165)\tLoss 2.1533 (2.1533)\tTop-5 Accuracy 91.460 (91.460)\n",
            "Epoch: [27][100/938]\tBatch Time 0.231 (0.238)\tData Load Time 0.000 (0.002)\tLoss 2.4393 (2.2227)\tTop-5 Accuracy 85.333 (89.855)\n",
            "Epoch: [27][200/938]\tBatch Time 0.261 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.2259 (2.2246)\tTop-5 Accuracy 88.759 (89.893)\n",
            "Epoch: [27][300/938]\tBatch Time 0.233 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.2461 (2.2297)\tTop-5 Accuracy 90.439 (89.842)\n",
            "Epoch: [27][400/938]\tBatch Time 0.235 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.2053 (2.2310)\tTop-5 Accuracy 90.833 (89.820)\n",
            "Epoch: [27][500/938]\tBatch Time 0.229 (0.239)\tData Load Time 0.000 (0.000)\tLoss 2.3096 (2.2345)\tTop-5 Accuracy 90.349 (89.740)\n",
            "Epoch: [27][600/938]\tBatch Time 0.245 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.1419 (2.2346)\tTop-5 Accuracy 90.633 (89.754)\n",
            "Epoch: [27][700/938]\tBatch Time 0.244 (0.239)\tData Load Time 0.000 (0.000)\tLoss 2.2287 (2.2373)\tTop-5 Accuracy 91.667 (89.734)\n",
            "Epoch: [27][800/938]\tBatch Time 0.254 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.1799 (2.2365)\tTop-5 Accuracy 90.547 (89.772)\n",
            "Epoch: [27][900/938]\tBatch Time 0.228 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.2216 (2.2379)\tTop-5 Accuracy 92.179 (89.757)\n",
            "Validation: [0/157]\tBatch Time 0.332 (0.332)\tLoss 3.9273 (3.9273)\tTop-5 Accuracy 66.931 (66.931)\t\n",
            "Validation: [100/157]\tBatch Time 0.158 (0.154)\tLoss 3.9729 (3.9479)\tTop-5 Accuracy 66.184 (67.138)\t\n",
            "\n",
            " * LOSS - 3.952, TOP-5 ACCURACY - 67.061, BLEU-4 - 0.139162756264546\n",
            "\n",
            "\n",
            "Epochs since last improvement: 19\n",
            "\n",
            "epoch : 27 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
            " *** Epoch : 28\n",
            "Epoch: [28][0/938]\tBatch Time 0.428 (0.428)\tData Load Time 0.164 (0.164)\tLoss 2.2267 (2.2267)\tTop-5 Accuracy 89.548 (89.548)\n",
            "Epoch: [28][100/938]\tBatch Time 0.231 (0.240)\tData Load Time 0.000 (0.002)\tLoss 2.0762 (2.2068)\tTop-5 Accuracy 91.878 (90.089)\n",
            "Epoch: [28][200/938]\tBatch Time 0.252 (0.238)\tData Load Time 0.000 (0.001)\tLoss 2.3492 (2.2098)\tTop-5 Accuracy 88.321 (90.129)\n",
            "Epoch: [28][300/938]\tBatch Time 0.232 (0.238)\tData Load Time 0.000 (0.001)\tLoss 2.1298 (2.2071)\tTop-5 Accuracy 90.191 (90.187)\n",
            "Epoch: [28][400/938]\tBatch Time 0.211 (0.239)\tData Load Time 0.000 (0.001)\tLoss 2.1203 (2.2098)\tTop-5 Accuracy 89.714 (90.127)\n",
            "Epoch: [28][500/938]\tBatch Time 0.252 (0.239)\tData Load Time 0.000 (0.000)\tLoss 2.2190 (2.2147)\tTop-5 Accuracy 90.885 (90.066)\n",
            "Epoch: [28][600/938]\tBatch Time 0.218 (0.239)\tData Load Time 0.000 (0.000)\tLoss 2.1485 (2.2145)\tTop-5 Accuracy 90.704 (90.076)\n",
            "Epoch: [28][700/938]\tBatch Time 0.237 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.1138 (2.2171)\tTop-5 Accuracy 89.863 (90.036)\n",
            "Epoch: [28][800/938]\tBatch Time 0.238 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.0955 (2.2201)\tTop-5 Accuracy 92.506 (90.007)\n",
            "Epoch: [28][900/938]\tBatch Time 0.228 (0.238)\tData Load Time 0.000 (0.000)\tLoss 2.3016 (2.2239)\tTop-5 Accuracy 87.637 (89.937)\n",
            "Validation: [0/157]\tBatch Time 0.328 (0.328)\tLoss 3.9190 (3.9190)\tTop-5 Accuracy 68.148 (68.148)\t\n",
            "Validation: [100/157]\tBatch Time 0.153 (0.152)\tLoss 4.0987 (3.9784)\tTop-5 Accuracy 64.433 (66.774)\t\n",
            "\n",
            " * LOSS - 3.962, TOP-5 ACCURACY - 66.956, BLEU-4 - 0.13681156568328878\n",
            "\n",
            "\n",
            "Epochs since last improvement: 20\n",
            "\n",
            "epoch : 28 Checkpoint saved : checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYkUXWVbsn7T"
      },
      "source": [
        "checkpoint = '/content/BEST_checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar'  # model checkpoint\n",
        "#checkpoint = '/content/gdrive/MyDrive/EVA4P2_S12_ImageCaptioning/BEST_checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pt'\n",
        "#checkpoint = '/content/gdrive/MyDrive/EVA4P2_S12_ImageCaptioning/BEST_checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar'\n",
        "#word_map_file = '/content/data_output/WORDMAP_flickr8k_5_cap_per_img_5_min_word_freq.json'  # word map, ensure it's the same the data was encoded with and the model was trained with\n",
        "word_map_file = '/content/gdrive/MyDrive/EVA4P2_S12_ImageCaptioning/WORDMAP_flickr8k_5_cap_per_img_5_min_word_freq.json'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
        "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
        "\n",
        "# Load model\n",
        "checkpoint = torch.load(checkpoint)\n",
        "decoder = checkpoint['decoder']\n",
        "decoder = decoder.to(device)\n",
        "decoder.eval()\n",
        "encoder = checkpoint['encoder']\n",
        "encoder = encoder.to(device)\n",
        "encoder.eval()\n",
        "\n",
        "# Load word map (word2ix)\n",
        "with open(word_map_file, 'r') as j:\n",
        "    word_map = json.load(j)\n",
        "rev_word_map = {v: k for k, v in word_map.items()}\n",
        "vocab_size = len(word_map)\n",
        "\n",
        "# Normalization transform\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uhd1qPfuUyj3"
      },
      "source": [
        "def evaluate(beam_size):\n",
        "    \"\"\"\n",
        "    Evaluation\n",
        "    :param beam_size: beam size at which to generate captions for evaluation\n",
        "    :return: BLEU-4 score\n",
        "    \"\"\"\n",
        "    # DataLoader\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        CaptionDataset(data_folder, data_name, 'TEST', transform=transforms.Compose([normalize])),\n",
        "        batch_size=1, shuffle=True, num_workers=1, pin_memory=True)\n",
        "\n",
        "    # TODO: Batched Beam Search\n",
        "    # Therefore, do not use a batch_size greater than 1 - IMPORTANT!\n",
        "\n",
        "    # Lists to store references (true captions), and hypothesis (prediction) for each image\n",
        "    # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n",
        "    # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n",
        "    references = list()\n",
        "    hypotheses = list()\n",
        "\n",
        "    # For each image\n",
        "    for i, (image, caps, caplens, allcaps) in enumerate(\n",
        "            tqdm(loader, desc=\"EVALUATING AT BEAM SIZE \" + str(beam_size))):\n",
        "\n",
        "        k = beam_size  # All below comments will be for k = 3\n",
        "\n",
        "        # Move to GPU device, if available\n",
        "        image = image.to(device)  # (1, 3, 256, 256)\n",
        "\n",
        "        # Encode\n",
        "        encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim) -> [1, 14, 14, 2048]\n",
        "        enc_image_size = encoder_out.size(1)  # 14\n",
        "        encoder_dim = encoder_out.size(3)     # 2048    \n",
        "\n",
        "        # Flatten encoding\n",
        "        encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim) -> [1, 196, 2048]\n",
        "        num_pixels = encoder_out.size(1) # 196\n",
        "\n",
        "        # We'll treat the problem as having a batch size of k. Here k = 3\n",
        "        encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim) \n",
        "        # torch.expand expands tensor. Let us say (1, 196, 512) and k=3, then torch.expand will expand it to (3, 196, 512)\n",
        "\n",
        "        '''\n",
        "        Tensor to store top k previous words at each step; now they're just <start>. These 'k_prev_words' will be converted as embeddings & fed to LSTM for next word predictions. \n",
        "        Below step is just initializing with <start> to give to LSTM for first time. \n",
        "        Later they will be populated based on LSTM predictions inside while loop as sequence progresses.\n",
        "        (k, 1) --> [3, 1]. Will have only 2631 at begining which belongs to <start>.\n",
        "        '''\n",
        "        k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  \n",
        "        \n",
        "        '''\n",
        "        Tensor to store top k sequences; now they're just <start>. Will have only [2631] at begining which belongs to <start>.\n",
        "        [[2631],\n",
        "         [2631],\n",
        "         [2631]]\n",
        "        '''\n",
        "        seqs = k_prev_words  # (k, 1) --> [3, 1]. \n",
        "\n",
        "        '''\n",
        "        Tensor to store top k sequences' scores; now they're just 0\n",
        "        [[0.],\n",
        "         [0.],\n",
        "         [0.]]\n",
        "        ''' \n",
        "        top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1) --> [3, 1]\n",
        "\n",
        "        # Lists to store completed sequences and scores\n",
        "        complete_seqs = list()\n",
        "        complete_seqs_scores = list()\n",
        "\n",
        "        # Start decoding\n",
        "        step = 1                                          # step will get incremented by 1 inside while loop till all the sequences hit <end>\n",
        "        h, c = decoder.init_hidden_state(encoder_out)  #[batch_size, decoder_dim], if k = 3 , then [3, 512]\n",
        "\n",
        "        # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
        "        while True:\n",
        "\n",
        "            embeddings = decoder.embedding(k_prev_words).squeeze(1)  \n",
        "            # (s, embed_dim) -> [3, 512]. We are converting k_prev_words(corresponding word_map indexes) to embeddings          \n",
        "\n",
        "            awe, _ = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels). Passing image encoding & previous hidden state to attention.\n",
        "            # Attention will give attention weighted encoding(awe) & alpha(weights).Unlike training here we are interested only in (awe).                                                 \n",
        "\n",
        "            gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
        "            awe = gate * awe  # Passing the awe through sigmoid gate\n",
        "\n",
        "            h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  \n",
        "            # (s, decoder_dim). Calling LSTM. Inputs are embeddings of previously predicted words(k_prev_words), attention weighted encodings(awe), previous h & previous c\n",
        "\n",
        "            scores = decoder.fc(h)  # (s, vocab_size). Gets predictions in the form of scores by passing h through fc layer --> [3, 2633]         \n",
        "            scores = F.log_softmax(scores, dim=1)  # [3, 2633]\n",
        "            # Doing log_softmax on 'scores' across 1st dimension i.e. perform log_softmax operation of 2633 values to widen the gap among these values\n",
        "\n",
        "            # Add\n",
        "            scores = top_k_scores.expand_as(scores) + scores # (s, vocab_size) --> [3, 2633]\n",
        "            # expand_as --> means expand the 'top_k_scores' tensor same as size of 'scores' tensor.\n",
        "            # Here top_k_scores of size [3, 1] is expanded to size of 'scores' which is [3, 2633] and adding cumulatively. From this, top_k_scores are calculated below again.\n",
        "            # This is done to ensure that top scores are kept up-to-date based on latest predictions.        \n",
        "        \n",
        "\n",
        "            # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
        "            # torch.topk(input, k, dim=None, largest=True, sorted=True, *, out=None) -> (Tensor, LongTensor)\n",
        "            # .topk --> (values, indices) is returned, where the indices are the indices of the elements in the original input tensor.\n",
        "            #            values - k largest elements of the given input tensor along a given dimension. If dim is not given, the last dimension of the input is chosen.\n",
        "\n",
        "            if step == 1:\n",
        "                top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
        "            else:\n",
        "                # Unroll and find top scores, and their unrolled indices\n",
        "                top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  \n",
        "                # (s) --> selecting 'k=3' top-scores from 3 * 2633 flat tensor. \n",
        "                   \n",
        "            # Convert unrolled indices to actual indices of scores\n",
        "            # vocab_size -> 2633 i.e. there are 2633 words in vaocabulary. \n",
        "            # If an index comes as 2634, then that means it is 1st index in 2633. i.e. 2633 + 1 = 2634. This is captured in next_word_inds\n",
        "            # Let us say top_k_words = 2689, 5322,   91, then prev_word_inds = [1, 2, 0] and next_word_inds = [56 (2633+56), 56 (2633*2 +56), 91 (0 + 91)]\n",
        "            prev_word_inds = top_k_words / vocab_size  # (s) --> [3]\n",
        "            next_word_inds = top_k_words % vocab_size  # (s) --> [3]\n",
        " \n",
        "            '''\n",
        "            Add new words to sequences. Adding 'next_word_inds' that we got based on 'top_k_words' to already existing sequence. Example as below:\n",
        "            [[2631,    56, 99],\n",
        "             [2631,    56, 89],\n",
        "             [2631,    91, 10]]\n",
        "\n",
        "            Here [2631, 2631, 2631] is what we started with. We got [56, 56, 91] on step =1 & [99, 89, 10] on step =2 and so on....  \n",
        "            '''  \n",
        "            seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
        "      \n",
        "            '''\n",
        "            Which sequences are incomplete (didn't reach <end>)? Capturing indexes of incomplete sequences. eg: [0, 1, 2] for k =3 means all three sequences are incomplete\n",
        "            as any of them didnt hit <end> so far. If <end> i.e. 2632 is encountered that index is removed from sequence. Let us say '1' hit <end> , then \n",
        "            incomplete_inds will eliminate 1 and keep only [0,2]\n",
        "            '''\n",
        "            incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
        "                               next_word != word_map['<end>']]\n",
        "       \n",
        "            '''       \n",
        "            List to capture index of completed sequences i.e. sequences which reached <end>. aka 2632\n",
        "            Let us say out of [0, 1, 2], sequence 1 hit <end>. Then 'incomplete_inds' based on above logic will get updated as [0, 2] whereas 'next_word_inds' will\n",
        "            continue to have 3 elements in it. So 'set(range(len(next_word_inds)))' will be {0, 1, 2}. 'set(incomplete_inds)' will {0, 1}.\n",
        "            Difference between these two will exacly give the index of completed sequence ie 1 & will get captured in 'complete_inds' list as [1]\n",
        "            '''\n",
        "            complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds)) \n",
        "\n",
        "            # Set aside complete sequences\n",
        "            if len(complete_inds) > 0:\n",
        "                complete_seqs.extend(seqs[complete_inds].tolist()) # Fetches completed sequence from 'seqs' using 'complete_inds' index and adds it to 'complete_seqs'\n",
        "                complete_seqs_scores.extend(top_k_scores[complete_inds]) # Takes score of completed sequence from 'top_k_scores' and adds it to 'complete_seqs_scores'\n",
        "\n",
        "            k -= len(complete_inds)  # reduce beam length accordingly. If 'k =3' & one of the sequence hit <end>, then 'k' will be reduced by 1 to make 'k = 2'\n",
        "\n",
        "            # Proceed with incomplete sequences\n",
        "            if k == 0:\n",
        "                break\n",
        "            \n",
        "            '''\n",
        "            Updating 'seqs' with 'incomplete_inds'. In otherwords, taking out completed sequences from 'seqs'. Similarly 'h' and 'c' also downsized.\n",
        "            Let us say size of 'seqs' was [3, 13]. This means there are 3 sequences and each sequence has 13 words captured so far.\n",
        "            Let us say all these 3 sequences are incomplete ie. [0, 1, 2] indexes of 'seqs' are incomplete. Now imagine 13th word for 1st index is <end> i.e. 2632.\n",
        "            Since it hit <end>, we will conside this index '1' as complete. Accordingly 'seqs' also needs to be updated as there is no point in further predicting words for \n",
        "            the sequence belonging to index 1 as it reached its <end>. Below step updates 'seqs\" by removing completed indexes, in this example 1. So new size of 'seqs'\n",
        "            will be [2, 13] and there will only be 2 sequences remaining for further processing as they didnt hit <end> so far.\n",
        "            Similarly we need to take out this completed sequence from 'h' & 'c' also as we dont need to feed it again to LSTM for prediction.\n",
        "            'h' will be updated from [3, 512] -> [2, 512]\n",
        "            'c' will be updated from [3, 512] -> [3, 512]\n",
        "            Also we need to remove this from encodings, top_k_scores, next_word_inds also. 'encoder_out' will be updated from [3, 196, 2048] -> [2, 196, 2048].\n",
        "            'top_k_scores' from [3] to [2]. 'next_word_inds' from [3] to [2].\n",
        "            Also we need to update 'k_prev_words' too. Because we are passing embeddings of 'k_prev_words' to LSTM to make predictions. So previous word that belongs to \n",
        "            completed sequence needs to be removed. We do this copying over 'next_word_inds' belonging to 'incomplete_inds [0, 2]' to 'k_prev_words' as below:\n",
        "            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
        "            Accordingly, size of 'k_prev_words' will be updated from [3,1] to [2,1]           \n",
        "            '''\n",
        "             \n",
        "            seqs = seqs[incomplete_inds]        \n",
        "            h    = h[prev_word_inds[incomplete_inds]]\n",
        "            c    = c[prev_word_inds[incomplete_inds]]\n",
        "            encoder_out  = encoder_out[prev_word_inds[incomplete_inds]]\n",
        "            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
        "            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
        "             \n",
        "            # Break if things have been going on too long. Something should be wrong if we didn't hit <end> even after 50 steps. After all we are creating image captions\n",
        "            # here, not writing description of image.\n",
        "            if step > 50:\n",
        "                break\n",
        "            step += 1   # Incrementing step\n",
        "\n",
        "        # complete_seqs_scores      --> This List stores the scores belonging to each sequence for each iteration. Gets reset with each iteration also.\n",
        "        # max(complete_seqs_scores) --> After a particular iteration is completed, takes maximum from 'complete_seqs_scores'.\n",
        "        # j = complete_seqs_scores.index(max(complete_seqs_scores)) --> We are finding index of 'max(complete_seqs_scores)' and capturing it in 'j'.\n",
        " \n",
        "        j = complete_seqs_scores.index(max(complete_seqs_scores))     \n",
        "        \n",
        "        # Using the 'max score index - j' take the sequence of that index from 'complete_seqs' and stores it in 'seq'. \n",
        "        # Please note that 'seq' will get overwritten with each iteration 'i'\n",
        "        seq = complete_seqs[j]          \n",
        "               \n",
        "        # References are captured. Same logic as we used in 'validation'. These are ground truths we got from input dataset. All 5 captions per image will be stored for bleu score.\n",
        "        img_caps = allcaps[0].tolist()\n",
        "        img_captions = list(\n",
        "            map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}],\n",
        "                img_caps))  # remove <start> and pads\n",
        "        references.append(img_captions)\n",
        "\n",
        "        # Hypotheses are captured using 'seq' we populated above. These are predictions we got from network.\n",
        "        hypotheses.append([w for w in seq if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}])\n",
        "\n",
        "        assert len(references) == len(hypotheses)\n",
        "\n",
        "    # Calculate BLEU-4 scores\n",
        "    bleu4 = corpus_bleu(references, hypotheses)\n",
        "\n",
        "    return bleu4"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P40u7NqXi5K5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92f2221c-2c52-41a7-eccb-1d9f3acc323d"
      },
      "source": [
        "beam_size = 3\n",
        "print(\"\\nBLEU-4 score @ beam size of %d is %.4f.\" % (beam_size, evaluate(beam_size)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EVALUATING AT BEAM SIZE 3:   0%|          | 0/5000 [00:00<?, ?it/s]/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n",
            "EVALUATING AT BEAM SIZE 3: 100%|██████████| 5000/5000 [03:53<00:00, 21.39it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "BLEU-4 score @ beam size of 3 is 0.2106.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsAmWzkMNmoZ",
        "outputId": "f7ffb095-b872-493a-b090-91e78e4bae80"
      },
      "source": [
        "x = torch.rand(3,4,2)\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.7775, 0.0470],\n",
              "         [0.7418, 0.5029],\n",
              "         [0.0276, 0.7762],\n",
              "         [0.8285, 0.5989]],\n",
              "\n",
              "        [[0.2478, 0.3601],\n",
              "         [0.1465, 0.4953],\n",
              "         [0.5763, 0.5703],\n",
              "         [0.2093, 0.9217]],\n",
              "\n",
              "        [[0.5986, 0.6990],\n",
              "         [0.1599, 0.9496],\n",
              "         [0.0229, 0.8896],\n",
              "         [0.6324, 0.6696]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXwazaPhtpC2"
      },
      "source": [
        "!cp '/content/BEST_checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar' '/content/gdrive/MyDrive/EVA4P2_S12_ImageCaptioning'"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vwW93yevlCy"
      },
      "source": [
        "!cp '/content/data_output/WORDMAP_flickr8k_5_cap_per_img_5_min_word_freq.json' '/content/gdrive/MyDrive/EVA4P2_S12_ImageCaptioning'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YAMq9M2f24a"
      },
      "source": [
        "# Below steps are to save encoder & decoder models in cpu. These will be later used for cloud based inferencing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-erplLWnvrt7"
      },
      "source": [
        "# BEFORE Running this cell, make sure to reload encoder & decoder using Best_Checkpoint saved 'pth.tar' file\n",
        "torch.save({\n",
        "    \"encoder\": encoder.state_dict(),\n",
        "    \"decoder\": decoder.state_dict()\n",
        "}, \"/content/BEST_checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pt\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1sF7lyNU4oO"
      },
      "source": [
        "!cp '/content/BEST_checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pt' '/content/gdrive/MyDrive/EVA4P2_S12_ImageCaptioning'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXZAWJHoLWo7"
      },
      "source": [
        "encoder_script = torch.jit.script(encoder.to(\"cpu\"))   # Creating 'encoder_script' object to save cpu jitscript model"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nkfl3kBRfY1w"
      },
      "source": [
        "### Defined a new decoder class 'DecoderWithAttention2' to support CPU jitscripted model. Changes compared to 'DecoderWithAttention' can be tracked by searching for 'chg'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9W9576LLdXN"
      },
      "source": [
        "from typing import List      # To give type hint to avoid jitscript error below\n",
        "\n",
        "class DecoderWithAttention2(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        attention_dim,\n",
        "        embed_dim,\n",
        "        decoder_dim,\n",
        "        vocab_size,\n",
        "        encoder_dim=2048,\n",
        "        dropout=0.5\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param attention_dim: size of attention network\n",
        "        :param embed_dim: embedding size\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param vocab_size: size of vocabulary\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param dropout: dropout\n",
        "        \"\"\"\n",
        "        super(DecoderWithAttention2, self).__init__()\n",
        "\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.attention_dim = attention_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attention = Attention(\n",
        "            encoder_dim, decoder_dim, attention_dim\n",
        "        )  # attention network\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
        "        self.dropout = nn.Dropout(p=self.dropout)\n",
        "        self.decode_step = nn.LSTMCell(\n",
        "            embed_dim + encoder_dim, decoder_dim, bias=True\n",
        "        )  # decoding LSTMCell\n",
        "        self.init_h = nn.Linear(\n",
        "            encoder_dim, decoder_dim\n",
        "        )  # linear layer to find initial hidden state of LSTMCell\n",
        "        self.init_c = nn.Linear(\n",
        "            encoder_dim, decoder_dim\n",
        "        )  # linear layer to find initial cell state of LSTMCell\n",
        "        self.f_beta = nn.Linear(\n",
        "            decoder_dim, encoder_dim\n",
        "        )  # linear layer to create a sigmoid-activated gate\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(\n",
        "            decoder_dim, vocab_size\n",
        "        )  # linear layer to find scores over vocabulary\n",
        "        self.init_weights()  # initialize some layers with the uniform distribution\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
        "        \"\"\"\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def load_pretrained_embeddings(self, embeddings):\n",
        "        \"\"\"\n",
        "        Loads embedding layer with pre-trained embeddings.\n",
        "\n",
        "        :param embeddings: pre-trained embeddings\n",
        "        \"\"\"\n",
        "        self.embedding.weight = nn.Parameter(embeddings)\n",
        "\n",
        "    def fine_tune_embeddings(self, fine_tune=True):\n",
        "        \"\"\"\n",
        "        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n",
        "\n",
        "        :param fine_tune: Allow?\n",
        "        \"\"\"\n",
        "        for p in self.embedding.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "\n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        \"\"\"\n",
        "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
        "\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :return: hidden state, cell state\n",
        "        \"\"\"\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)\n",
        "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n",
        "        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
        "        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
        "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = encoder_out.size(0)\n",
        "        encoder_dim = encoder_out.size(-1)\n",
        "        vocab_size = self.vocab_size\n",
        "\n",
        "        # Flatten image\n",
        "        encoder_out = encoder_out.view(\n",
        "            batch_size, -1, encoder_dim\n",
        "        )  # (batch_size, num_pixels, encoder_dim)\n",
        "        num_pixels = encoder_out.size(1)\n",
        "\n",
        "        # Sort input data by decreasing lengths; why? apparent below\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(\n",
        "            dim=0, descending=True\n",
        "        )\n",
        "        encoder_out = encoder_out[sort_ind]\n",
        "        encoded_captions = encoded_captions[sort_ind]\n",
        "\n",
        "        # Embedding\n",
        "        embeddings = self.embedding(\n",
        "            encoded_captions\n",
        "        )  # (batch_size, max_caption_length, embed_dim)\n",
        "\n",
        "        # Initialize LSTM state\n",
        "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
        "\n",
        "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
        "        # So, decoding lengths are actual lengths - 1\n",
        "        decode_lengths: List[int]  = (caption_lengths - 1).tolist()  #chg. Getting jit error of \"Expected type hint for result of tolist():\" hence modified to give type hint\n",
        "\n",
        "        # Create tensors to hold word predicion scores and alphas\n",
        "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size)    #chg(not moving to gpu device() as we done during training, because we are using cpu model)\n",
        "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels)         #chg(not moving to gpu device() as we done during training, because we are using cpu model)\n",
        "\n",
        "        # At each time-step, decode by\n",
        "        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n",
        "        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
        "        for t in range(max(decode_lengths)):\n",
        "          # batch_size_t = sum([l > t for l in decode_lengths])          \n",
        "            batch_size_t = torch.sum(torch.tensor([l > t for l in decode_lengths])).item()  #chg. To avoid below torchscript errors\n",
        "                                                                                         # 'Python builtin <built-in function sum> is currently not supported in Torchscript'\n",
        "            attention_weighted_encoding, alpha = self.attention(\n",
        "                encoder_out[:batch_size_t], h[:batch_size_t]\n",
        "            )\n",
        "            gate = self.sigmoid(\n",
        "                self.f_beta(h[:batch_size_t])\n",
        "            )  # gating scalar, (batch_size_t, encoder_dim)\n",
        "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
        "            h, c = self.decode_step(\n",
        "                torch.cat(\n",
        "                    [embeddings[:batch_size_t, t, :], attention_weighted_encoding],\n",
        "                    dim=1,\n",
        "                ),\n",
        "                (h[:batch_size_t], c[:batch_size_t]),\n",
        "            )  # (batch_size_t, decoder_dim)\n",
        "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
        "            predictions[:batch_size_t, t, :] = preds\n",
        "            alphas[:batch_size_t, t, :] = alpha\n",
        "\n",
        "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-pQIqYNLm2z"
      },
      "source": [
        "decoder2 = DecoderWithAttention2(\n",
        "        attention_dim=attention_dim,\n",
        "        embed_dim=emb_dim,\n",
        "        decoder_dim=decoder_dim,\n",
        "        vocab_size=len(word_map),\n",
        "        dropout=dropout,\n",
        "        encoder_dim=512\n",
        "    ).to(\"cpu\")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlRod4bkLhwF",
        "outputId": "a5509858-b16b-4d1e-ef04-319537aea626"
      },
      "source": [
        "decoder2.load_state_dict(decoder.state_dict())  # loading 'decoder2' using 'decoder' we created earlier while training "
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVdkTTdUMl6B"
      },
      "source": [
        "decoder2_script = torch.jit.script(decoder2.to(\"cpu\")) # Creating 'decoder_script' object to save cpu jitscript model"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDAkh4syLjxd"
      },
      "source": [
        "encoder_script.save(\"Test_flickr8k_caption.encoder.scripted.pt\") # Saving .pt files"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXihxDebMcx3"
      },
      "source": [
        "decoder2_script.save(\"Test_flickr8k_caption.decoder.scripted.pt\") # Saving .pt files"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3T86EqcMgFc"
      },
      "source": [
        "!cp '/content/Test_flickr8k_caption.encoder.scripted.pt' '/content/gdrive/MyDrive/EVA4P2_S12_ImageCaptioning'\n",
        "!cp '/content/Test_flickr8k_caption.decoder.scripted.pt' '/content/gdrive/MyDrive/EVA4P2_S12_ImageCaptioning'"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mup9zwmcmIoW"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkWe7vRgTJ1J"
      },
      "source": [
        "with open(\"/content/gdrive/MyDrive/EVA4P2_S12_ImageCaptioning/WORDMAP_flickr8k_5_cap_per_img_5_min_word_freq.json\", \"r\") as j:\n",
        "    word_map = json.load(j)\n",
        "rev_word_map = {v: k for k, v in word_map.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZA6JHKLjTry-"
      },
      "source": [
        "encoder = torch.jit.load(\"/content/Test_flickr8k_caption.encoder.scripted.pt\")\n",
        "decoder = torch.jit.load(\"/content/Test_flickr8k_caption.decoder.scripted.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSe99NnNT9NH"
      },
      "source": [
        "#! wget -O playing.jpg https://cff2.earth.com/uploads/2018/07/25115124/Kids-now-spend-twice-as-much-time-playing-indoors-than-outdoors.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "MZtt_I_yT_1X",
        "outputId": "d861da28-a92e-45e7-ae3c-79e0f47d61d7"
      },
      "source": [
        "from PIL import Image\n",
        "img = Image.open('jumping_cat.jpg')\n",
        "img = img.resize((450, 450))\n",
        "img"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=450x450 at 0x7F419ADC2F10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpnawE8oUHma"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import json\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "\n",
        "def caption_image_beam_search(encoder, decoder, image_path, word_map, beam_size=3):\n",
        "    \"\"\"\n",
        "    Reads an image and captions it with beam search.\n",
        "\n",
        "    :param encoder: encoder model\n",
        "    :param decoder: decoder model\n",
        "    :param image_path: path to image\n",
        "    :param word_map: word map\n",
        "    :param beam_size: number of sequences to consider at each decode-step\n",
        "    :return: caption, weights for visualization\n",
        "    \"\"\"\n",
        "\n",
        "    k = beam_size\n",
        "    vocab_size = len(word_map)\n",
        "\n",
        "    # Read image and process\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    trans = T.Compose(\n",
        "        [\n",
        "            T.Resize((256, 256)),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    image = trans(img)  # (3, 256, 256)\n",
        "\n",
        "    # Encode\n",
        "    image = image.unsqueeze(0)  # (1, 3, 256, 256)\n",
        "    encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
        "    enc_image_size = encoder_out.size(1)\n",
        "    encoder_dim = encoder_out.size(3)\n",
        "\n",
        "    # Flatten encoding\n",
        "    encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
        "    num_pixels = encoder_out.size(1)\n",
        "\n",
        "    # We'll treat the problem as having a batch size of k\n",
        "    encoder_out = encoder_out.expand(\n",
        "        k, num_pixels, encoder_dim\n",
        "    )  # (k, num_pixels, encoder_dim)\n",
        "\n",
        "    # Tensor to store top k previous words at each step; now they're just <start>\n",
        "    k_prev_words = torch.tensor([[word_map[\"<start>\"]]] * k, dtype=torch.long) # (k, 1)\n",
        "\n",
        "    # Tensor to store top k sequences; now they're just <start>\n",
        "    seqs = k_prev_words  # (k, 1)\n",
        "\n",
        "    # Tensor to store top k sequences' scores; now they're just 0\n",
        "    top_k_scores = torch.zeros(k, 1)  # (k, 1)\n",
        "\n",
        "    # Tensor to store top k sequences' alphas; now they're just 1s\n",
        "    #seqs_alpha = torch.ones(k, 1, enc_image_size, enc_image_size)  # (k, 1, enc_image_size, enc_image_size)\n",
        "\n",
        "    # Lists to store completed sequences, their alphas and scores\n",
        "    complete_seqs = list()\n",
        "    #complete_seqs_alpha = list()\n",
        "    complete_seqs_scores = list()\n",
        "\n",
        "    # Start decoding\n",
        "    step = 1\n",
        "    h, c = decoder.init_hidden_state(encoder_out)\n",
        "\n",
        "    # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
        "    while True:\n",
        "\n",
        "        embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
        "\n",
        "        awe, alpha = decoder.attention(\n",
        "            encoder_out, h\n",
        "        )  # (s, encoder_dim), (s, num_pixels)\n",
        "\n",
        "        '''\n",
        "        alpha = alpha.view(\n",
        "            -1, enc_image_size, enc_image_size\n",
        "        )  # (s, enc_image_size, enc_image_size)\n",
        "        '''\n",
        "\n",
        "        gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
        "        awe = gate * awe\n",
        "\n",
        "        h, c = decoder.decode_step(\n",
        "            torch.cat([embeddings, awe], dim=1), (h, c)\n",
        "        )  # (s, decoder_dim)\n",
        "\n",
        "        scores = decoder.fc(h)  # (s, vocab_size)\n",
        "        scores = F.log_softmax(scores, dim=1)\n",
        "\n",
        "        # Add\n",
        "        scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
        "\n",
        "        # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
        "        if step == 1:\n",
        "            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
        "        else:\n",
        "            # Unroll and find top scores, and their unrolled indices\n",
        "            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
        "\n",
        "        # Convert unrolled indices to actual indices of scores\n",
        "        prev_word_inds = top_k_words // vocab_size  # (s)\n",
        "        next_word_inds = top_k_words % vocab_size  # (s)\n",
        "\n",
        "        # Add new words to sequences, alphas\n",
        "        # import pdb; pdb.set_trace()\n",
        "        seqs = torch.cat(\n",
        "            [seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1\n",
        "        )  # (s, step+1)\n",
        "        #seqs_alpha = torch.cat(\n",
        "        #    [seqs_alpha[prev_word_inds], alpha[prev_word_inds].unsqueeze(1)], dim=1\n",
        "        #)  # (s, step+1, enc_image_size, enc_image_size)\n",
        "\n",
        "        # Which sequences are incomplete (didn't reach <end>)?\n",
        "        incomplete_inds = [\n",
        "            ind\n",
        "            for ind, next_word in enumerate(next_word_inds)\n",
        "            if next_word != word_map[\"<end>\"]\n",
        "        ]\n",
        "        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
        "\n",
        "        # Set aside complete sequences\n",
        "        if len(complete_inds) > 0:\n",
        "            complete_seqs.extend(seqs[complete_inds].tolist())\n",
        "            #complete_seqs_alpha.extend(seqs_alpha[complete_inds].tolist())\n",
        "            complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
        "        k -= len(complete_inds)  # reduce beam length accordingly\n",
        "\n",
        "        # Proceed with incomplete sequences\n",
        "        if k == 0:\n",
        "            break\n",
        "        seqs = seqs[incomplete_inds]\n",
        "        #seqs_alpha = seqs_alpha[incomplete_inds]\n",
        "        h = h[prev_word_inds[incomplete_inds]]\n",
        "        c = c[prev_word_inds[incomplete_inds]]\n",
        "        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
        "        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
        "        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
        "\n",
        "        # Break if things have been going on too long\n",
        "        if step > 50:\n",
        "            break\n",
        "        step += 1\n",
        "\n",
        "    i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
        "    seq = complete_seqs[i]\n",
        "    #alphas = complete_seqs_alpha[i]\n",
        "\n",
        "    #return seq, alphas\n",
        "    return seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWsxErb_UlCK",
        "outputId": "8cac28a7-8522-425c-a82a-e276625b4a48"
      },
      "source": [
        "with torch.no_grad():\n",
        "    seq = caption_image_beam_search(encoder, decoder, \"jumping_cat.jpg\", word_map, beam_size=3)\n",
        "seq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2631, 1, 7, 3, 4, 5, 13, 9, 10, 2632]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HQS3fVE5Uq64",
        "outputId": "c7aafb39-fb29-46c2-ea81-ddbf1b57db9f"
      },
      "source": [
        "words = \" \".join([rev_word_map[ind] for ind in seq if ind not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}])\n",
        "words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'a white dog is running through the snow'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98plkR-1UwAe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
