{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "EVA4P2_S12_ImageCaption_V1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anilbhatt1/Deep_Learning_EVA4_Phase2/blob/master/EVA4P2_S12_ImageCaption_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlGk2nuq4ZCr",
        "outputId": "46954375-5ff7-4c6c-adfb-47532111860c"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat May 15 18:00:43 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiAN4b6Qtrne"
      },
      "source": [
        "### Reference : https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning\n",
        "This notebook is based on above reference. It uses encoder, decoder, attention, data creation, train, validation & test methods exactly as present in the above reference. For understanding the code, an index called 'px' is introduced in decoder and attention. This is to enable display statements based on the number of iterations while training & validations. These dispaly statements will help to understand the network better. Code below is heavily commented at each line.\n",
        "Encoder is using pre-trained RESNET 101 as used in reference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95gzlYvniAPz",
        "outputId": "99b69337-1b3c-4d44-890b-7ee435731498"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_5J-2DRZ8Co",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09f2c7c1-1199-4421-bcf6-9e5d44e3374a"
      },
      "source": [
        "!pip install torch==1.5.0+cu92 torchvision==0.6.0+cu92 torchtext==0.6.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.0+cu92\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu92/torch-1.5.0%2Bcu92-cp37-cp37m-linux_x86_64.whl (603.7MB)\n",
            "\u001b[K     |████████████████████████████████| 603.7MB 30kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.0+cu92\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu92/torchvision-0.6.0%2Bcu92-cp37-cp37m-linux_x86_64.whl (6.5MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5MB 52.0MB/s \n",
            "\u001b[?25hCollecting torchtext==0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/17/e7c588245aece7aa93f360894179374830daf60d7ed0bbb59332de3b3b61/torchtext-0.6.0-py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.5.0+cu92) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5.0+cu92) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.6.0+cu92) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 40.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Installing collected packages: torch, torchvision, sentencepiece, torchtext\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "  Found existing installation: torchvision 0.9.1+cu101\n",
            "    Uninstalling torchvision-0.9.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.1+cu101\n",
            "  Found existing installation: torchtext 0.9.1\n",
            "    Uninstalling torchtext-0.9.1:\n",
            "      Successfully uninstalled torchtext-0.9.1\n",
            "Successfully installed sentencepiece-0.1.95 torch-1.5.0+cu92 torchtext-0.6.0 torchvision-0.6.0+cu92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_3Y4xuqZ_8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29321d0f-6ff3-4f0c-e938-c2f7cc3a5e02"
      },
      "source": [
        "pip install scipy==1.1.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scipy==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/de/0c22c6754370ba6b1fa8e53bd6e514d4a41a181125d405a501c215cbdbd6/scipy-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (31.2MB)\n",
            "\u001b[K     |████████████████████████████████| 31.2MB 98kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.1.0) (1.19.5)\n",
            "\u001b[31mERROR: pymc3 3.11.2 has requirement scipy>=1.2.0, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement scipy>=1.2.0, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "Successfully installed scipy-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teyHqGVbqdfJ",
        "outputId": "ba0a5797-67d7-4dda-8a21-60caa8636a09"
      },
      "source": [
        "import zipfile\n",
        "from zipfile import ZipFile\n",
        "from time import time\n",
        "from datetime import datetime \n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torch.utils.data\n",
        "import torchvision.utils as utils\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import numpy as np\n",
        "print('Pytorch version:', torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pytorch version: 1.5.0+cu92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUny_bqyoWiw"
      },
      "source": [
        "!unzip -q '/content/gdrive/MyDrive/EVA4P2_S12_ImageCaptioning/Flickr8k_Dataset.zip'\n",
        "!unzip -q '/content/gdrive/MyDrive/EVA4P2_S12_ImageCaptioning/Flickr8k_text.zip'\n",
        "!unzip -q '/content/gdrive/MyDrive/EVA4P2_S12_ImageCaptioning/caption_datasets.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-PwtuyEy857"
      },
      "source": [
        "import h5py\n",
        "import json\n",
        "from scipy.misc import imread, imresize\n",
        "from collections import Counter\n",
        "from random import seed, choice, sample\n",
        "\n",
        "def create_input_files(dataset, karpathy_json_path, image_folder, captions_per_image, min_word_freq, output_folder,\n",
        "                       max_len=100):\n",
        "    \"\"\"\n",
        "    Creates input files for training, validation, and test data.\n",
        "    :param dataset: name of dataset, one of 'coco', 'flickr8k', 'flickr30k'\n",
        "    :param karpathy_json_path: path of Karpathy JSON file with splits and captions\n",
        "    :param image_folder: folder with downloaded images\n",
        "    :param captions_per_image: number of captions to sample per image\n",
        "    :param min_word_freq: words occuring less frequently than this threshold are binned as <unk>s\n",
        "    :param output_folder: folder to save files\n",
        "    :param max_len: don't sample captions longer than this length\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    Sample format of flickr8k json file is as follows:\n",
        "    {\"images\": [{\"sentids\": [0, 1, 2, 3, 4], \n",
        "     \"imgid\": 0, \"sentences\": [{\"tokens\": [\"a\", \"black\", \"dog\", \"is\", \"running\", \"after\", \"a\", \"white\", \"dog\", \"in\", \"the\", \"snow\"], \"raw\": \"A black dog is running after a white dog in the snow .\", \"imgid\": 0, \"sentid\": 0}, \n",
        "                               {\"tokens\": [\"black\", \"dog\", \"chasing\", \"brown\", \"dog\", \"through\", \"snow\"], \"raw\": \"Black dog chasing brown dog through snow\", \"imgid\": 0, \"sentid\": 1}, \n",
        "\t\t\t\t\t\t                   {\"tokens\": [\"two\", \"dogs\", \"chase\", \"each\", \"other\", \"across\", \"the\", \"snowy\", \"ground\"], \"raw\": \"Two dogs chase each other across the snowy ground .\", \"imgid\": 0, \"sentid\": 2}, \n",
        "\t\t\t\t\t\t                   {\"tokens\": [\"two\", \"dogs\", \"play\", \"together\", \"in\", \"the\", \"snow\"], \"raw\": \"Two dogs play together in the snow .\", \"imgid\": 0, \"sentid\": 3}, \n",
        "\t\t\t\t\t\t                   {\"tokens\": [\"two\", \"dogs\", \"running\", \"through\", \"a\", \"low\", \"lying\", \"body\", \"of\", \"water\"], \"raw\": \"Two dogs running through a low lying body of water .\", \"imgid\": 0, \"sentid\": 4}], \n",
        "\t\t\t\t\t\t                    \"split\": \"train\", \"filename\": \"2513260012_03d33305cf.jpg\"}, \n",
        "                {\"sentids\": [5, 6, 7, 8, 9], \n",
        "     \"imgid\": 1, \"sentences\": [{\"tokens\": [\"a\", \"little\", \"baby\", \"plays\", \"croquet\"], \"raw\": \"A little baby plays croquet .\", \"imgid\": 1, \"sentid\": 5},\n",
        "                               {\"tokens\": [\"a\", \"little\", \"girl\", \"plays\", \"croquet\", \"next\", \"to\", \"a\", \"truck\"], \"raw\": \"A little girl plays croquet next to a truck .\", \"imgid\": 1, \"sentid\": 6},\n",
        "                               {\"tokens\": [\"the\", \"child\", \"is\", \"playing\", \"croquette\", \"by\", \"the\", \"truck\"], \"raw\": \"The child is playing croquette by the truck .\", \"imgid\": 1, \"sentid\": 7}, \n",
        "                               {\"tokens\": [\"the\", \"kid\", \"is\", \"in\", \"front\", \"of\", \"a\", \"car\", \"with\", \"a\", \"put\", \"and\", \"a\", \"ball\"], \"raw\": \"The kid is in front of a car with a put and a ball .\", \"imgid\": 1, \"sentid\": 8}, \n",
        "                               {\"tokens\": [\"the\", \"little\", \"boy\", \"is\", \"playing\", \"with\", \"a\", \"croquet\", \"hammer\", \"and\", \"ball\", \"beside\", \"the\", \"car\"], \"raw\": \"The little boy is playing with a croquet hammer and ball beside the car .\", \"imgid\": 1, \"sentid\": 9}],\n",
        "                                \"split\": \"train\", \"filename\": \"2903617548_d3e38d7f88.jpg\"}\t\t\t\n",
        "    \"\"\"                      \n",
        "\n",
        "    assert dataset in {'coco', 'flickr8k', 'flickr30k'}\n",
        "\n",
        "    # Read Karpathy JSON\n",
        "    with open(karpathy_json_path, 'r') as j:\n",
        "        data = json.load(j)\n",
        "\n",
        "    # Read image paths and captions for each image\n",
        "    train_image_paths = []\n",
        "    train_image_captions = []\n",
        "    val_image_paths = []\n",
        "    val_image_captions = []\n",
        "    test_image_paths = []\n",
        "    test_image_captions = []\n",
        "    word_freq = Counter()   # list1 = ['x','y','z','x','x','x','y', 'z']. The output if counter is used on list1 should be something like :\n",
        "                            # Counter({'x': 4, 'y': 2, 'z': 2})\n",
        "\n",
        "    cnt = 0\n",
        "    for img in data['images']:\n",
        "        cnt += 1\n",
        "        captions = []\n",
        "        for c in img['sentences']:\n",
        "            # Update word frequency\n",
        "            word_freq.update(c['tokens'])\n",
        "            if len(c['tokens']) <= max_len:\n",
        "                captions.append(c['tokens'])      \n",
        "\n",
        "        if len(captions) == 0:\n",
        "            continue\n",
        "\n",
        "        path = os.path.join(image_folder, img['filepath'], img['filename']) if dataset == 'coco' else os.path.join(\n",
        "            image_folder, img['filename'])\n",
        "        \n",
        "        if cnt < 5:\n",
        "            print('Count, word_freq :', cnt, word_freq)\n",
        "            print('length: ',len(captions), 'captions: ', captions) \n",
        "            print('img-filename: ', img['filename'])\n",
        "            print('img :', img)\n",
        "\n",
        "        if img['split'] in {'train', 'restval'}:\n",
        "            train_image_paths.append(path)\n",
        "            train_image_captions.append(captions)\n",
        "        elif img['split'] in {'val'}:\n",
        "            val_image_paths.append(path)\n",
        "            val_image_captions.append(captions)\n",
        "        elif img['split'] in {'test'}:\n",
        "            test_image_paths.append(path)\n",
        "            test_image_captions.append(captions)\n",
        "        if cnt < 5:\n",
        "            print('Count:', cnt, 'len(train_image_paths):',len(train_image_paths), 'len(train_image_captions):', len(train_image_captions))\n",
        "            print('train_image_paths:',train_image_paths)\n",
        "            print('train_image_captions:',train_image_captions) \n",
        "\n",
        "    print ('count :', cnt, '# of train_images, val_images, test_images : ', len(train_image_paths), len(val_image_paths), len(test_image_paths)) \n",
        "    # Sanity check\n",
        "    assert len(train_image_paths) == len(train_image_captions)\n",
        "    assert len(val_image_paths) == len(val_image_captions)\n",
        "    assert len(test_image_paths) == len(test_image_captions)\n",
        "\n",
        "    # Create word map\n",
        "    words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq] # words occuring less frequently than min_word_freq are binned as <unk>s\n",
        "    word_map = {k: v + 1 for v, k in enumerate(words)} # v+1 to reserve first index 0 for <pad>. word_map is a dict like {<pad>:0, 'a':1, 'an':2,..}\n",
        "    word_map['<unk>'] = len(word_map) + 1\n",
        "    word_map['<start>'] = len(word_map) + 1\n",
        "    word_map['<end>'] = len(word_map) + 1\n",
        "    word_map['<pad>'] = 0\n",
        "\n",
        "    print('Count of total images:', cnt)\n",
        "    print('Entire word_freq:', word_freq)\n",
        "    print('Words Start:', words[0:100])\n",
        "    print('Words End:', words[len(words)-100:len(words)])\n",
        "    print('word_map:', word_map)\n",
        "\n",
        "    # Create a base/root name for all output files\n",
        "    base_filename = dataset + '_' + str(captions_per_image) + '_cap_per_img_' + str(min_word_freq) + '_min_word_freq'\n",
        "    print('base_filename : ', base_filename)\n",
        "\n",
        "    # Save word map to a JSON\n",
        "    with open(os.path.join(output_folder, 'WORDMAP_' + base_filename + '.json'), 'w') as j:\n",
        "        json.dump(word_map, j)\n",
        "\n",
        "    \n",
        "    # Sample captions for each image, save images to HDF5 file, and captions and their lengths to JSON files\n",
        "    seed(123)\n",
        "    for impaths, imcaps, split in [(train_image_paths, train_image_captions, 'TRAIN'),\n",
        "                                   (val_image_paths, val_image_captions, 'VAL'),\n",
        "                                   (test_image_paths, test_image_captions, 'TEST')]:\n",
        "\n",
        "        print(' *** Split Change *** :', split)\n",
        "        cnt = 0\n",
        "        with h5py.File(os.path.join(output_folder, split + '_IMAGES_' + base_filename + '.hdf5'), 'a') as h:\n",
        "            # Make a note of the number of captions we are sampling per image\n",
        "            h.attrs['captions_per_image'] = captions_per_image\n",
        "\n",
        "            # Create dataset inside HDF5 file to store images\n",
        "            images = h.create_dataset('images', (len(impaths), 3, 256, 256), dtype='uint8')\n",
        "\n",
        "            print(\"\\nReading %s images and captions, storing to file...\\n\" % split)\n",
        "\n",
        "            enc_captions = []\n",
        "            caplens = []\n",
        "            \n",
        "            for i, path in enumerate(tqdm(impaths)):\n",
        "                cnt +=1 \n",
        "                # Sample captions\n",
        "                if len(imcaps[i]) < captions_per_image:\n",
        "                    if cnt < 10:\n",
        "                        print('imcaps[i] when len(imcaps[i]) < captions_per_image:', imcaps[i])        \n",
        "                    captions = imcaps[i] + [choice(imcaps[i]) for _ in range(captions_per_image - len(imcaps[i]))]  \n",
        "                    # choice() returns a random item. mylist = [\"apple\", \"banana\", \"cherry\"], print(choice(mylist)) --> banana\n",
        "                    # We are filling the gap between captions_per_image and length of caption by available words randomly chosen from that specific caption itself\n",
        "                    # Let us say len(imcaps[i]) = 3 & captions_per_image = 5, then 4th & 5th captions will be generated by above logic\n",
        "                    if cnt < 50:\n",
        "                       print('len(imcaps[i]):', len(imcaps[i]))\n",
        "                       print('captions when len(imcaps[i]) < captions_per_image:', captions) \n",
        "                else:                    \n",
        "                    captions = sample(imcaps[i], k=captions_per_image) \n",
        "                    # Prints list of random items of given length. list1 = [1, 2, 3, 4, 5] , print(sample(list1,3))  --> [2,3,5]\n",
        "                    # If len(imcaps[i]) = 5 and captions_per_image = 5, then all 5 captions will be written\n",
        "                    # If len(imcaps[i]) = 7 and captions_per_image = 5, then 5 captions out of 7 will be randomly chosen via random() and written\n",
        "                    if cnt < 10:\n",
        "                        print('captions when len(imcaps[i]) GT> captions_per_image:', captions)\n",
        "\n",
        "                # Sanity check\n",
        "                assert len(captions) == captions_per_image\n",
        "\n",
        "                # Read images\n",
        "                img = imread(impaths[i])\n",
        "                if len(img.shape) == 2:           # If gray scale, add 1 more axis to bring to [256, 256, 3] format\n",
        "                    img = img[:, :, np.newaxis]   # An empty axis created on 3rd dimension i.e. axis = 2 (0 & 1 are existing axes)\n",
        "                    img = np.concatenate([img, img, img], axis=2)  # Filling values of 0 & 1 axis (2d frame), three times to make it 3D\n",
        "                                                                   # eg: (256, 256) is filled 3 times to give RGB i.e. (256, 256, 3) \n",
        "                img = imresize(img, (256, 256))   # eg: (128, 128, 3) -> (256, 256, 3)\n",
        "                img = img.transpose(2, 0, 1)      # changes (256,256,3) to (3, 256, 256)\n",
        "                assert img.shape == (3, 256, 256)\n",
        "                assert np.max(img) <= 255\n",
        "\n",
        "                # Save image to HDF5 file\n",
        "                images[i] = img\n",
        "\n",
        "                if cnt < 2:\n",
        "                   print('Encoding captions start stat', 'count:', cnt, 'start Length:', caplens, 'enc_captions:', enc_captions, 'len(captions):', len(captions))\n",
        "                for j, c in enumerate(captions):\n",
        "                    # Encode captions\n",
        "                    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in c] + [\n",
        "                        word_map['<end>']] + [word_map['<pad>']] * (max_len - len(c))\n",
        "                    # Create encoding for each caption. Use index of <unk> if a particular word from caption is not found in word_map.\n",
        "                    # Pad the vacant space with index belonging to <pad>\n",
        "                    # If max_len we are passing as argument is 50 & caption length from dataset is 14, then 50-14 = 36 will be padded as <pad>\n",
        "\n",
        "                    # Find caption lengths\n",
        "                    c_len = len(c) + 2   # Plus 2 as we are adding <start> and <end> indexes\n",
        "\n",
        "                    enc_captions.append(enc_c)\n",
        "                    caplens.append(c_len)\n",
        "                    if cnt < 2:\n",
        "                        print('Encoding captions End stat', 'count:', cnt, 'end Length:', caplens, 'enc_captions:', enc_captions)\n",
        "\n",
        "            # Sanity check\n",
        "            assert images.shape[0] * captions_per_image == len(enc_captions) == len(caplens)\n",
        "            print('***Total Count***:',cnt)\n",
        "            # Save encoded captions and their lengths to JSON files\n",
        "            with open(os.path.join(output_folder, split + '_CAPTIONS_' + base_filename + '.json'), 'w') as j:\n",
        "                json.dump(enc_captions, j)\n",
        "\n",
        "            with open(os.path.join(output_folder, split + '_CAPLENS_' + base_filename + '.json'), 'w') as j:\n",
        "                json.dump(caplens, j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-BG7D_2zpl_",
        "outputId": "abef249f-5703-4f59-fcf2-8e7f66315fcd"
      },
      "source": [
        "#### MANUALLY Create a folder '/content/data_output' before executing this cell\n",
        "\n",
        "create_input_files(dataset='flickr8k',\n",
        "                       karpathy_json_path='/content/dataset_flickr8k.json',\n",
        "                       image_folder='/content/Flicker8k_Dataset/',\n",
        "                       captions_per_image=5,\n",
        "                       min_word_freq=5,\n",
        "                       output_folder='/content/data_output/',\n",
        "                       max_len=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count, word_freq : 1 Counter({'dog': 4, 'a': 3, 'the': 3, 'snow': 3, 'two': 3, 'dogs': 3, 'black': 2, 'running': 2, 'in': 2, 'through': 2, 'is': 1, 'after': 1, 'white': 1, 'chasing': 1, 'brown': 1, 'chase': 1, 'each': 1, 'other': 1, 'across': 1, 'snowy': 1, 'ground': 1, 'play': 1, 'together': 1, 'low': 1, 'lying': 1, 'body': 1, 'of': 1, 'water': 1})\n",
            "length:  5 captions:  [['a', 'black', 'dog', 'is', 'running', 'after', 'a', 'white', 'dog', 'in', 'the', 'snow'], ['black', 'dog', 'chasing', 'brown', 'dog', 'through', 'snow'], ['two', 'dogs', 'chase', 'each', 'other', 'across', 'the', 'snowy', 'ground'], ['two', 'dogs', 'play', 'together', 'in', 'the', 'snow'], ['two', 'dogs', 'running', 'through', 'a', 'low', 'lying', 'body', 'of', 'water']]\n",
            "img-filename:  2513260012_03d33305cf.jpg\n",
            "img : {'sentids': [0, 1, 2, 3, 4], 'imgid': 0, 'sentences': [{'tokens': ['a', 'black', 'dog', 'is', 'running', 'after', 'a', 'white', 'dog', 'in', 'the', 'snow'], 'raw': 'A black dog is running after a white dog in the snow .', 'imgid': 0, 'sentid': 0}, {'tokens': ['black', 'dog', 'chasing', 'brown', 'dog', 'through', 'snow'], 'raw': 'Black dog chasing brown dog through snow', 'imgid': 0, 'sentid': 1}, {'tokens': ['two', 'dogs', 'chase', 'each', 'other', 'across', 'the', 'snowy', 'ground'], 'raw': 'Two dogs chase each other across the snowy ground .', 'imgid': 0, 'sentid': 2}, {'tokens': ['two', 'dogs', 'play', 'together', 'in', 'the', 'snow'], 'raw': 'Two dogs play together in the snow .', 'imgid': 0, 'sentid': 3}, {'tokens': ['two', 'dogs', 'running', 'through', 'a', 'low', 'lying', 'body', 'of', 'water'], 'raw': 'Two dogs running through a low lying body of water .', 'imgid': 0, 'sentid': 4}], 'split': 'train', 'filename': '2513260012_03d33305cf.jpg'}\n",
            "Count: 1 len(train_image_paths): 1 len(train_image_captions): 1\n",
            "train_image_paths: ['/content/Flicker8k_Dataset/2513260012_03d33305cf.jpg']\n",
            "train_image_captions: [[['a', 'black', 'dog', 'is', 'running', 'after', 'a', 'white', 'dog', 'in', 'the', 'snow'], ['black', 'dog', 'chasing', 'brown', 'dog', 'through', 'snow'], ['two', 'dogs', 'chase', 'each', 'other', 'across', 'the', 'snowy', 'ground'], ['two', 'dogs', 'play', 'together', 'in', 'the', 'snow'], ['two', 'dogs', 'running', 'through', 'a', 'low', 'lying', 'body', 'of', 'water']]]\n",
            "Count, word_freq : 2 Counter({'a': 10, 'the': 8, 'dog': 4, 'is': 4, 'in': 3, 'snow': 3, 'two': 3, 'dogs': 3, 'little': 3, 'croquet': 3, 'black': 2, 'running': 2, 'through': 2, 'of': 2, 'plays': 2, 'truck': 2, 'playing': 2, 'car': 2, 'with': 2, 'and': 2, 'ball': 2, 'after': 1, 'white': 1, 'chasing': 1, 'brown': 1, 'chase': 1, 'each': 1, 'other': 1, 'across': 1, 'snowy': 1, 'ground': 1, 'play': 1, 'together': 1, 'low': 1, 'lying': 1, 'body': 1, 'water': 1, 'baby': 1, 'girl': 1, 'next': 1, 'to': 1, 'child': 1, 'croquette': 1, 'by': 1, 'kid': 1, 'front': 1, 'put': 1, 'boy': 1, 'hammer': 1, 'beside': 1})\n",
            "length:  5 captions:  [['a', 'little', 'baby', 'plays', 'croquet'], ['a', 'little', 'girl', 'plays', 'croquet', 'next', 'to', 'a', 'truck'], ['the', 'child', 'is', 'playing', 'croquette', 'by', 'the', 'truck'], ['the', 'kid', 'is', 'in', 'front', 'of', 'a', 'car', 'with', 'a', 'put', 'and', 'a', 'ball'], ['the', 'little', 'boy', 'is', 'playing', 'with', 'a', 'croquet', 'hammer', 'and', 'ball', 'beside', 'the', 'car']]\n",
            "img-filename:  2903617548_d3e38d7f88.jpg\n",
            "img : {'sentids': [5, 6, 7, 8, 9], 'imgid': 1, 'sentences': [{'tokens': ['a', 'little', 'baby', 'plays', 'croquet'], 'raw': 'A little baby plays croquet .', 'imgid': 1, 'sentid': 5}, {'tokens': ['a', 'little', 'girl', 'plays', 'croquet', 'next', 'to', 'a', 'truck'], 'raw': 'A little girl plays croquet next to a truck .', 'imgid': 1, 'sentid': 6}, {'tokens': ['the', 'child', 'is', 'playing', 'croquette', 'by', 'the', 'truck'], 'raw': 'The child is playing croquette by the truck .', 'imgid': 1, 'sentid': 7}, {'tokens': ['the', 'kid', 'is', 'in', 'front', 'of', 'a', 'car', 'with', 'a', 'put', 'and', 'a', 'ball'], 'raw': 'The kid is in front of a car with a put and a ball .', 'imgid': 1, 'sentid': 8}, {'tokens': ['the', 'little', 'boy', 'is', 'playing', 'with', 'a', 'croquet', 'hammer', 'and', 'ball', 'beside', 'the', 'car'], 'raw': 'The little boy is playing with a croquet hammer and ball beside the car .', 'imgid': 1, 'sentid': 9}], 'split': 'train', 'filename': '2903617548_d3e38d7f88.jpg'}\n",
            "Count: 2 len(train_image_paths): 2 len(train_image_captions): 2\n",
            "train_image_paths: ['/content/Flicker8k_Dataset/2513260012_03d33305cf.jpg', '/content/Flicker8k_Dataset/2903617548_d3e38d7f88.jpg']\n",
            "train_image_captions: [[['a', 'black', 'dog', 'is', 'running', 'after', 'a', 'white', 'dog', 'in', 'the', 'snow'], ['black', 'dog', 'chasing', 'brown', 'dog', 'through', 'snow'], ['two', 'dogs', 'chase', 'each', 'other', 'across', 'the', 'snowy', 'ground'], ['two', 'dogs', 'play', 'together', 'in', 'the', 'snow'], ['two', 'dogs', 'running', 'through', 'a', 'low', 'lying', 'body', 'of', 'water']], [['a', 'little', 'baby', 'plays', 'croquet'], ['a', 'little', 'girl', 'plays', 'croquet', 'next', 'to', 'a', 'truck'], ['the', 'child', 'is', 'playing', 'croquette', 'by', 'the', 'truck'], ['the', 'kid', 'is', 'in', 'front', 'of', 'a', 'car', 'with', 'a', 'put', 'and', 'a', 'ball'], ['the', 'little', 'boy', 'is', 'playing', 'with', 'a', 'croquet', 'hammer', 'and', 'ball', 'beside', 'the', 'car']]]\n",
            "Count, word_freq : 3 Counter({'a': 17, 'the': 12, 'dog': 9, 'in': 9, 'is': 7, 'snow': 7, 'pink': 5, 'brown': 4, 'through': 3, 'two': 3, 'dogs': 3, 'little': 3, 'croquet': 3, 'with': 3, 'something': 3, 'its': 3, 'mouth': 3, 'black': 2, 'running': 2, 'of': 2, 'plays': 2, 'truck': 2, 'playing': 2, 'car': 2, 'and': 2, 'ball': 2, 'holding': 2, 'after': 1, 'white': 1, 'chasing': 1, 'chase': 1, 'each': 1, 'other': 1, 'across': 1, 'snowy': 1, 'ground': 1, 'play': 1, 'together': 1, 'low': 1, 'lying': 1, 'body': 1, 'water': 1, 'baby': 1, 'girl': 1, 'next': 1, 'to': 1, 'child': 1, 'croquette': 1, 'by': 1, 'kid': 1, 'front': 1, 'put': 1, 'boy': 1, 'hammer': 1, 'beside': 1, 'has': 1, 'hot': 1, 'hat': 1, 'shirt': 1, 'carrying': 1, 'while': 1, 'walking': 1, 'looking': 1, 'forward': 1})\n",
            "length:  5 captions:  [['a', 'brown', 'dog', 'in', 'the', 'snow', 'has', 'something', 'hot', 'pink', 'in', 'its', 'mouth'], ['a', 'brown', 'dog', 'in', 'the', 'snow', 'holding', 'a', 'pink', 'hat'], ['a', 'brown', 'dog', 'is', 'holding', 'a', 'pink', 'shirt', 'in', 'the', 'snow'], ['a', 'dog', 'is', 'carrying', 'something', 'pink', 'in', 'its', 'mouth', 'while', 'walking', 'through', 'the', 'snow'], ['a', 'dog', 'with', 'something', 'pink', 'in', 'its', 'mouth', 'is', 'looking', 'forward']]\n",
            "img-filename:  3338291921_fe7ae0c8f8.jpg\n",
            "img : {'sentids': [10, 11, 12, 13, 14], 'imgid': 2, 'sentences': [{'tokens': ['a', 'brown', 'dog', 'in', 'the', 'snow', 'has', 'something', 'hot', 'pink', 'in', 'its', 'mouth'], 'raw': 'A brown dog in the snow has something hot pink in its mouth .', 'imgid': 2, 'sentid': 10}, {'tokens': ['a', 'brown', 'dog', 'in', 'the', 'snow', 'holding', 'a', 'pink', 'hat'], 'raw': 'A brown dog in the snow holding a pink hat .', 'imgid': 2, 'sentid': 11}, {'tokens': ['a', 'brown', 'dog', 'is', 'holding', 'a', 'pink', 'shirt', 'in', 'the', 'snow'], 'raw': 'A brown dog is holding a pink shirt in the snow .', 'imgid': 2, 'sentid': 12}, {'tokens': ['a', 'dog', 'is', 'carrying', 'something', 'pink', 'in', 'its', 'mouth', 'while', 'walking', 'through', 'the', 'snow'], 'raw': 'A dog is carrying something pink in its mouth while walking through the snow .', 'imgid': 2, 'sentid': 13}, {'tokens': ['a', 'dog', 'with', 'something', 'pink', 'in', 'its', 'mouth', 'is', 'looking', 'forward'], 'raw': 'A dog with something pink in its mouth is looking forward .', 'imgid': 2, 'sentid': 14}], 'split': 'train', 'filename': '3338291921_fe7ae0c8f8.jpg'}\n",
            "Count: 3 len(train_image_paths): 3 len(train_image_captions): 3\n",
            "train_image_paths: ['/content/Flicker8k_Dataset/2513260012_03d33305cf.jpg', '/content/Flicker8k_Dataset/2903617548_d3e38d7f88.jpg', '/content/Flicker8k_Dataset/3338291921_fe7ae0c8f8.jpg']\n",
            "train_image_captions: [[['a', 'black', 'dog', 'is', 'running', 'after', 'a', 'white', 'dog', 'in', 'the', 'snow'], ['black', 'dog', 'chasing', 'brown', 'dog', 'through', 'snow'], ['two', 'dogs', 'chase', 'each', 'other', 'across', 'the', 'snowy', 'ground'], ['two', 'dogs', 'play', 'together', 'in', 'the', 'snow'], ['two', 'dogs', 'running', 'through', 'a', 'low', 'lying', 'body', 'of', 'water']], [['a', 'little', 'baby', 'plays', 'croquet'], ['a', 'little', 'girl', 'plays', 'croquet', 'next', 'to', 'a', 'truck'], ['the', 'child', 'is', 'playing', 'croquette', 'by', 'the', 'truck'], ['the', 'kid', 'is', 'in', 'front', 'of', 'a', 'car', 'with', 'a', 'put', 'and', 'a', 'ball'], ['the', 'little', 'boy', 'is', 'playing', 'with', 'a', 'croquet', 'hammer', 'and', 'ball', 'beside', 'the', 'car']], [['a', 'brown', 'dog', 'in', 'the', 'snow', 'has', 'something', 'hot', 'pink', 'in', 'its', 'mouth'], ['a', 'brown', 'dog', 'in', 'the', 'snow', 'holding', 'a', 'pink', 'hat'], ['a', 'brown', 'dog', 'is', 'holding', 'a', 'pink', 'shirt', 'in', 'the', 'snow'], ['a', 'dog', 'is', 'carrying', 'something', 'pink', 'in', 'its', 'mouth', 'while', 'walking', 'through', 'the', 'snow'], ['a', 'dog', 'with', 'something', 'pink', 'in', 'its', 'mouth', 'is', 'looking', 'forward']]]\n",
            "Count, word_freq : 4 Counter({'a': 22, 'the': 19, 'dog': 14, 'is': 9, 'in': 9, 'brown': 8, 'snow': 7, 'running': 6, 'pink': 5, 'beach': 4, 'black': 3, 'through': 3, 'two': 3, 'dogs': 3, 'little': 3, 'croquet': 3, 'with': 3, 'something': 3, 'its': 3, 'mouth': 3, 'on': 3, 'across': 2, 'of': 2, 'water': 2, 'plays': 2, 'truck': 2, 'playing': 2, 'by': 2, 'car': 2, 'and': 2, 'ball': 2, 'holding': 2, 'after': 1, 'white': 1, 'chasing': 1, 'chase': 1, 'each': 1, 'other': 1, 'snowy': 1, 'ground': 1, 'play': 1, 'together': 1, 'low': 1, 'lying': 1, 'body': 1, 'baby': 1, 'girl': 1, 'next': 1, 'to': 1, 'child': 1, 'croquette': 1, 'kid': 1, 'front': 1, 'put': 1, 'boy': 1, 'hammer': 1, 'beside': 1, 'has': 1, 'hot': 1, 'hat': 1, 'shirt': 1, 'carrying': 1, 'while': 1, 'walking': 1, 'looking': 1, 'forward': 1, 'along': 1, 'wearing': 1, 'collar': 1, 'walks': 1, 'sand': 1, 'near': 1, 'large': 1, 'ocean': 1})\n",
            "length:  5 captions:  [['a', 'brown', 'dog', 'is', 'running', 'along', 'a', 'beach'], ['a', 'brown', 'dog', 'wearing', 'a', 'black', 'collar', 'running', 'across', 'the', 'beach'], ['a', 'dog', 'walks', 'on', 'the', 'sand', 'near', 'the', 'water'], ['brown', 'dog', 'running', 'on', 'the', 'beach'], ['the', 'large', 'brown', 'dog', 'is', 'running', 'on', 'the', 'beach', 'by', 'the', 'ocean']]\n",
            "img-filename:  488416045_1c6d903fe0.jpg\n",
            "img : {'sentids': [15, 16, 17, 18, 19], 'imgid': 3, 'sentences': [{'tokens': ['a', 'brown', 'dog', 'is', 'running', 'along', 'a', 'beach'], 'raw': 'A brown dog is running along a beach .', 'imgid': 3, 'sentid': 15}, {'tokens': ['a', 'brown', 'dog', 'wearing', 'a', 'black', 'collar', 'running', 'across', 'the', 'beach'], 'raw': 'A brown dog wearing a black collar running across the beach .', 'imgid': 3, 'sentid': 16}, {'tokens': ['a', 'dog', 'walks', 'on', 'the', 'sand', 'near', 'the', 'water'], 'raw': 'A dog walks on the sand near the water .', 'imgid': 3, 'sentid': 17}, {'tokens': ['brown', 'dog', 'running', 'on', 'the', 'beach'], 'raw': 'Brown dog running on the beach .', 'imgid': 3, 'sentid': 18}, {'tokens': ['the', 'large', 'brown', 'dog', 'is', 'running', 'on', 'the', 'beach', 'by', 'the', 'ocean'], 'raw': 'The large brown dog is running on the beach by the ocean .', 'imgid': 3, 'sentid': 19}], 'split': 'train', 'filename': '488416045_1c6d903fe0.jpg'}\n",
            "Count: 4 len(train_image_paths): 4 len(train_image_captions): 4\n",
            "train_image_paths: ['/content/Flicker8k_Dataset/2513260012_03d33305cf.jpg', '/content/Flicker8k_Dataset/2903617548_d3e38d7f88.jpg', '/content/Flicker8k_Dataset/3338291921_fe7ae0c8f8.jpg', '/content/Flicker8k_Dataset/488416045_1c6d903fe0.jpg']\n",
            "train_image_captions: [[['a', 'black', 'dog', 'is', 'running', 'after', 'a', 'white', 'dog', 'in', 'the', 'snow'], ['black', 'dog', 'chasing', 'brown', 'dog', 'through', 'snow'], ['two', 'dogs', 'chase', 'each', 'other', 'across', 'the', 'snowy', 'ground'], ['two', 'dogs', 'play', 'together', 'in', 'the', 'snow'], ['two', 'dogs', 'running', 'through', 'a', 'low', 'lying', 'body', 'of', 'water']], [['a', 'little', 'baby', 'plays', 'croquet'], ['a', 'little', 'girl', 'plays', 'croquet', 'next', 'to', 'a', 'truck'], ['the', 'child', 'is', 'playing', 'croquette', 'by', 'the', 'truck'], ['the', 'kid', 'is', 'in', 'front', 'of', 'a', 'car', 'with', 'a', 'put', 'and', 'a', 'ball'], ['the', 'little', 'boy', 'is', 'playing', 'with', 'a', 'croquet', 'hammer', 'and', 'ball', 'beside', 'the', 'car']], [['a', 'brown', 'dog', 'in', 'the', 'snow', 'has', 'something', 'hot', 'pink', 'in', 'its', 'mouth'], ['a', 'brown', 'dog', 'in', 'the', 'snow', 'holding', 'a', 'pink', 'hat'], ['a', 'brown', 'dog', 'is', 'holding', 'a', 'pink', 'shirt', 'in', 'the', 'snow'], ['a', 'dog', 'is', 'carrying', 'something', 'pink', 'in', 'its', 'mouth', 'while', 'walking', 'through', 'the', 'snow'], ['a', 'dog', 'with', 'something', 'pink', 'in', 'its', 'mouth', 'is', 'looking', 'forward']], [['a', 'brown', 'dog', 'is', 'running', 'along', 'a', 'beach'], ['a', 'brown', 'dog', 'wearing', 'a', 'black', 'collar', 'running', 'across', 'the', 'beach'], ['a', 'dog', 'walks', 'on', 'the', 'sand', 'near', 'the', 'water'], ['brown', 'dog', 'running', 'on', 'the', 'beach'], ['the', 'large', 'brown', 'dog', 'is', 'running', 'on', 'the', 'beach', 'by', 'the', 'ocean']]]\n",
            "count : 8000 # of train_images, val_images, test_images :  6000 1000 1000\n",
            "Count of total images: 8000\n",
            "Entire word_freq: "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/6000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:164: DeprecationWarning:     `imread` is deprecated!\n",
            "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "    Use ``imageio.imread`` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:169: DeprecationWarning:     `imresize` is deprecated!\n",
            "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "    Use ``skimage.transform.resize`` instead.\n",
            "  0%|          | 7/6000 [00:00<01:27, 68.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Counter({'a': 62433, 'in': 18811, 'the': 18248, 'on': 10675, 'is': 9251, 'and': 8796, 'dog': 8111, 'with': 7702, 'man': 7194, 'of': 6653, 'two': 5611, 'white': 3921, 'black': 3830, 'boy': 3575, 'are': 3466, 'woman': 3392, 'girl': 3312, 'to': 3135, 'wearing': 3049, 'at': 2869, 'people': 2849, 'water': 2764, 'red': 2681, 'young': 2615, 'brown': 2566, 'an': 2400, 'his': 2334, 'blue': 2270, 'dogs': 2111, 'running': 2057, 'through': 2020, 'playing': 1992, 'shirt': 1953, 'while': 1953, 'down': 1829, 'standing': 1777, 'ball': 1773, 'little': 1764, 'grass': 1612, 'snow': 1542, 'child': 1540, 'person': 1521, 'jumping': 1472, 'over': 1401, 'front': 1372, 'three': 1369, 'sitting': 1357, 'holding': 1314, 'up': 1279, 'field': 1277, 'small': 1269, 'by': 1233, 'large': 1227, 'green': 1223, 'one': 1211, 'yellow': 1202, 'group': 1195, 'her': 1171, 'walking': 1164, 'children': 1151, 'men': 1098, 'into': 1068, 'air': 1060, 'beach': 1046, 'near': 1020, 'mouth': 988, 'jumps': 979, 'another': 951, 'for': 937, 'street': 933, 'runs': 921, 'from': 913, 'its': 910, 'riding': 901, 'bike': 865, 'stands': 864, 'as': 855, 'girls': 843, 'outside': 787, 'other': 762, 'off': 761, 'rock': 755, 'out': 753, 'next': 745, 'orange': 743, 'looking': 741, 'play': 738, 'pink': 734, 'player': 727, 'camera': 696, 'pool': 687, 'their': 678, 'hat': 677, 'jacket': 675, 'boys': 662, 'women': 650, 'around': 643, 'behind': 628, 'some': 616, 'background': 591, 'dirt': 588, 'toy': 580, 'soccer': 580, 'sits': 577, 'dressed': 561, 'mountain': 556, 'has': 554, 'wall': 550, 'walks': 546, 'crowd': 524, 'plays': 522, 'along': 521, 'stand': 515, 'looks': 509, 'park': 507, 'building': 506, 'climbing': 503, 'four': 499, 'top': 488, 'face': 482, 'football': 480, 'across': 471, 'grassy': 471, 'stick': 467, 'sand': 465, 'ocean': 463, 'smiling': 463, 'holds': 463, 'rides': 456, 'hill': 443, 'swimming': 441, 'skateboard': 435, 'doing': 431, 'tennis': 429, 'carrying': 427, 'car': 426, 'snowy': 422, 'each': 421, 'tree': 420, 'baby': 419, 'picture': 418, 'bicycle': 416, 'hair': 414, 'together': 409, 'jump': 405, 's': 400, 'him': 398, 'it': 396, 'basketball': 396, 'area': 395, 'road': 394, 'that': 394, 'tan': 392, 'back': 387, 'trick': 383, 'swing': 382, 'race': 380, 'shorts': 377, 'bench': 375, 'sidewalk': 374, 'head': 371, 'covered': 370, 'run': 366, 'catch': 366, 'game': 361, 'sit': 358, 'helmet': 356, 'ground': 354, 'hand': 348, 'dress': 347, 'something': 344, 'fence': 339, 'frisbee': 337, 'kids': 336, 'lake': 332, 'being': 328, 'path': 322, 'walk': 318, 'ramp': 318, 'wave': 317, 'city': 315, 'skateboarder': 314, 'long': 312, 'purple': 309, 'several': 308, 'there': 303, 'slide': 303, 'side': 301, 'high': 293, 'track': 289, 'baseball': 288, 'players': 286, 'posing': 284, 'wooden': 284, 'big': 278, 'sunglasses': 277, 'boat': 276, 'watches': 275, 'coat': 269, 'dark': 265, 'uniform': 261, 'trees': 260, 'look': 260, 'pants': 259, 'table': 257, 'rocks': 257, 'them': 256, 'ride': 254, 'watching': 248, 'rope': 248, 'couple': 247, 'towards': 247, 'grey': 247, 'beside': 246, 'arms': 245, 'rocky': 245, 'suit': 245, 'under': 244, 'motorcycle': 244, 'hands': 243, 'sign': 240, 'snowboarder': 237, 'watch': 237, 'river': 233, 'does': 230, 'horse': 228, 'racing': 226, 'jeans': 224, 'above': 224, 'older': 224, 'lady': 222, 'striped': 220, 'colored': 219, 'ice': 216, 'pose': 215, 'who': 215, 'colorful': 214, 'woods': 211, 'onto': 211, 'midair': 208, 'guy': 206, 'glasses': 206, 'mountains': 204, 'he': 204, 'taking': 204, 'leaps': 204, 'haired': 202, 'yard': 201, 'playground': 201, 'blonde': 200, 'climbs': 200, 'asian': 198, 'against': 198, 'collar': 197, 'blond': 194, 'cliff': 194, 'cap': 194, 'performing': 193, 'body': 191, 'smiles': 190, 'open': 190, 'surfer': 189, 'bird': 188, 'team': 188, 'laying': 186, 'chasing': 183, 'kid': 183, 'rider': 183, 'after': 182, 'hockey': 182, 'many': 180, 'fountain': 180, 'wet': 179, 'skier': 178, 'surrounded': 177, 'old': 173, 'inside': 173, 'outdoors': 173, 'brick': 172, 'during': 171, 'biker': 171, 'flying': 170, 'shore': 170, 'takes': 170, 'edge': 170, 'away': 169, 'toddler': 169, 'others': 169, 'light': 167, 't': 165, 'guitar': 163, 'hanging': 163, 'very': 162, 'middle': 162, 'forest': 160, 'trying': 160, 'five': 159, 'gray': 158, 'someone': 158, 'backpack': 158, 'outfit': 157, 'bed': 156, 'object': 155, 'steps': 155, 'making': 153, 'night': 153, 'floor': 153, 'pole': 153, 'talking': 153, 'nearby': 152, 'line': 152, 'whilst': 151, 'about': 149, 'flowers': 148, 'arm': 147, 'going': 147, 'trail': 146, 'past': 146, 'tall': 146, 'swinging': 146, 'surfboard': 146, 'sky': 144, 'toward': 144, 'eating': 144, 'dancing': 143, 'poses': 142, 'board': 142, 'bridge': 141, 'this': 141, 'leaves': 141, 'waves': 141, 'all': 140, 'leaping': 138, 'window': 138, 'outdoor': 137, 'day': 136, 'bag': 136, 'course': 135, 'clothes': 134, 'chair': 133, 'legs': 133, 'fighting': 133, 'house': 132, 'leash': 131, 'plastic': 130, 'costume': 130, 'room': 129, 'splashing': 128, 'carries': 128, 'shallow': 128, 'clothing': 128, 'stone': 128, 'shirts': 126, 'obstacle': 125, 'between': 124, 'climber': 124, 'catches': 123, 'ready': 123, 'bright': 122, 'sliding': 121, 'swings': 121, 'skateboarding': 121, 'adult': 120, 'sweater': 119, 'getting': 119, 'they': 119, 'bathing': 119, 'sled': 118, 'waiting': 117, 'concrete': 117, 'winter': 116, 'skiing': 115, 'metal': 115, 'lawn': 115, 'gear': 115, 'mud': 115, 'wears': 114, 'male': 114, 'trampoline': 114, 'uniforms': 113, 'railing': 113, 'jersey': 112, 'store': 111, 'stream': 111, 'number': 110, 'golden': 110, 'pulling': 110, 'sandy': 109, 'catching': 109, 'distance': 109, 'fire': 109, 'upside': 109, 'stairs': 109, 'train': 108, 'lot': 108, 'set': 107, 'gets': 107, 'drink': 107, 'fishing': 107, 'get': 107, 'tongue': 107, 'bar': 107, 'sun': 107, 'tries': 106, 'throwing': 106, 'adults': 105, 'smile': 104, 'overlooking': 104, 'shirtless': 104, 'ski': 103, 'rail': 103, 'swims': 103, 'female': 102, 'like': 102, 'couch': 102, 'wooded': 101, 'flies': 101, 'makes': 101, 'tricks': 100, 'puppy': 100, 'drinking': 100, 'busy': 99, 'tire': 99, 'chases': 98, 'vest': 98, 'lying': 97, 'american': 97, 'cellphone': 97, 'surfing': 97, 'animal': 97, 'pond': 96, 'performs': 96, 'swim': 95, 'laughing': 95, 'flag': 95, 'leaning': 95, 'right': 94, 'flip': 94, 'shopping': 94, 'she': 93, 'dock': 93, 'puddle': 93, 'photo': 92, 'reading': 92, 'trunks': 92, 'huge': 92, 'slides': 92, 'eyes': 92, 'horses': 92, 'food': 92, 'shoes': 91, 'hats': 91, 'sunset': 90, 'left': 89, 'bubbles': 89, 'climb': 89, 'cart': 89, 'bat': 89, 'nose': 88, 'snowboard': 88, 'scarf': 88, 'stunt': 88, 'coming': 88, 'deep': 88, 'or': 87, 'hold': 87, 'feet': 87, 'life': 87, 'family': 86, 'equipment': 85, 'waterfall': 85, 'bikes': 85, 'view': 84, 'greyhound': 84, 'elderly': 84, 'hurdle': 84, 'both': 83, 'tent': 83, 'no': 83, 'truck': 82, 'skis': 82, 'setting': 82, 'lone': 82, 'skating': 82, 'harness': 82, 'biting': 82, 'umbrella': 81, 'bus': 81, 'muddy': 81, 'wetsuit': 81, 'restaurant': 80, 'take': 80, 'hiker': 80, 'falling': 80, 'dry': 79, 'skirt': 79, 'bags': 79, 'guys': 79, 'goal': 79, 'paper': 79, 'court': 79, 'kayak': 79, 'mask': 79, 'vehicle': 78, 'fight': 78, 'dresses': 78, 'six': 77, 'book': 77, 'surf': 77, 'be': 77, 'sweatshirt': 77, 'tank': 77, 'bmx': 77, 'flags': 76, 'go': 76, 'crowded': 76, 'slope': 76, 'pile': 76, 'ledge': 76, 'structure': 76, 'german': 76, 'cigarette': 76, 'raft': 76, 'short': 75, 'airborne': 75, 'kick': 75, 'cross': 74, 'driving': 74, 'cement': 74, 'goes': 73, 'inflatable': 73, 'wood': 73, 'faces': 73, 'skate': 73, 'goggles': 73, 'cyclist': 72, 'subway': 72, 'have': 72, 'costumes': 72, 'kicking': 72, 'parking': 72, 'teenage': 71, 'graffiti': 71, 'canoe': 71, 'shaking': 71, 'bottle': 71, 'dance': 71, 'buildings': 70, 'turn': 70, 'diving': 70, 'splashes': 70, 'fallen': 70, 'shepherd': 70, 'sports': 69, 'ring': 69, 'ears': 69, 'hit': 68, 'blanket': 68, 'leather': 67, 'parade': 67, 'pictures': 67, 'smoking': 67, 'backyard': 67, 'chairs': 67, 'tunnel': 67, 'smaller': 67, 'low': 66, 'throws': 66, 'full': 66, 'beard': 66, 'boots': 66, 'piece': 66, 'held': 66, 'bikini': 66, 'balls': 66, 'gathered': 66, 'wheel': 65, 'surface': 65, 'silver': 65, 'sunny': 65, 'kicks': 65, 'closeup': 65, 'jackets': 65, 'outfits': 64, 'cat': 64, 'using': 64, 'event': 64, 'hiking': 64, 'cars': 64, 'pushing': 64, 'rugby': 64, 'pointing': 63, 'stuffed': 63, 'fluffy': 63, 'bicyclist': 63, 'bucket': 62, 'box': 62, 'painted': 62, 'bald': 62, 'glass': 61, 'steep': 61, 'statue': 61, 'sticks': 61, 'paint': 61, 'scooter': 61, 'suits': 61, 'swimsuit': 61, 'tube': 61, 'stage': 61, 'microphone': 61, 'gold': 60, 'door': 60, 'make': 60, 'volleyball': 60, 'flower': 60, 'drinks': 59, 'shot': 59, 'sleeping': 59, 'hugging': 59, 'furry': 59, 'corner': 59, 'points': 58, 'crossing': 58, 'leg': 58, 'same': 58, 'blowing': 58, 'hind': 58, 'band': 58, 'hangs': 58, 'staring': 57, 'leans': 57, 'kissing': 57, 'net': 57, 'wrestle': 57, 'wrestling': 57, 'bunch': 56, 'attempting': 56, 'few': 56, 'spectators': 56, 'lays': 56, 'hoop': 55, 'snowboarding': 55, 'log': 55, 'wading': 55, 'party': 55, 'motorcyclist': 55, 'below': 55, 'puppies': 55, 'cup': 55, 'cow': 55, 'fenced': 54, 'garden': 54, 'african': 54, 'facing': 54, 'desert': 54, 'wide': 54, 'attempts': 54, 'teams': 54, 'close': 54, 'show': 54, 'beige': 54, 'throw': 54, 'bicycles': 53, 'sprinkler': 53, 'fast': 53, 'beautiful': 53, 'fish': 53, 'gym': 53, 'way': 52, 'racket': 52, 'racetrack': 52, 'end': 52, 'phone': 52, 'eats': 52, 'attached': 52, 'naked': 52, 'filled': 52, 'poles': 52, 'sticking': 52, 'cowboy': 52, 'police': 52, 'onlookers': 51, 'rolling': 51, 'rain': 51, 'talks': 51, 'dances': 51, 'competition': 51, 'which': 51, 'gravel': 51, 'base': 51, 'sheep': 51, 'waving': 50, 'seat': 50, 'motocross': 50, 'do': 50, 'seated': 50, 'clear': 50, 'headband': 50, 'empty': 50, 'underwater': 50, 'hula': 50, 'signs': 49, 'round': 49, 'toys': 49, 'bull': 49, 'neck': 49, 'wear': 49, 'ladies': 49, 'bride': 49, 'branch': 49, 'gather': 49, 'plaid': 49, 'pushes': 49, 'hits': 49, 'softball': 49, 'downhill': 49, 'mohawk': 49, 'pavement': 48, 'paddling': 48, 'platform': 48, 'racer': 48, 'prepares': 48, 'public': 48, 'handstand': 48, 'among': 47, 'heads': 47, 'lit': 47, 'having': 47, 'skateboards': 47, 'market': 47, 'shop': 47, 'shoulder': 47, 'traffic': 47, 'school': 46, 'bearded': 46, 'chewing': 46, 'reads': 46, 'different': 46, 'gloves': 46, 'landscape': 45, 'rural': 45, 'races': 45, 'drives': 45, 'barefoot': 45, 'just': 45, 'hose': 45, 'cream': 45, 'birds': 45, 'dirty': 45, 'kitchen': 45, 'rough': 45, 'scene': 45, 'tie': 45, 'beer': 45, 'bars': 45, 'resting': 44, 'tug': 44, 'bottom': 44, 'teeth': 44, 'foot': 44, 'splash': 44, 'urban': 44, 'balloon': 44, 'thrown': 44, 'display': 43, 'smoke': 43, 'creek': 43, 'mother': 43, 'says': 43, 'paved': 43, 'painting': 43, 'waits': 43, 'wait': 43, 'opposing': 43, 'lies': 43, 'greyhounds': 42, 'animals': 42, 'before': 42, 'stadium': 42, 'indoor': 42, 'flight': 42, 'plants': 42, 'lap': 42, 'younger': 42, 'half': 42, 'pipe': 42, 'carpet': 42, 'blows': 41, 'fall': 41, 'showing': 41, 'match': 41, 'reaching': 41, 'skates': 41, 'boats': 41, 'moving': 41, 'parked': 41, 'wedding': 41, 'headphones': 41, 'cricket': 41, 'santa': 41, 'gun': 41, 'chase': 40, 'bandanna': 40, 'video': 40, 'lights': 40, 'wings': 40, 'seen': 40, 'foreground': 40, 'talk': 40, 'where': 40, 'art': 40, 'block': 40, 'paddle': 40, 'lined': 39, 'wire': 39, 'motorbike': 39, 'curly': 39, 'funny': 39, 'matching': 39, 'carnival': 39, 'deck': 39, 'poodle': 39, 'atv': 39, 'can': 39, 'mouths': 39, 'fly': 38, 'christmas': 38, 'outstretched': 38, 'photograph': 38, 'third': 38, 'pulled': 38, 'reaches': 38, 'spotted': 38, 'helmets': 38, 'newspaper': 38, 'audience': 38, 'unicycle': 38, 'goalie': 38, 'staircase': 38, 'amusement': 38, 'singing': 38, 'hitting': 38, 'hay': 38, 'retriever': 37, 'hole': 37, 'war': 37, 'purse': 37, 'grinding': 37, 'martial': 37, 'bank': 37, 'bushes': 37, 'shooting': 37, 'shakes': 37, 'spray': 37, 'bubble': 37, 'ropes': 37, 'brightly': 37, 'alongside': 37, 'runner': 37, 'atop': 37, 'appears': 36, 'floating': 36, 'finger': 36, 'larger': 36, 'raises': 36, 'muzzle': 36, 'country': 36, 'preparing': 36, 'try': 36, 'roller': 36, 'terrain': 36, 'follows': 36, 'pulls': 36, 'falls': 36, 'teenagers': 36, 'skiers': 36, 'onstage': 36, 'hooded': 36, 'ear': 36, 'plate': 36, 'giant': 36, 'chain': 36, 'surfs': 36, 'hoops': 36, 'kite': 36, 'fair': 36, 'stop': 35, 'denim': 35, 'putting': 35, 'alone': 35, 'covering': 35, 'digging': 35, 'pack': 35, 'jeep': 35, 'hoodie': 35, 'shoulders': 35, 'muzzled': 35, 'rapids': 35, 'hikers': 35, 'enjoys': 35, 'instruments': 35, 'balancing': 35, 'rowing': 35, 'formation': 35, 'wrestler': 35, 'perform': 34, 'station': 34, 'backwards': 34, 'spinning': 34, 'pier': 34, 'music': 34, 'indoors': 34, 'puts': 34, 'cold': 34, 'shaggy': 34, 'kneeling': 34, 'picnic': 34, 'shadow': 34, 'himself': 34, 'gives': 33, 'break': 33, 'row': 33, 'leap': 33, 'handrail': 33, 'laughs': 33, 'jumped': 33, 'wheelie': 33, 'paddles': 33, 'backs': 33, 'shows': 33, 'giving': 33, 'writing': 32, 'pull': 32, 'backpacks': 32, 'playfully': 32, 'gate': 32, 'owner': 32, 'hang': 32, 'bite': 32, 'cut': 32, 'safety': 32, 'competing': 32, 'ladder': 32, 'riders': 32, 'counter': 32, 'licking': 32, 'sniffing': 32, 'clouds': 32, 'necklace': 32, 'enjoying': 32, 'terrier': 31, 'passing': 31, 'fetch': 31, 'violin': 31, 'duck': 31, 'parachute': 31, 'basket': 31, 'tackle': 31, 'doorway': 31, 'seven': 31, 'machine': 31, 'bites': 31, 'underneath': 31, 'cloth': 31, 'jungle': 31, 'fur': 31, 'fingers': 31, 'skinned': 31, 'paws': 30, 'bikers': 30, 'collie': 30, 'taken': 30, 'friend': 30, 'made': 30, 'mid': 30, 'computer': 30, 'rollerblades': 30, 'step': 30, 'quickly': 30, 'coats': 30, 'splashed': 30, 'these': 30, 'camouflage': 30, 'hillside': 30, 'professional': 30, 'bowl': 30, 'wheelchair': 30, 'mound': 30, 'construction': 30, 'ducks': 30, 'hot': 29, 'straw': 29, 'float': 29, 'sea': 29, 'helps': 29, 'spots': 29, 'rollerblading': 29, 'skater': 29, 'pigeons': 29, 'trunk': 29, 'lean': 29, 'crouches': 29, 'decorated': 29, 'balances': 29, 'barrier': 29, 'homeless': 29, 'cone': 29, 'chest': 28, 'peace': 28, 'land': 28, 'opposite': 28, 'bear': 28, 'biking': 28, 'balloons': 28, 'caught': 28, 'plane': 28, 'arts': 28, 'laugh': 28, 'walkway': 28, 'stump': 28, 'referee': 28, 'rink': 28, 'kiss': 28, 'coffee': 28, 'home': 28, 'sooners': 28, 'frame': 28, 'karate': 28, 'screen': 28, 'dune': 28, 'stroller': 28, 'bouncing': 28, 'mirror': 28, 'string': 28, 'wagon': 28, 'heavy': 28, 'rodeo': 28, 'tracks': 28, 'courtyard': 28, 'hug': 28, 'waters': 27, 'medium': 27, 'sized': 27, 'reach': 27, 'climbers': 27, 'landing': 27, 'suspended': 27, 'shaped': 27, 'tattoo': 27, 'blurry': 27, 'speed': 27, 'without': 27, 'military': 27, 'range': 27, 'watched': 27, 'monkey': 27, 'motorcycles': 27, 'cardboard': 27, 'fetching': 27, 'drum': 27, 'coaster': 27, 'formal': 27, 'streets': 27, 'jean': 26, 'flowered': 26, 'cyclists': 26, 'direction': 26, 'rolls': 26, 'mountainside': 26, 'driver': 26, 'headscarf': 26, 'blocks': 26, 'barking': 26, 'eat': 26, 'jerseys': 26, 'dead': 26, 'touching': 26, 'see': 26, 'crashing': 26, 'raised': 26, 'begins': 26, 'clown': 26, 'practicing': 26, 'uses': 26, 'rubber': 26, 'blow': 26, 'frozen': 26, 'miami': 26, 'eye': 26, 'happily': 26, 'bending': 26, 'kneels': 26, 'makeup': 26, 'jogging': 26, 'knit': 26, 'petting': 26, 'put': 25, 'forward': 25, 'tires': 25, 'crosses': 25, 'boxing': 25, 'airplane': 25, 'shown': 25, 'grinds': 25, 'dribbles': 25, 'sprayed': 25, 'shoreline': 25, 'pass': 25, 'only': 25, 'neon': 25, 'well': 25, 'kayaking': 25, 'corn': 25, 'leaving': 25, 'balcony': 25, 'cake': 25, 'bend': 25, 'turning': 25, 'mountaintop': 25, 'dusk': 25, 'training': 25, 'photographer': 24, 'hills': 24, 'curb': 24, 'rest': 24, 'bicyclists': 24, 'teenager': 24, 'helping': 24, 'hugs': 24, 'tackled': 24, 'overalls': 24, 'but': 24, '2': 24, 'coach': 24, 'snowball': 24, 'lay': 24, 'town': 24, 'attire': 24, 'lab': 24, 'grocery': 24, 'porch': 24, 'policeman': 24, 'racquet': 24, 'lots': 24, 'surrounding': 24, 'curve': 24, 'boardwalk': 24, 'waterskiing': 24, 'patch': 24, 'skirts': 24, 'disc': 23, 'muzzles': 23, 'sides': 23, 'friends': 23, 'themselves': 23, 'comes': 23, 'sport': 23, 'free': 23, 'wrapped': 23, 'jet': 23, 'tricycle': 23, 'closed': 23, 'cloudy': 23, 'pit': 23, 'ahead': 23, 'rests': 23, 'officer': 23, 'pine': 23, 'asleep': 23, 'father': 23, 'tail': 23, 'tents': 23, 'action': 23, 'crouching': 23, 'valley': 23, 'smokes': 23, 'working': 23, 'golf': 23, 'though': 22, 'almost': 22, 'cheerleaders': 22, 'following': 22, 'thumbs': 22, 'redheaded': 22, 'dives': 22, 'type': 22, 'sculpture': 22, 'barrel': 22, 'moves': 22, 'flips': 22, 'lift': 22, 'swimmer': 22, 'strip': 22, 'multicolored': 22, 'fake': 22, 'candles': 22, 'colors': 22, 'eastern': 22, 'plain': 22, 'class': 22, 'towel': 22, 'ribbon': 22, 'first': 22, 'cones': 22, 'hike': 22, 'warm': 22, 'tattoos': 22, 'rainbow': 22, 'square': 22, 'gathering': 22, 'puck': 22, 'woodland': 21, 'enjoy': 21, 'merry': 21, 'cave': 21, 'image': 21, 'sandals': 21, 'surfers': 21, 'knee': 21, 'kiddie': 21, 'fans': 21, 'brush': 21, 'palm': 21, 'mat': 21, 'patio': 21, 'students': 21, 'passes': 21, 'leads': 21, 'wheeled': 21, 'wheeler': 21, 'dust': 21, 'topless': 21, 'vests': 21, 'sharp': 21, 'catcher': 21, 'chews': 21, 'wine': 21, 'time': 21, 'crosswalk': 21, 'tiger': 21, 'bow': 21, 'beam': 21, 'kisses': 21, 'scarves': 21, 'soda': 21, 'part': 21, 'speaking': 21, 'vehicles': 21, 'spraying': 21, 'happy': 20, 'peak': 20, 'silhouette': 20, 'turned': 20, 'also': 20, 'foam': 20, 'sprinklers': 20, 'help': 20, 'herself': 20, 'living': 20, 'church': 20, 'center': 20, 'indian': 20, 'leading': 20, 'touches': 20, 'crawls': 20, 'move': 20, 'multi': 20, 'aged': 20, 'place': 20, 'turns': 20, 'groom': 20, '4': 20, 'item': 20, 'scaling': 20, 'sofa': 20, 'shower': 20, 'members': 20, 'embrace': 20, 'tutu': 20, 'fancy': 20, 'spread': 20, 'itself': 20, 'pirate': 20, 'seats': 20, 'swan': 20, 'stares': 19, '5': 19, 'case': 19, 'various': 19, 'what': 19, 'cover': 19, 'chased': 19, 'tackling': 19, 'rollerblader': 19, 'protective': 19, 'inline': 19, 'tables': 19, 'carriage': 19, 'snowsuit': 19, 'leashes': 19, 'ridden': 19, 'japanese': 19, 'agility': 19, 'bare': 19, 'sleeps': 19, 'circle': 19, 'grab': 19, 'rafting': 19, 'pitbull': 19, 'foggy': 19, 'birthday': 19, 'skull': 19, 'rug': 19, 'motion': 19, 'stomach': 19, 'innertube': 19, 'beads': 19, 'boogie': 19, 'benches': 19, 'cheek': 19, 'huddle': 19, 'licks': 19, 'hard': 19, 'scuba': 19, 'natural': 18, 'hallway': 18, 'sideways': 18, 'icy': 18, 'cafe': 18, 'wind': 18, 'banner': 18, 'walls': 18, 'not': 18, 'oklahoma': 18, 'pajamas': 18, 'pouring': 18, 'work': 18, '3': 18, 'neighborhood': 18, 'fun': 18, 'thin': 18, 'markings': 18, 'retrieving': 18, 'teen': 18, 'print': 18, 'pair': 18, 'alley': 18, 'ship': 18, 'you': 18, 'farm': 18, 'device': 18, 'money': 18, 'picking': 18, 'single': 18, 'boxer': 18, 'boulder': 18, 'sheet': 18, 'drums': 18, 'listening': 18, 'workers': 18, 'shoot': 18, 'played': 18, 'skinny': 18, 'geese': 18, 'rows': 18, 'flops': 18, 'tulips': 18, 'spiderman': 18, 'apron': 18, 'railroad': 18, 'sumo': 18, 'shade': 17, 'sleeved': 17, 'cheerleader': 17, 'floats': 17, 'bounds': 17, 'glove': 17, 'protest': 17, 'followed': 17, 'beneath': 17, 'scales': 17, 'carry': 17, 'how': 17, 'swimsuits': 17, 'headfirst': 17, 'fishes': 17, 'bleachers': 17, 'gallery': 17, 'partially': 17, 'flock': 17, 'bounce': 17, 'fabric': 17, 'foliage': 17, 'wheels': 17, 'worker': 17, 'lead': 17, 'compete': 17, 'shoe': 17, 'mostly': 17, 'eight': 17, 'brunette': 17, 'bends': 17, 'photographs': 17, 'stuck': 17, 'camel': 17, 'camels': 17, 'attempt': 17, 'fruit': 17, 'raising': 17, 'lips': 17, 'barren': 17, 'teammate': 17, 'silly': 17, 'runners': 17, 'crawling': 17, 'marching': 17, 'emerges': 17, 'post': 17, 'so': 17, '8': 17, 'doberman': 17, 'wrestlers': 17, 'feeding': 17, 'obama': 17, 'chalk': 17, 'bowling': 17, 'stripes': 16, 'grabs': 16, 'starting': 16, 'flipping': 16, 'polka': 16, 'chew': 16, 'sheer': 16, 'lands': 16, 'sings': 16, 'style': 16, 'slightly': 16, 'pillow': 16, 'cape': 16, 'narrow': 16, 'amidst': 16, 'autumn': 16, 'racers': 16, 'guard': 16, 'breaking': 16, 'daughter': 16, 'buckets': 16, 'i': 16, 'traveling': 16, 'females': 16, 'figure': 16, 'van': 16, 'treat': 16, 'kayaker': 16, 'touch': 16, 'weather': 16, 'drive': 16, 'teal': 16, 'sets': 16, 'sneakers': 16, 'speaks': 16, 'desk': 16, 'tops': 16, 'sniffs': 16, 'jockeys': 16, 'pitch': 16, 'distant': 16, 'floral': 16, 'robe': 16, 'sail': 16, 'casting': 16, 'second': 16, 'males': 16, 'concert': 16, 'rear': 16, 'plaza': 16, 'caps': 16, 'houses': 16, 'sword': 16, 'bathroom': 16, 'hood': 16, 'deer': 16, 'roof': 15, 'advertisement': 15, 'space': 15, 'pitcher': 15, 'spins': 15, 'point': 15, 'murky': 15, 'stool': 15, 'shovel': 15, 'crane': 15, 'grabbing': 15, 'athlete': 15, 'pet': 15, 'stair': 15, 'control': 15, 'participate': 15, 'crossed': 15, 'chinese': 15, 'retrieves': 15, 'leafy': 15, 'jewelry': 15, 'tossing': 15, 'meadow': 15, 'electric': 15, 'musicians': 15, 'relaxing': 15, 'elephant': 15, 'athletic': 15, 'beagle': 15, 'bikinis': 15, 'gentleman': 15, 'approaching': 15, 'sledding': 15, 'headed': 15, 'bush': 15, 'business': 15, 'straight': 15, 'boarding': 15, 'crying': 15, 'lifts': 15, 'khaki': 15, 'push': 15, 'stairway': 15, 'instrument': 15, 'shaved': 15, 'robes': 15, 'arcade': 15, 'been': 15, 'chocolate': 15, 'jack': 15, 'infant': 15, 'masks': 15, 'monument': 15, 'position': 15, 'midst': 15, 'candy': 15, 'cloud': 15, 'buried': 15, 'appear': 15, 'officers': 15, 'speeds': 15, 'human': 15, 'driveway': 15, 'skyline': 15, 'tied': 15, 'hardhat': 15, 'helmeted': 15, 'marked': 15, 'crouched': 15, 'bay': 15, 'playpen': 15, 'bath': 15, 'bathtub': 15, 'campfire': 15, 'binoculars': 15, 'tripod': 14, 'meal': 14, 'broken': 14, 'luggage': 14, 'visible': 14, 'navy': 14, 'strange': 14, 'flames': 14, 'flat': 14, 'reflection': 14, 'digs': 14, 'uniformed': 14, 'lines': 14, 'artist': 14, 'suv': 14, 'mural': 14, 'plant': 14, 'follow': 14, 'still': 14, 'opponent': 14, 'jockey': 14, 'branches': 14, 'tag': 14, 'stars': 14, 'numbered': 14, 'kites': 14, 'horizon': 14, 'festival': 14, 'stretching': 14, 'clad': 14, 'uphill': 14, 'dreadlocks': 14, 'ponytail': 14, 'photographed': 14, 'wakeboard': 14, 'read': 14, 'pacifier': 14, 'arena': 14, 'intersection': 14, 'tri': 14, 'cage': 14, 'traditional': 14, 'link': 14, 'pigtails': 14, 'rowboat': 14, 'sparklers': 14, 'club': 14, 'knees': 14, 'opens': 14, 'horseback': 14, 'pale': 14, 'tugging': 14, 'bungee': 14, 'belly': 14, 'countryside': 14, 'shoveling': 14, 'wakeboarding': 14, 'scenic': 14, 'logs': 14, 'either': 14, 'toddlers': 14, 'fountains': 14, 'waist': 14, 'stops': 13, 'enclosed': 13, 'pointed': 13, 'paw': 13, 'hiding': 13, 'snowcapped': 13, 'hikes': 13, 'cameras': 13, 'poster': 13, 'cast': 13, 'individuals': 13, 'swimmers': 13, 'identical': 13, 'camping': 13, 'apple': 13, 'tosses': 13, 'ridge': 13, 'tub': 13, 'drawing': 13, 'covers': 13, 'pony': 13, 'mall': 13, 'balance': 13, 'spot': 13, 'lighting': 13, 'trotting': 13, 'sweaters': 13, 'stretches': 13, 'pushed': 13, 'opening': 13, 'motorcyclists': 13, 'checkered': 13, 'approaches': 13, 'museum': 13, 'lane': 13, 'waterskier': 13, 'snowmobile': 13, 'multiple': 13, 'dancer': 13, 'cows': 13, 'dimly': 13, 'descending': 13, 'fisherman': 13, 'kayaks': 13, 'musical': 13, 'growling': 13, 'jumper': 13, 'bundled': 13, 'star': 13, 'poodles': 13, 'rollerskating': 13, 'soft': 13, 'costumed': 13, 'doors': 13, 'lounge': 13, 'dot': 13, 'handlebars': 13, 'jogs': 13, 'booth': 13, 'slip': 13, 'firetruck': 13, 'tv': 13, 'oriental': 13, 'toilet': 13, 'floaties': 13, 'cards': 13, 'wig': 13, 'barn': 13, 'dropping': 12, 'flute': 12, 'parka': 12, 'wades': 12, 'laptop': 12, 'selling': 12, 'vendor': 12, 'rings': 12, 'hut': 12, 'sing': 12, 'far': 12, 'hardwood': 12, 'silhouetted': 12, 'squirted': 12, 'brother': 12, 'fan': 12, 'aerial': 12, 'excited': 12, 'ran': 12, 'performer': 12, 'collars': 12, 'gestures': 12, 'tattooed': 12, 'relaxes': 12, 'chicken': 12, 'pen': 12, 'underwear': 12, 'marker': 12, 'ribbons': 12, 'stare': 12, 'goat': 12, '#': 12, 'skimpy': 12, 'pick': 12, 'reflective': 12, 'wand': 12, 'batman': 12, 'trashcan': 12, 'picks': 12, 'burning': 12, 'handles': 12, 'fuzzy': 12, 'colourful': 12, 'umbrellas': 12, 'snowboarders': 12, 'color': 12, 'fireworks': 12, 'items': 12, 'trainer': 12, 'husky': 12, 'cooking': 12, 'feather': 12, 'summer': 12, 'weeds': 12, 'tropical': 12, 'pathway': 12, 'dribbling': 12, 'babies': 12, 'canoes': 12, 'office': 12, 'crown': 12, 'paints': 12, 'swords': 12, 'soaked': 12, 'rally': 12, 'member': 12, 'shoots': 12, 'attack': 12, 'dusty': 12, 'helicopter': 12, 'railings': 12, 'carts': 12, 'bee': 12, 'including': 11, 'fireplace': 11, 'advertising': 11, 'peeks': 11, 'bottles': 11, 'tight': 11, 'pedestrians': 11, 'new': 11, 'facial': 11, 'leotard': 11, 'skateboarders': 11, 'cheer': 11, 'folding': 11, 'gliding': 11, 'windows': 11, 'stretch': 11, 'seaweed': 11, 'descends': 11, 'drag': 11, 'heavily': 11, 'leashed': 11, 'cushion': 11, 'bouncy': 11, 'limb': 11, 'halloween': 11, 'clapping': 11, 'skies': 11, 'snake': 11, 'graffitied': 11, 'wade': 11, 'aqua': 11, 'come': 11, 'flowery': 11, 'dish': 11, 'sprays': 11, 'mess': 11, 'bounding': 11, 'overhead': 11, 'o': 11, 'tiny': 11, 'bunny': 11, 'hurdles': 11, 'beverage': 11, 'elaborate': 11, 'blocking': 11, 'army': 11, 'similar': 11, 'boards': 11, 'injured': 11, 'pan': 11, 'own': 11, 'fellow': 11, 'works': 11, 'leaf': 11, 'casts': 11, 'carpeted': 11, 'mom': 11, 'bread': 11, 'headdress': 11, 'observes': 11, 'diver': 11, 'sporting': 11, 'incline': 11, 'multicolor': 11, 'kicked': 11, 'wild': 11, 'motor': 11, 'nighttime': 11, 'dive': 11, 'was': 11, 'container': 11, 'tights': 11, 'footballer': 11, 'glider': 11, 'tricycles': 11, 'native': 11, 'seagulls': 11, 'calm': 10, 'electronic': 10, 'handle': 10, 'great': 10, 'religious': 10, 'photos': 10, 'ends': 10, 'roll': 10, 'cheering': 10, 'score': 10, 'lifting': 10, 'barks': 10, 'lacrosse': 10, 'saying': 10, 'sat': 10, 'college': 10, 'within': 10, 'turquoise': 10, 'mini': 10, 'heels': 10, 'patterned': 10, 'tightrope': 10, 'nice': 10, 'tile': 10, 'fat': 10, 'lifted': 10, 'whistle': 10, 'parallel': 10, 'backpacker': 10, 'noses': 10, 'floppy': 10, 'chickens': 10, 'bottoms': 10, 'returns': 10, 'dangling': 10, 'warmly': 10, 'canal': 10, 'papers': 10, 'blurred': 10, 'give': 10, 'logo': 10, 'checking': 10, 'owners': 10, 'share': 10, 'mouse': 10, 'pours': 10, 'choppy': 10, 'strollers': 10, 'mug': 10, 'sort': 10, 'cans': 10, 'boxes': 10, 'zip': 10, 'backstroke': 10, 'rag': 10, 'cats': 10, 'stretched': 10, 'nap': 10, 'snowboards': 10, 'shops': 10, 'things': 10, 'tractor': 10, 'wakeboarder': 10, 'pot': 10, 'sunlight': 10, 'mans': 10, 'overlooks': 10, 'european': 10, 'flannel': 10, 'progress': 10, 'practices': 10, 'parasailing': 10, 'whist': 10, 'canyon': 10, 'pauses': 10, 'everywhere': 10, 'sprints': 10, 'driven': 10, 'bearing': 10, 'bone': 10, 'artificial': 10, 'tuxedos': 10, 'carried': 10, 'operating': 10, 'bigger': 10, 'love': 10, 'furniture': 10, 'posts': 10, 'youth': 10, 'stove': 10, 'pizza': 10, 'shed': 10, 'alike': 10, 'garage': 10, 'accordion': 10, 'sidelines': 10, 'target': 10, 'examine': 10, 'pumpkins': 10, 'punching': 10, 'hydrant': 10, 'hospital': 10, '6': 9, 'sponsored': 9, 'peeking': 9, 'border': 9, 'interacting': 9, 'backdrop': 9, 'juggling': 9, 'cute': 9, 'socks': 9, 'piano': 9, 'singer': 9, 'splits': 9, 'hall': 9, 'dinner': 9, 'suburban': 9, 'folded': 9, 'plank': 9, 'power': 9, 'travels': 9, 'guitars': 9, 'frolics': 9, 'polo': 9, 'spider': 9, 'crashes': 9, 'skaters': 9, 'scenery': 9, 'asking': 9, 'pretending': 9, 'uncut': 9, 'amongst': 9, 'hound': 9, 'meet': 9, 'extreme': 9, 'horizontal': 9, 'numbers': 9, 'watercraft': 9, 'snowbank': 9, 'chunk': 9, 'vertical': 9, 'official': 9, 'marsh': 9, 'sailboat': 9, 'eyed': 9, 'harnesses': 9, 'university': 9, 'dancers': 9, 'bridesmaids': 9, 'button': 9, 'bicycler': 9, 'peers': 9, 'pad': 9, 'trots': 9, 'launches': 9, 'cob': 9, 'mobile': 9, 'sister': 9, 'belongings': 9, 'bowls': 9, 'magazines': 9, 'tossed': 9, 'skiiers': 9, 'pads': 9, 'parents': 9, 'interesting': 9, 'mustache': 9, 'joy': 9, 'transportation': 9, 'darkened': 9, 'garbage': 9, 'trash': 9, 'jumpsuit': 9, 'bent': 9, 'quarterback': 9, 'placed': 9, 'cups': 9, 'marathon': 9, 'dried': 9, 'gymnast': 9, 'squatting': 9, 'yawning': 9, 'garb': 9, 'pug': 9, 'directions': 9, 'squirrel': 9, 'ninja': 9, 'security': 9, 'dandelion': 9, 'tourists': 9, 'skeleton': 9, 'guitarist': 9, 'chopsticks': 9, 'breed': 9, 'forested': 9, 'pitching': 9, 'emerging': 9, 'bound': 9, 'stones': 9, 'retrieve': 9, 'raincoat': 9, 'juice': 9, 'fencing': 9, 'characters': 9, 'billboard': 9, 'asphalt': 9, 'overpass': 9, 'doll': 9, 'lower': 9, 'extended': 9, 'expression': 9, 'mountainous': 9, 'boston': 9, 'squirting': 9, 'stall': 9, 'sleds': 9, 'dragging': 9, 'industrial': 9, 'checks': 9, 'labrador': 9, 'shirted': 9, 'performance': 9, 'belt': 9, 'storm': 9, 'muscular': 9, 'telephone': 9, 'strapped': 9, 'windsurfer': 9, 'padded': 9, 'tackles': 9, 'raise': 9, 'bulldog': 9, 'movie': 9, 'ramps': 9, 'hilly': 9, 'obstacles': 9, 'bears': 9, 'marble': 9, 'cobblestone': 9, 'books': 9, 'pinata': 9, 'gown': 9, 'evening': 9, 'bagpipes': 9, 'plates': 9, 'tiled': 9, 'aiming': 9, 'theater': 9, 'badminton': 9, 'television': 9, 'ballet': 9, 'rocket': 9, 'st': 9, 'potato': 9, 'broom': 9, 'yelling': 9, 'robot': 9, 'rolled': 9, 'backlit': 8, 'couples': 8, 'kitten': 8, 'passengers': 8, 'starts': 8, 'ceiling': 8, 'teddy': 8, 'popping': 8, 'fetches': 8, 'strewn': 8, 'enter': 8, 'tarp': 8, 'distorted': 8, 'chin': 8, 'bounces': 8, 'circuit': 8, 'boxers': 8, 'rusty': 8, 'embracing': 8, 'tape': 8, 'rod': 8, 'potted': 8, 'start': 8, 'unusual': 8, 'practice': 8, 'sharing': 8, 'trucks': 8, 'disk': 8, 'patches': 8, 'ditch': 8, 'stepping': 8, 'clearing': 8, 'marketplace': 8, 'ascending': 8, 'pokes': 8, 'sleeveless': 8, 'rack': 8, 'steering': 8, 'hi': 8, 'straps': 8, 'maroon': 8, 'dalmation': 8, 'listens': 8, 'participating': 8, 'watermelon': 8, 'legged': 8, 'guarding': 8, 'exhibit': 8, 'highway': 8, 'surprised': 8, 'printed': 8, 'seashore': 8, 'studio': 8, 'clothed': 8, 'celebrating': 8, 'claus': 8, 'musician': 8, 'sucking': 8, 'beanie': 8, 'blocked': 8, 'written': 8, 'act': 8, 'gas': 8, 'containing': 8, 'serve': 8, 'too': 8, 'village': 8, 'feathers': 8, 'tussle': 8, 'fairy': 8, 'pyramid': 8, 'heart': 8, 'bagpipe': 8, 'medieval': 8, 'trumpet': 8, 'batter': 8, 'herding': 8, 'ticket': 8, 'kilt': 8, 'handed': 8, 'skips': 8, 'located': 8, 'baseman': 8, 'pounces': 8, 'fashioned': 8, 'bucking': 8, 'trains': 8, 'feathered': 8, 'purses': 8, 'creating': 8, 'clings': 8, 'monster': 8, 'lush': 8, 'policemen': 8, 'shovels': 8, 'snack': 8, 'attention': 8, 'amid': 8, 'son': 8, 'serious': 8, 'beak': 8, 'earrings': 8, 'kayakers': 8, 'telescope': 8, 'cartwheel': 8, 'seattle': 8, 'hotel': 8, 'reddish': 8, 'reflecting': 8, 'stripe': 8, 'romp': 8, 'pets': 8, 'cycling': 8, 'sailing': 8, 'examining': 8, 'cleaning': 8, 'jogger': 8, 'residential': 8, 'apples': 8, 'domino': 8, 'sacks': 8, 'port': 8, 'stores': 8, 'paperwork': 8, 'drummer': 8, 'firing': 8, 'coloring': 8, 'vine': 8, 'starring': 8, 'tracksuit': 8, 'cries': 8, 'newborn': 8, 'words': 8, 'cutting': 8, 'chip': 8, 'milk': 8, 'statues': 8, 'magazine': 8, 'library': 8, 'eagle': 8, 'cannon': 8, 'sleeve': 8, 'escalator': 8, 'record': 7, 'outcropping': 7, 'digital': 7, 'use': 7, 'banners': 7, 'upward': 7, 'hello': 7, 'kitty': 7, 'ankle': 7, 'packed': 7, 'parasail': 7, 'homemade': 7, 'superman': 7, 'material': 7, 'sale': 7, 'somthing': 7, 'beginning': 7, 'soaking': 7, 'challenging': 7, 'steers': 7, 'hides': 7, 'trench': 7, 'castle': 7, 'outstreached': 7, 'rescue': 7, 'athletes': 7, 'unseen': 7, 'shephard': 7, 'lining': 7, 'butt': 7, 'grins': 7, 'lasso': 7, 'carrier': 7, 'bin': 7, 'enclosure': 7, 'viz': 7, 'motorbikes': 7, 'bridal': 7, 'ten': 7, 'cop': 7, 'handing': 7, 'pop': 7, 'chubby': 7, 'writes': 7, 'soaring': 7, 'batting': 7, 'speak': 7, 'elevator': 7, 'machines': 7, 'handbag': 7, 'mickey': 7, 'horns': 7, 'fist': 7, 'chainsaw': 7, 'carving': 7, 'hopping': 7, 'dachshund': 7, 'really': 7, 'seating': 7, 'meeting': 7, 'gymnastics': 7, 'slopes': 7, 'beret': 7, 'camo': 7, 'backpacking': 7, 'overhang': 7, 'jagged': 7, 'peach': 7, 'soldier': 7, 'cable': 7, 'rails': 7, 'airport': 7, 'wheelbarrow': 7, 'cliffs': 7, 'dollar': 7, 'pieces': 7, 'gymnasium': 7, 'overweight': 7, 'portrait': 7, 'deflated': 7, 'butterfly': 7, 'order': 7, 'melting': 7, 'thumb': 7, 'shades': 7, 'watery': 7, 'playful': 7, 'checked': 7, 'rival': 7, 'cowboys': 7, 'corgi': 7, 'jar': 7, 'juggles': 7, 'pointy': 7, 'tags': 7, 'crash': 7, 'peaks': 7, 'teaching': 7, 'pumpkin': 7, 'sparkler': 7, 'resort': 7, 'boarder': 7, 'awning': 7, 'slalom': 7, 'interact': 7, 'community': 7, 'baskets': 7, 'sleeves': 7, 'spout': 7, 'necked': 7, 'knife': 7, 'lipstick': 7, 'travel': 7, 'toss': 7, 'festive': 7, 'test': 7, 'observing': 7, 'fairground': 7, 'oar': 7, 'campsite': 7, 'puffy': 7, 'sad': 7, 'droplets': 7, 'performers': 7, 'parasails': 7, 'themed': 7, 'pail': 7, 'florida': 7, 'firefighter': 7, 'thick': 7, 'tee': 7, 'stunts': 7, 'squirt': 7, 'begging': 7, 'gathers': 7, 'attacking': 7, 'check': 7, 'haircut': 7, 'trailing': 7, 'treads': 7, 'expressions': 7, 'speeding': 7, 'galloping': 7, 'recently': 7, 'pattern': 7, 'spoon': 7, 'brindle': 7, 'forehead': 7, 'site': 7, 'colorfully': 7, 'clean': 7, 'crossbones': 7, 'buy': 7, 'bug': 7, 'struggle': 7, 'seagull': 7, 'flaming': 7, 'priest': 7, 'turkeys': 7, 'twin': 7, 'bouquet': 7, 'french': 7, 'smelling': 7, 'participates': 7, 'submerged': 7, 'bale': 7, 'tongues': 7, 'bookstore': 7, 'evil': 7, 'letters': 7, 'bitten': 7, 'cigarettes': 7, 'hell': 7, 'braids': 6, 'canopy': 6, 'int': 6, 'makeshift': 6, 'sell': 6, 'rottweiler': 6, 'strings': 6, 'confetti': 6, 'pretty': 6, 'card': 6, 'posters': 6, 'struggles': 6, 'piggy': 6, 'seeds': 6, 'carefully': 6, 'shines': 6, 'bark': 6, 'breaks': 6, 'dunking': 6, 'greenhouse': 6, 'tools': 6, 'world': 6, 'framed': 6, 'tournament': 6, 'sparring': 6, 'shining': 6, 'lockers': 6, 'mossy': 6, 'formally': 6, 'hairy': 6, 'canvas': 6, 'backward': 6, 'curtain': 6, 'cords': 6, 'beyond': 6, 'quad': 6, 'gnawing': 6, 'waterfalls': 6, 'guns': 6, 'presentation': 6, 'dj': 6, 'craft': 6, 'camcorder': 6, 'location': 6, 'ferry': 6, 'dolphin': 6, 'swampy': 6, 'snowman': 6, 'twirling': 6, '(': 6, 'apart': 6, 'strap': 6, 'rv': 6, 'classic': 6, 'prepare': 6, 'twisting': 6, 'except': 6, 'profile': 6, 'trails': 6, 'watermelons': 6, 'teaches': 6, 'dim': 6, 'scantily': 6, 'goats': 6, 'earphones': 6, 'circular': 6, 'perched': 6, 'liquid': 6, 'lunges': 6, 'celebrate': 6, 'formations': 6, 'parachuting': 6, 'tool': 6, 'repels': 6, 'soars': 6, 'conversation': 6, 'rocking': 6, 'storefront': 6, 'entrance': 6, 'dunes': 6, 'summit': 6, 'blindfolded': 6, 'whose': 6, 'union': 6, 'creature': 6, 'posed': 6, 'maneuvers': 6, 'flipped': 6, 'motorized': 6, 'fog': 6, 'series': 6, 'toe': 6, 'filling': 6, 'canoeing': 6, 'smooth': 6, 'waterway': 6, 'crowds': 6, 'complete': 6, 'finish': 6, 'stripped': 6, 'stoop': 6, 'footballers': 6, 'league': 6, 'cameraman': 6, 'lime': 6, 'caution': 6, 'playhouse': 6, 'diner': 6, 'navigates': 6, 'greenery': 6, 'shrubs': 6, 'bumpy': 6, 'hear': 6, 'windsurfing': 6, 'fencers': 6, 'upper': 6, 'gazes': 6, 'jesus': 6, 'song': 6, 'rafts': 6, 'glides': 6, 'newspapers': 6, 'shiny': 6, 'wilderness': 6, 'nature': 6, 'screams': 6, 'morning': 6, 'bracelet': 6, 'surfboarder': 6, 'dyed': 6, 'good': 6, 'tails': 6, 'afternoon': 6, 'bib': 6, 'called': 6, 'lamp': 6, 'glacier': 6, 'computers': 6, 'lens': 6, 'individual': 6, 'dumps': 6, 'halter': 6, 'drain': 6, 'apartment': 6, 'collide': 6, 'tumbling': 6, 'sails': 6, 'earring': 6, 'size': 6, 'wrestles': 6, 'overlook': 6, 'squirts': 6, 'ollie': 6, 'terriers': 6, 'built': 6, 'fireman': 6, 'camp': 6, 'eyebrows': 6, 'moustache': 6, 'cellphones': 6, 'change': 6, 'exercise': 6, 'form': 6, 'gondola': 6, 'skyscraper': 6, 'squats': 6, 'self': 6, 'cheeks': 6, 'diaper': 6, 'fields': 6, 'striking': 6, 'keep': 6, 'misty': 6, 'stuff': 6, 'drenched': 6, 'saddle': 6, 'much': 6, 'if': 6, 'contest': 6, 'pro': 6, 'pelican': 6, 'harbor': 6, 'suited': 6, 'numerous': 6, 'defending': 6, 'swans': 6, 'messy': 6, 'insect': 6, 'cartwheels': 6, 'hooping': 6, 'used': 6, 'easter': 6, 'march': 6, 'hawaiian': 6, 'michael': 6, 'jackson': 6, 'pierced': 6, 'cutout': 6, 'rubs': 6, 'tutus': 6, 'mountaineer': 6, 'gazing': 6, 'armenian': 6, 'genocide': 6, 'pretend': 6, 'kangaroo': 6, 'paintball': 6, 'map': 6, 'mascot': 6, 'sidecar': 6, 'hamburgers': 6, 'frying': 6, 'spotlight': 6, 'buggy': 6, 'bathrobe': 6, 'graduation': 6, 'dolphins': 6, 'croquet': 5, 'drops': 5, 'bluff': 5, 'rainy': 5, 'beers': 5, 'igloo': 5, 'threw': 5, 'sells': 5, 'woven': 5, 'dots': 5, 'leafless': 5, 'rubbing': 5, 'split': 5, 'clinging': 5, 'floors': 5, 'kneel': 5, 'dunks': 5, 'crystal': 5, 'runway': 5, 'poking': 5, 'unfinished': 5, 'jetty': 5, 'monitor': 5, 'paintings': 5, 'bringing': 5, 'twilight': 5, 'somersault': 5, 'somebody': 5, 'pumps': 5, 'incoming': 5, 'campus': 5, 'gesture': 5, 'dining': 5, 'nearly': 5, 'beaded': 5, 'support': 5, 'massive': 5, 'hips': 5, 'carring': 5, 'films': 5, 'closely': 5, 'docked': 5, 'shawl': 5, 'mittens': 5, 'pairs': 5, 'alleyway': 5, ')': 5, 'breeds': 5, 'partly': 5, 'sailor': 5, 'structures': 5, 'gymnastic': 5, 'aid': 5, 'groomsmen': 5, 'cane': 5, 'brownish': 5, 'candle': 5, 'rounds': 5, 'pops': 5, 'sheets': 5, 'atm': 5, 'dalmatian': 5, 'lambs': 5, 'lunch': 5, 'mats': 5, 'interested': 5, 'ties': 5, 'column': 5, 'pillar': 5, 'columns': 5, 'cowgirl': 5, 'special': 5, 'elegant': 5, 'pedestrian': 5, 'irish': 5, 'gated': 5, 'connected': 5, 'pasta': 5, 'observe': 5, 'dad': 5, 'inflated': 5, 'modern': 5, 'screaming': 5, 'paying': 5, 'hops': 5, 'upturned': 5, 'sound': 5, 'clowns': 5, '&': 5, 'chatting': 5, 'miniature': 5, 'mitt': 5, 'maneuver': 5, 'combat': 5, 'watering': 5, 'harnessed': 5, 'pulley': 5, 'congregate': 5, 'possession': 5, 'spring': 5, 'frog': 5, 'plushie': 5, 'breath': 5, 'defenders': 5, 'pig': 5, 'pooh': 5, 'acting': 5, 'higher': 5, 'jog': 5, 'waterfront': 5, 'winding': 5, 'more': 5, 'spiky': 5, 'wake': 5, 'surfboards': 5, 'nude': 5, 'cheers': 5, 'suspenders': 5, 'necklaces': 5, 'teens': 5, 'scratching': 5, 'heading': 5, 'drilling': 5, 'active': 5, 'cookie': 5, 'had': 5, 'intently': 5, 'extends': 5, 'avoid': 5, 'batsman': 5, 'misses': 5, 'unique': 5, 'period': 5, 'bit': 5, 'bulls': 5, 'mist': 5, 'punches': 5, 'moon': 5, 'casual': 5, 'focus': 5, 'guards': 5, 'headset': 5, 'artists': 5, 'stopped': 5, 'return': 5, 'geyser': 5, '23': 5, 'sunshade': 5, 'scary': 5, 'tethered': 5, 'flowing': 5, 'bedroom': 5, 'becomes': 5, 'waterski': 5, 'wires': 5, 'podium': 5, 'viewer': 5, 'current': 5, 'decorative': 5, 'bows': 5, 'shelter': 5, 'shadows': 5, 'upraised': 5, 'ambulance': 5, 'moment': 5, 'coverings': 5, 'reception': 5, 'footprints': 5, 'massage': 5, 'abandoned': 5, 'snowing': 5, 'teacher': 5, 'sledge': 5, 'ridding': 5, 'acrobatic': 5, 'huddled': 5, 'adjusting': 5, 'photographers': 5, 'when': 5, 'lease': 5, 'worn': 5, 'objects': 5, 'strike': 5, 'goofy': 5, 'whom': 5, 'bricks': 5, 'laid': 5, 'than': 5, 'scruffy': 5, 'wrapping': 5, 'tinkerbell': 5, 'jacked': 5, 'saber': 5, 'height': 5, 'tray': 5, 'main': 5, 'piercing': 5, 'calf': 5, 'sling': 5, 'sundress': 5, 'keeps': 5, 'fell': 5, 'mouthed': 5, 'parent': 5, 'scratches': 5, 'kind': 5, 'placing': 5, 'rabbit': 5, 'battle': 5, 'powder': 5, 'mexican': 5, 'aims': 5, 'engine': 5, 'leggings': 5, 'cigars': 5, 'marx': 5, 'equestrian': 5, 'seal': 5, 'squeezing': 5, 'clears': 5, 'arch': 5, 'pasture': 5, 'strips': 5, 'relax': 5, 'converse': 5, 'pursued': 5, 'crocodile': 5, 'mean': 5, 'concerned': 5, 'trip': 5, 'double': 5, 'instructor': 5, 'games': 5, 'bales': 5, 'island': 5, 'speedo': 5, 'wakeboards': 5, 'downtown': 5, 'collared': 5, 'explosion': 5, 'saxophone': 5, 'refrigerator': 5, 'waiter': 5, 'unhappy': 5, 'signal': 5, 'corndogs': 5, 'frames': 5, 'oversized': 5, 'presents': 5, 'ornate': 5, 'model': 5, 'product': 5, 'pirates': 5, 'boa': 5, 'roadway': 5, 'trophy': 5, 'inground': 5, 'devil': 5, 'scooters': 5, 'cheerleading': 5, 'capes': 5, 'freshly': 5, 'tiara': 5, 'zoo': 5, 'chains': 5, 'litter': 5, 'plush': 5, 'handgun': 5, 'feeds': 5, 'eggs': 5, 'clover': 5, 'limousine': 5, 'punk': 5, 'overcoat': 5, 'towed': 5, 'hovering': 5, 'wicker': 5, 'conversing': 5, 'guiding': 5, 'sheltie': 5, 'inspecting': 5, 'legos': 5, 'merchandise': 5, 'stained': 5, 'streaks': 5, 'lion': 5, 'chess': 5, 'lip': 5, 'snap': 5, 'interviews': 5, 'snarling': 5, 'washington': 5, 'observed': 5, 'real': 5, 'products': 5, 'guide': 5, 'derby': 5, 'medals': 5, 'sash': 5, 'peanut': 5, 'butter': 5, 'angels': 5, 'loading': 5, 'nails': 5, 'offering': 5, 'jeeps': 5, 'hopscotch': 5, 'windsurfs': 5, 'sewing': 5, 'chairlift': 5, 'milkshake': 5, 'stools': 5, 'hammer': 4, 'chaps': 4, 'underground': 4, 'wildflowers': 4, 'pitches': 4, 'engage': 4, 'raceway': 4, 'fences': 4, 'greet': 4, 'peaking': 4, 'peering': 4, 'stay': 4, 'piled': 4, 'attractive': 4, 'elder': 4, 'shaded': 4, 'baton': 4, 'toboggan': 4, 'buses': 4, 'hip': 4, 'casually': 4, 'liberty': 4, 'begin': 4, 'bares': 4, 'arab': 4, 'upon': 4, 'learning': 4, 'vault': 4, 'floored': 4, 'dig': 4, 'struggling': 4, 'steel': 4, 'route': 4, 'displaying': 4, 'artwork': 4, 'spar': 4, 'baring': 4, 'hung': 4, 'shallows': 4, 'sniff': 4, 'leave': 4, 'teammates': 4, 'shady': 4, 'repelling': 4, 'newly': 4, 'taxi': 4, 'prizes': 4, 'riverbank': 4, 'supplies': 4, 'pain': 4, 'spreads': 4, 'nike': 4, 'mixing': 4, 'pretends': 4, 'parachutes': 4, 'para': 4, 'exercises': 4, 'horseshoes': 4, 'propped': 4, 'ancient': 4, 'spaniel': 4, 'incense': 4, 'navigating': 4, 'puppet': 4, 'markers': 4, 'feild': 4, 'reached': 4, 'mesh': 4, 'popsicle': 4, 'curious': 4, 'places': 4, 'launch': 4, 'masked': 4, 'barefooted': 4, 'poised': 4, 'elephants': 4, 'turbans': 4, 'note': 4, 'dappled': 4, 'walked': 4, 'curvy': 4, 'capped': 4, 'chested': 4, 'yoga': 4, 'upwards': 4, 'balding': 4, 'earth': 4, 'loop': 4, 'file': 4, 'barbed': 4, 'retreiver': 4, 'chat': 4, 'directs': 4, 'saw': 4, 'blacktop': 4, 'fleece': 4, 'master': 4, 'bamboo': 4, 'shine': 4, 'closes': 4, 'wintry': 4, 'shapes': 4, 'streaked': 4, 'rushing': 4, 'searching': 4, 'fit': 4, 'gloved': 4, 'executes': 4, 'walker': 4, 'snows': 4, 'rise': 4, 'casino': 4, 'blindfolds': 4, 'boulders': 4, 'british': 4, '13': 4, 'crab': 4, 'workout': 4, 'winnie': 4, 'returning': 4, 'clay': 4, 'fashion': 4, 'puddles': 4, 'waterskies': 4, 'assistance': 4, 'grinning': 4, 'completely': 4, 'salon': 4, 'frowning': 4, 'drift': 4, 'upset': 4, 'ad': 4, 'china': 4, 'ejected': 4, 'receives': 4, 'scottish': 4, 'approach': 4, 'fights': 4, 'pouncing': 4, 'bandaged': 4, 'archway': 4, 'ravine': 4, 'crevasse': 4, 'nips': 4, 'bared': 4, 'cotton': 4, 'victory': 4, 'opponents': 4, 'bluejeans': 4, 'admire': 4, 'admiring': 4, 'props': 4, 'whitewater': 4, 'cash': 4, 'register': 4, 'tips': 4, 'kart': 4, 'cabin': 4, 'manicured': 4, 'brings': 4, 'socializing': 4, 'pickup': 4, 'billowing': 4, 'videotaped': 4, 'ceremony': 4, 'alert': 4, 'corridor': 4, 'records': 4, 'we': 4, 'nears': 4, 'elevation': 4, 'flooded': 4, 'admires': 4, 'hairstyle': 4, 'wipes': 4, 'moss': 4, 'routine': 4, 'auditorium': 4, 'barriers': 4, 'early': 4, 'wristbands': 4, 'tabby': 4, 'dummy': 4, 'gesturing': 4, 'tuxedo': 4, 'reeds': 4, 'sparks': 4, 'bonnets': 4, 'arched': 4, 'tow': 4, 'waterskis': 4, 'brooms': 4, 'depicting': 4, 'velvet': 4, 'coaching': 4, 'faucet': 4, 'spigot': 4, 'tap': 4, 'slacks': 4, 'ascends': 4, 'skatepark': 4, 'carying': 4, 'terrace': 4, 'waterside': 4, 'snowmobiles': 4, 'sooner': 4, 'marks': 4, 'demonstrating': 4, 'cargo': 4, 'causing': 4, 'spiral': 4, 'contents': 4, 'lobby': 4, 'squat': 4, 'lie': 4, 'curved': 4, 'traverses': 4, 'rushes': 4, 'curiously': 4, 'icicle': 4, 'stripy': 4, 'canon': 4, 'torn': 4, 'australian': 4, 'spiked': 4, 'spreading': 4, 'flapping': 4, 'quilt': 4, 'grown': 4, 'skipping': 4, 'pillows': 4, 'bananas': 4, 'garter': 4, 'laptops': 4, 'battling': 4, 'rollerskates': 4, 'booths': 4, 'brushes': 4, 'dragged': 4, 'gowns': 4, 'suitcase': 4, 'bedspread': 4, 'louis': 4, 'vuitton': 4, 'mechanical': 4, 'goatee': 4, 'floatation': 4, 'missing': 4, 'call': 4, 'theme': 4, 'led': 4, 'cuts': 4, 'fresh': 4, 'hugged': 4, 'nightclub': 4, 'rowers': 4, 'frisbees': 4, 'surround': 4, 'seems': 4, 'cracked': 4, 'sweatshirts': 4, 'whispering': 4, 'mark': 4, 'brothers': 4, 'alligator': 4, 'bowler': 4, 'electrical': 4, 'trailer': 4, 'vending': 4, 'lack': 4, 'defends': 4, 'portable': 4, 'toilets': 4, 'flowering': 4, 'bodies': 4, 'wolf': 4, 'mets': 4, 'story': 4, 'standard': 4, 'n': 4, 'gift': 4, 'jug': 4, 'needle': 4, 'letter': 4, 'produce': 4, 'russell': 4, 'touched': 4, 'soap': 4, 'displayed': 4, 'barricade': 4, 'sizes': 4, 'pinned': 4, 'displays': 4, 'awards': 4, 'obscured': 4, 'sleigh': 4, 'howling': 4, 'groups': 4, 'huskies': 4, 'greenish': 4, 'rises': 4, 'piles': 4, 'suds': 4, 'embraces': 4, 'diners': 4, 'local': 4, 'trim': 4, 'huts': 4, 'assisting': 4, 'washing': 4, 'involving': 4, 'prize': 4, 'embankment': 4, 'diapers': 4, 'roadside': 4, 'ollies': 4, '12': 4, 'offers': 4, 'balanced': 4, 'entering': 4, 'presses': 4, 'will': 4, 'steam': 4, 'lollipop': 4, 'banks': 4, 'washes': 4, 'lounging': 4, 'department': 4, 'film': 4, 'uno': 4, 'wetsuits': 4, 'odd': 4, 'limo': 4, 'figures': 4, 'unknown': 4, 'sequined': 4, 'beat': 4, 'conversations': 4, 'us': 4, 'vegetation': 4, 'shots': 4, 'protection': 4, 'marches': 4, 'siting': 4, 'political': 4, 'sunrise': 4, 'ballerinas': 4, 'astride': 4, 'smartly': 4, 'littered': 4, 'sponge': 4, 'sparkling': 4, 'ok': 4, 'swung': 4, 'unison': 4, 'united': 4, 'states': 4, 'seesaw': 4, 'faded': 4, 'notes': 4, 'blankets': 4, 'stack': 4, 'rooftop': 4, 'company': 4, 'chained': 4, 'burn': 4, 'scrambling': 4, 'streamers': 4, 'meter': 4, 'cutouts': 4, 'heron': 4, 'countertop': 4, 'tide': 4, 'crate': 4, 'picket': 4, 'rifle': 4, 'arrow': 4, 'patiently': 4, 'drawn': 4, 'condoms': 4, 'twirls': 4, 'join': 4, 'rafters': 4, 'build': 4, 'wigs': 4, 'feed': 4, 'tilted': 4, 'corners': 4, 'directly': 4, 'flyer': 4, 'barely': 4, 'shoppers': 4, 'centipede': 4, 'ruins': 4, 'advertisements': 4, 'classroom': 4, 'dandelions': 4, 'veil': 4, 'slipper': 4, 'gigolo': 4, 'nurses': 4, 'brides': 4, 'hopper': 4, 'groucho': 4, 'tagged': 4, 'desks': 4, 'cry': 4, 'parasailer': 4, 'controller': 4, 'pajama': 4, 'those': 4, 'bernard': 4, 'cracker': 4, 'potty': 4, 'impeach': 4, 'flings': 4, 'cigar': 4, 'sink': 4, 'donkeys': 4, 'whales': 4, 'einstein': 4, 'noodle': 4, 'pharmacy': 4, 'dumpster': 4, 'videotaping': 3, 'mix': 3, 'transit': 3, 'nine': 3, 'handicapped': 3, 'railed': 3, 'electronics': 3, 'devices': 3, 'scouts': 3, 'stroll': 3, 'headlights': 3, 'dane': 3, 'onward': 3, 'paperback': 3, 'cots': 3, 'beds': 3, 'hopes': 3, 'mambo': 3, 'humping': 3, 'hurrying': 3, 'kimono': 3, 'iced': 3, 'york': 3, 'expanse': 3, 'turban': 3, 'vaulting': 3, 'gap': 3, 'fill': 3, 'backed': 3, 'grazes': 3, 'super': 3, 'dalmatians': 3, 'desolate': 3, 'ethnic': 3, 'clutching': 3, 'easel': 3, 'fangs': 3, 'moped': 3, 'drivers': 3, 'abseiling': 3, 'wife': 3, 'overgrown': 3, 'grasses': 3, 'taller': 3, 'draped': 3, 'pedal': 3, 'shelf': 3, 'tea': 3, 'beverages': 3, 'touchdown': 3, 'mannequins': 3, 'basset': 3, 'belts': 3, 'competitive': 3, 'safely': 3, 'landed': 3, 'roughly': 3, 'dug': 3, 'coastline': 3, 'horseshoe': 3, 'strikes': 3, 'inspects': 3, 'muscle': 3, 'tourist': 3, 'blood': 3, 'varying': 3, 'dogsled': 3, 'camper': 3, 'peek': 3, 'carved': 3, 'crumbling': 3, 'reflections': 3, 'kept': 3, 'slab': 3, 'hollywood': 3, 'examines': 3, 'quarter': 3, 'prisoner': 3, 'raced': 3, 'remote': 3, 'olympics': 3, 'assist': 3, 'pocket': 3, 'doghouse': 3, 'youn': 3, 'buying': 3, 'nibbling': 3, 'learn': 3, 'slam': 3, 'donut': 3, 'intertube': 3, 'toast': 3, 'angle': 3, 'non': 3, 'shrubbery': 3, 'reacts': 3, 'heard': 3, 'cluster': 3, 'amused': 3, 'carves': 3, 'oxford': 3, 'sipping': 3, 'setter': 3, 'housing': 3, 'interracial': 3, 'farmers': 3, 'messily': 3, 'spaghetti': 3, 'customer': 3, 'web': 3, 'cam': 3, 'tugs': 3, 'spandex': 3, 'panel': 3, 'name': 3, 'badge': 3, 'passenger': 3, 'colander': 3, 'mountaineers': 3, 'sweat': 3, 'stony': 3, 'overhanging': 3, 'phrase': 3, 'springs': 3, 'similarly': 3, 'facility': 3, 'punch': 3, 'temple': 3, 'spotters': 3, 'showering': 3, 'source': 3, 'sloping': 3, 'keeping': 3, 'aim': 3, 'trimmed': 3, 'monk': 3, 'wrap': 3, 'spanish': 3, 'bill': 3, 'bills': 3, 'smeared': 3, 'novelty': 3, 'energizer': 3, 'attraction': 3, 'waring': 3, 'flooring': 3, 'shoeless': 3, 'signing': 3, 'created': 3, 'reindeer': 3, 'chats': 3, 'kimonos': 3, 'winds': 3, 'bra': 3, 'afro': 3, 'dinghy': 3, 'hide': 3, 'crests': 3, 'shelves': 3, 'crevice': 3, 'dresser': 3, 'applying': 3, 'done': 3, 'sparse': 3, 'tattered': 3, 'billboards': 3, 'streaming': 3, 'protects': 3, 'glow': 3, 'decoration': 3, 'overturned': 3, 'burgundy': 3, 'afghan': 3, 'prom': 3, 'lighter': 3, 'protect': 3, 'boot': 3, 'labeled': 3, 'musher': 3, 'drill': 3, 'iron': 3, 'hunched': 3, 'encouraging': 3, 'rundown': 3, 'warehouse': 3, 'wrinkled': 3, 'sight': 3, 'cheered': 3, 'piggyback': 3, 'adorned': 3, 'travelling': 3, 'defensive': 3, 'goalkeeper': 3, 'sandal': 3, 'vintage': 3, 'colliding': 3, 'dinosaur': 3, 'obedience': 3, 'tip': 3, 'lolly': 3, 'growls': 3, 'oars': 3, 'seem': 3, 'twig': 3, 'quietly': 3, 'sloped': 3, 'cookies': 3, 'visor': 3, 'demonstrates': 3, 'pedals': 3, 'upright': 3, 'completes': 3, 'cord': 3, 'because': 3, 'wasteland': 3, 'pebble': 3, 'pebbles': 3, 'rounding': 3, 'bandage': 3, 'wiping': 3, 'treks': 3, 'flailing': 3, 'law': 3, 'aisle': 3, 'praying': 3, 'bands': 3, 'saris': 3, 'environment': 3, 'smock': 3, 'skills': 3, 'egret': 3, 'spilled': 3, 'motorboat': 3, 'determined': 3, 'forefront': 3, 'shrine': 3, 'basement': 3, 'sprinting': 3, 'lagoon': 3, 'fixing': 3, 'swooping': 3, 'mock': 3, 'sees': 3, 'junk': 3, 'rubble': 3, 'bushy': 3, 'arabian': 3, 'offstage': 3, 'cycle': 3, 'shouting': 3, 'rimmed': 3, 'award': 3, 'awaiting': 3, 'tickets': 3, 'yawns': 3, 'groceries': 3, 'kicker': 3, 'peaceful': 3, 'burlap': 3, 'sack': 3, 'vegetables': 3, 'spikes': 3, 'solitary': 3, 'length': 3, 'supports': 3, 'torso': 3, 'soldiers': 3, 'ignoring': 3, 'speaker': 3, 'late': 3, 'secured': 3, 'polaris': 3, 'weird': 3, 'filmed': 3, 'lecture': 3, 'dotted': 3, 'blown': 3, 'grin': 3, 'pedaling': 3, 'wraps': 3, 'headscarfs': 3, 'packages': 3, 'zara': 3, 'passed': 3, 'chewed': 3, 'vacant': 3, 'retrievers': 3, 'darker': 3, 'campground': 3, 'blazing': 3, 'cobbled': 3, 'language': 3, 'fedora': 3, 'tired': 3, 'identically': 3, 'removing': 3, 'everyone': 3, 'labs': 3, 'livestock': 3, 'accompanied': 3, 'fingerpaints': 3, 'decorations': 3, 'oxen': 3, 'dodges': 3, 'sandbox': 3, 'labradoodle': 3, 'rollerskater': 3, 'fort': 3, 'facepaint': 3, 'india': 3, 'strapless': 3, 'conference': 3, 'submerges': 3, 'tooth': 3, 'mowed': 3, 'paraglider': 3, 'engaged': 3, 'sox': 3, 'zigzag': 3, 'atvs': 3, 'descent': 3, 'western': 3, 'revealing': 3, 'phones': 3, 'crack': 3, 'symbol': 3, 'goose': 3, 'youngsters': 3, 'cooks': 3, 'stacks': 3, 'washed': 3, 'retaining': 3, 'occupied': 3, 'nipple': 3, 'piercings': 3, 'aquarium': 3, 'safari': 3, 'rods': 3, 'firefighters': 3, 'greyish': 3, 'kissed': 3, 'blues': 3, 'coated': 3, 'happening': 3, 'balck': 3, 'stride': 3, 'shocked': 3, 'messenger': 3, 'ink': 3, 'images': 3, 'passerby': 3, 'squinting': 3, 'grasps': 3, 'cook': 3, 'intense': 3, 'captured': 3, 'tower': 3, 'licked': 3, 'paddled': 3, 'fighters': 3, 'panting': 3, 'u': 3, 'crouch': 3, 'dramatically': 3, 'purchasing': 3, 'hundreds': 3, 'potties': 3, 'blossoms': 3, 'exiting': 3, 'blossoming': 3, 'topped': 3, 'loaded': 3, 'vast': 3, 'find': 3, 'skin': 3, 'thatched': 3, 'penske': 3, 'awkwardly': 3, 'cardigan': 3, 'restaraunt': 3, 'america': 3, 'saxophones': 3, 'married': 3, '7': 3, 'menus': 3, 'menu': 3, 'elevated': 3, 'minivan': 3, 'say': 3, 'tugboat': 3, 'most': 3, 'winks': 3, 'winking': 3, 'drapped': 3, 'tether': 3, 'mime': 3, 'scared': 3, 'apparatus': 3, 'carousel': 3, 'biplane': 3, 'purchase': 3, 'ragged': 3, 'wielding': 3, 'feature': 3, 'otherwise': 3, 'collection': 3, 'royal': 3, 'basketballs': 3, 'frolicking': 3, 'loose': 3, 'prancing': 3, 'finished': 3, 'fiery': 3, 'windy': 3, 'foothills': 3, 'prairie': 3, 'draft': 3, 'curled': 3, 'else': 3, 'grilling': 3, 'tandem': 3, 'yorkie': 3, 'winner': 3, 'fatigues': 3, 'boundary': 3, 'americans': 3, 'confused': 3, 'sari': 3, 'dome': 3, 'speckled': 3, 'pins': 3, 'were': 3, 'establishment': 3, 'pre': 3, 'cliffside': 3, 'dragon': 3, 'god': 3, 'clap': 3, 'dove': 3, 'applies': 3, 'engulfed': 3, 'firemen': 3, 'laps': 3, 'tangled': 3, 'ultimate': 3, 'straddles': 3, 'sprint': 3, 'dye': 3, 'laundry': 3, 'mop': 3, 'prevent': 3, 'strolls': 3, 'chops': 3, 'pistol': 3, 'protesters': 3, 'rush': 3, 'gull': 3, 'sorts': 3, 'shadowed': 3, 'wheat': 3, 'draw': 3, 'crib': 3, 'chili': 3, 'escape': 3, 'tiles': 3, 'keyboard': 3, 'noodles': 3, 'trekking': 3, 'hooking': 3, 'railway': 3, 'browses': 3, 'ornamental': 3, 'nets': 3, 'penguins': 3, 'zombie': 3, '28': 3, 'marking': 3, 'armbands': 3, 'thorugh': 3, 'squeeze': 3, 'downward': 3, 'views': 3, 'pigeon': 3, 'crocs': 3, 'activity': 3, 'bass': 3, 'oppose': 3, 'clause': 3, 'serving': 3, 'hummer': 3, 'ornament': 3, 'crank': 3, 'fires': 3, 'hovers': 3, 'nursing': 3, 'plains': 3, 'fries': 3, 'hearts': 3, 'emitting': 3, 'guided': 3, 'wrists': 3, 'bats': 3, 'ages': 3, 'ponchos': 3, 'lighthouse': 3, 'tussling': 3, 'missed': 3, 'wheelers': 3, 'scrubby': 3, 'rappelling': 3, 'sunshine': 3, 'sippy': 3, 'marina': 3, 'scuffle': 3, 'pride': 3, 'ipod': 3, 'speech': 3, 'priests': 3, 'mixed': 3, 'spin': 3, 'memorial': 3, 'flinging': 3, 'copper': 3, 'leaped': 3, 'effort': 3, 'leaned': 3, 'exhaust': 3, 'claps': 3, 'mardi': 3, 'gras': 3, 'waking': 3, 'lassie': 3, 'squares': 3, 'leafs': 3, 'shell': 3, 'cooling': 3, 'mingling': 3, 'shack': 3, 'whit': 3, 'investigate': 3, 'contraption': 3, 'herd': 3, 'dragsters': 3, 'hawk': 3, 'gaze': 3, 'flung': 3, 'sucks': 3, 'dew': 3, 'locking': 3, 'region': 3, 'laden': 3, 'bodyboard': 3, 'syrup': 3, 'interviewed': 3, 'mr': 3, 'p': 3, 'fully': 3, 'segway': 3, 'substance': 3, 'backgroud': 3, 'extremely': 3, 'cricketer': 3, 'search': 3, 'extravagant': 3, 'bmw': 3, 'frolic': 3, '25': 3, 'bathe': 3, 'roses': 3, 'renaissance': 3, 'melted': 3, 'persons': 3, 'tackler': 3, 'radio': 3, 'favorite': 3, '19': 3, 'swoops': 3, 'umpire': 3, 'lobster': 3, 'present': 3, 'rugged': 3, 'redbull': 3, 'shake': 3, 'perspective': 3, 'horn': 3, 'nt': 3, 'judge': 3, 'rover': 3, 'earpiece': 3, 'photographing': 3, 'serves': 3, 'opened': 3, 'goth': 3, 'microphones': 3, 'llama': 3, 'beachgoers': 3, 'flop': 3, 'completing': 3, 'mounds': 3, 'steer': 3, 'care': 3, 'amish': 3, 'mattress': 3, 'rocker': 3, 'lanterns': 3, 'acoustic': 3, 'parrot': 3, 'bounced': 3, 'fives': 3, 'tour': 3, 'lodge': 3, 'barber': 3, 'straining': 3, 'lavender': 3, 'lemonade': 3, 'sundown': 3, 'ringing': 3, 'trade': 3, 'poem': 3, 'toothbrush': 3, 'bout': 3, 'served': 3, 'carton': 3, 'dumbbell': 3, 'wharf': 3, 'recorder': 3, 'breeze': 3, 'dc': 3, 'vw': 3, 'fighter': 3, 'spell': 3, 'officials': 3, 'comic': 3, 'henna': 3, 'tubing': 3, 'muslim': 3, 'jukebox': 3, 'bunk': 3, 'chalkboard': 3, 'tunic': 3, 'demonstration': 3, 'highland': 3, 'africans': 3, 'leapfrog': 3, 'looked': 3, 'shipping': 3, 'barrels': 3, 'lizards': 3, 'beating': 3, 'dunk': 3, 'kennel': 3, 'donkey': 3, 'rollerbladers': 3, 'bases': 3, 'sunbathe': 3, 'turkey': 3, 'commercial': 2, 'mantle': 2, 'palying': 2, 'eyebrow': 2, 'handicap': 2, 'exit': 2, 'perhaps': 2, 'flashing': 2, 'twirl': 2, 'pylon': 2, 'fathers': 2, 'excitedly': 2, 'greeting': 2, 'glancing': 2, 'branded': 2, 'rotweiler': 2, 'halo': 2, 'accident': 2, 'visitors': 2, 'portion': 2, 'knitted': 2, 'headwraps': 2, 'meandering': 2, 'awnings': 2, 'statute': 2, 'wiener': 2, 'know': 2, 'scoop': 2, 'skyward': 2, 'transparent': 2, 'straddle': 2, 'competitor': 2, 'swimwear': 2, 'guardrail': 2, 'pilot': 2, 'gardening': 2, 'tend': 2, 'dolly': 2, 'skids': 2, 'viewing': 2, 'bump': 2, 'paintbrush': 2, 'blouse': 2, 'technical': 2, 'supported': 2, 'escorted': 2, 'wed': 2, 'newlyweds': 2, 'eyeglasses': 2, 'mountian': 2, 'choke': 2, 'snarls': 2, 'shit': 2, 'pour': 2, 'poured': 2, 'treated': 2, 'topples': 2, 'throught': 2, 'north': 2, 'carolina': 2, 'drifting': 2, 'mulch': 2, 'overlooked': 2, 'flexible': 2, 'skydiver': 2, 'manner': 2, 'schoolchildren': 2, 'exploring': 2, 'brilliant': 2, 'weathered': 2, 'slippers': 2, 'reclining': 2, 'sweatband': 2, 'daschund': 2, 'clump': 2, 'varied': 2, 'senior': 2, 'baggage': 2, 'snapping': 2, 'piloting': 2, 'peer': 2, 'outs': 2, 'drawings': 2, 'won': 2, 'penzance': 2, 'false': 2, 'grafitti': 2, 'downwards': 2, 'chef': 2, 'raging': 2, 'strong': 2, 'rafter': 2, 'metallic': 2, 'bystanders': 2, 'fame': 2, 'dads': 2, 'sexy': 2, 'hero': 2, 'weights': 2, 'notebook': 2, 'swerves': 2, 'trials': 2, 'gazebo': 2, 'comfort': 2, 'headlong': 2, 'farmland': 2, ':': 2, 'rising': 2, 'stoops': 2, 'stocking': 2, 'semi': 2, 'tuxes': 2, 'brimmed': 2, 'twigs': 2, 'weimaraner': 2, 'threatening': 2, 'gallops': 2, 'hilltop': 2, 'graze': 2, 'frosty': 2, 'incredible': 2, 'raining': 2, 'layer': 2, 'foal': 2, 'wicket': 2, 'cruise': 2, 'controlling': 2, 'complex': 2, 'tatoos': 2, 'sites': 2, 'shiner': 2, 'customers': 2, 'cramped': 2, 'celebrates': 2, 'handled': 2, 'elbow': 2, 'grayhound': 2, 'snowfield': 2, 'policewoman': 2, 'finds': 2, 'system': 2, 'faced': 2, 'foamy': 2, 'collects': 2, 'rummaging': 2, 'marine': 2, 'worried': 2, 'mottled': 2, 'pristine': 2, 'gain': 2, 'descend': 2, 'bust': 2, 'las': 2, 'vegas': 2, 'salvar': 2, 'planeta': 2, 'obscuring': 2, 'pearls': 2, 'pudding': 2, 'headlamp': 2, 'drags': 2, 'birdcage': 2, 'soggy': 2, 'disheveled': 2, 'collage': 2, 'weaves': 2, 'antlers': 2, 'snuggling': 2, 'scramble': 2, 'vert': 2, 'triangular': 2, 'divided': 2, 'hidden': 2, 'inverted': 2, 'sombrero': 2, 'permed': 2, 'asia': 2, 'tears': 2, 'character': 2, 'gates': 2, 'hitter': 2, 'speedboat': 2, 'puff': 2, 'damaged': 2, 'dwelling': 2, 'thrower': 2, 'sashes': 2, 'weaving': 2, 'pokemon': 2, 'yells': 2, 'attaching': 2, 'plugging': 2, 'sip': 2, 'pump': 2, 'adolescent': 2, 'launched': 2, 'pointer': 2, 'occasion': 2, 'salmon': 2, 'rose': 2, 'paddlers': 2, 'chicago': 2, '84': 2, 'peterson': 2, 'participant': 2, '1': 2, 'adhd': 2, 'bowed': 2, 'ac': 2, 'apparel': 2, 'final': 2, 'headdresses': 2, 'nicely': 2, 'boating': 2, 'ballplayer': 2, 'focuses': 2, 'pawing': 2, 'antique': 2, 'harvested': 2, 'partner': 2, 'squints': 2, 'cockpit': 2, 'continue': 2, 'schoolgirls': 2, 'daytime': 2, 'graphic': 2, 'scoring': 2, 'parlor': 2, 'design': 2, 'bouncer': 2, 'artistic': 2, 'bum': 2, 'darkly': 2, 'frolicks': 2, 'fists': 2, 'flexing': 2, 'setup': 2, 'toolbox': 2, 'bloody': 2, 'piglet': 2, 'jumpsuits': 2, 'pilots': 2, 'quarters': 2, 'clipped': 2, 'monitors': 2, 'blazer': 2, 'grapple': 2, 'motorbiker': 2, 'ace': 2, 'forests': 2, 'exposing': 2, 'spouts': 2, 'cattle': 2, 'rifles': 2, 'prayer': 2, 'headbands': 2, 'slice': 2, 'nadal': 2, 'need': 2, 'calico': 2, 'accents': 2, 'speedos': 2, 'hazmat': 2, 'stretcher': 2, 'dangerous': 2, 'limbs': 2, 'playgroud': 2, 'mart': 2, 'walmart': 2, 'vampire': 2, 'jousting': 2, 'trays': 2, 'saturated': 2, 'teach': 2, 'streamer': 2, 'daredevil': 2, 'lavish': 2, 'grasp': 2, 'once': 2, 'logos': 2, 'tambourines': 2, 'trapped': 2, 'age': 2, 'accented': 2, 'regalia': 2, 'mounted': 2, 'crust': 2, 'chins': 2, 'heavyset': 2, 'commuters': 2, 'includes': 2, 'sinks': 2, 'tubular': 2, 'debris': 2, 'concentrates': 2, 'tilts': 2, 'receive': 2, 'pedestal': 2, 'footbridge': 2, 'dirtbikers': 2, 'lonely': 2, 'eldery': 2, 'our': 2, 'undershirt': 2, 'farmer': 2, 'fashionably': 2, 'shielding': 2, 'coasting': 2, 'tumbleweed': 2, 'snowmobiling': 2, 'await': 2, 'googles': 2, 'discuss': 2, 'sideline': 2, 'collected': 2, 'straddling': 2, 'studies': 2, 'roughhousing': 2, 'avrovulcancom': 2, 'gentlemen': 2, 'flew': 2, 'lets': 2, 'gorgeous': 2, 'supporting': 2, 'caravan': 2, 'gnaws': 2, 'dips': 2, 'fierce': 2, 'rays': 2, 'creates': 2, 'longsleeve': 2, 'lick': 2, 'tartan': 2, 'melts': 2, 'drooling': 2, 'gonzaga': 2, 'waists': 2, 'towers': 2, 'trousers': 2, 'sweats': 2, 'duke': 2, 'spike': 2, 'aerodynamic': 2, 'possessions': 2, 'unpaved': 2, 'helment': 2, 'linked': 2, 'gothic': 2, 'stains': 2, 'breathing': 2, 'aimed': 2, 'ralley': 2, 'barechested': 2, 'overcast': 2, 'slinging': 2, 'sole': 2, 'trot': 2, 'dusted': 2, 'hunter': 2, 'synchronized': 2, 'snowmobiler': 2, 'prepared': 2, 'nursery': 2, 'triangle': 2, 'chart': 2, 'crates': 2, 'steady': 2, 'darkness': 2, 'couches': 2, 'confronts': 2, 'daughters': 2, 'buffalo': 2, 'sweatsuit': 2, 'glances': 2, 'consumed': 2, 'goods': 2, 'bumps': 2, 'startled': 2, 'recoils': 2, 'youngster': 2, 'package': 2, 'contemplates': 2, 'wands': 2, 'celebration': 2, 'daisies': 2, 'employees': 2, 'thinking': 2, 'silk': 2, 'mills': 2, 'chandelier': 2, 'grimacing': 2, 'staffordshire': 2, 'brass': 2, 'florist': 2, 'rears': 2, 'formed': 2, 'guarded': 2, 'drainage': 2, 'satchel': 2, 'lucky': 2, 'mill': 2, 'partying': 2, 'stance': 2, 'sailboard': 2, 'launcher': 2, 'released': 2, 'readying': 2, 'ladle': 2, 'walkers': 2, 'flood': 2, 'amphitheater': 2, 'shutters': 2, 'mexico': 2, 'grappling': 2, 'capris': 2, 'firehose': 2, 'extinguish': 2, 'puckering': 2, 'lump': 2, 'mirrored': 2, 'propping': 2, 'hump': 2, 'crazy': 2, 'towels': 2, 'nipping': 2, '52': 2, 'pitchers': 2, 'joke': 2, 'rollercoaster': 2, 'zooming': 2, 'ferns': 2, 'axe': 2, 'rapidly': 2, 'backhand': 2, 'mingle': 2, 'puma': 2, 'given': 2, 'handheld': 2, 'showerhead': 2, 'italy': 2, 'propel': 2, 'crew': 2, 'robed': 2, 'cloak': 2, 'crawl': 2, 'enough': 2, 'burrowing': 2, 'david': 2, 'poolside': 2, 'avoiding': 2, 'oncoming': 2, 'feels': 2, 'indescript': 2, 'traverse': 2, 'taught': 2, 'moonwalk': 2, 'whisper': 2, 'wool': 2, 'mitten': 2, 'spool': 2, 'overall': 2, 'thing': 2, 'cropped': 2, 'handling': 2, 'flopping': 2, 'closing': 2, 'nascar': 2, 'sportswear': 2, '281': 2, 'trek': 2, 'earmuffs': 2, 'hoodies': 2, 'doorstep': 2, 'midget': 2, 'acrobatics': 2, 'changes': 2, 'bearer': 2, 'petals': 2, 'architectural': 2, 'digger': 2, 'rickety': 2, 'wildly': 2, 'kickboxing': 2, 'backpackers': 2, 'snowed': 2, 'fix': 2, 'haystack': 2, 'buoy': 2, 'affectionately': 2, 'bathrooms': 2, 'stirs': 2, 'entangled': 2, 'aggressive': 2, 'versus': 2, 'bicyclers': 2, 'sheppard': 2, 'cherry': 2, 'cycles': 2, 'sleek': 2, 'navigate': 2, 'specialized': 2, 'any': 2, 'cot': 2, 'soup': 2, 'ladles': 2, 'recreational': 2, 'outwards': 2, 'restaurants': 2, 'beams': 2, 'loan': 2, 'pregnant': 2, 'protester': 2, 'pelicans': 2, 'surgical': 2, 'recieving': 2, 'surrounds': 2, 'grounded': 2, 'process': 2, 'ringed': 2, 'glowing': 2, 'grove': 2, 'curl': 2, 'glittery': 2, 'companion': 2, 'trike': 2, 'rappels': 2, 'bottled': 2, 'tram': 2, 'nuzzling': 2, 'signals': 2, 'flash': 2, 'tented': 2, 'barb': 2, 'poor': 2, 'battles': 2, 'perpendicular': 2, 'exotic': 2, 'mustached': 2, 'fringe': 2, 'chips': 2, 'bi': 2, 'fireball': 2, 'pong': 2, 'windowsill': 2, 'attacked': 2, 'pouch': 2, 'chests': 2, 'treats': 2, 'involved': 2, 'array': 2, 'capri': 2, 'stalks': 2, 'husks': 2, 'arizona': 2, 'harmonica': 2, 'cleans': 2, 'wiped': 2, 'knotted': 2, 'pinstripe': 2, 'auto': 2, 'wintertime': 2, 'madly': 2, 'halfway': 2, 'tying': 2, 'cloaks': 2, 'halves': 2, 'confrontation': 2, 'soil': 2, 'hoes': 2, 'homes': 2, 'showgirls': 2, 'receiving': 2, 'rice': 2, 'bellbottoms': 2, 'turf': 2, 'sightseeing': 2, 'clutches': 2, 'automobiles': 2, 'ways': 2, 'ref': 2, 'lighted': 2, 'unmanned': 2, 'whiel': 2, 'pom': 2, 'poms': 2, 'session': 2, 'bodysuit': 2, 'shaft': 2, 'spelunkers': 2, 'florescent': 2, 'crazily': 2, 'buzy': 2, 'notice': 2, 'retrieved': 2, 'tablet': 2, 'twisted': 2, 'dangles': 2, 'uw': 2, 'odeon': 2, 'mown': 2, 'slender': 2, '104': 2, 'storefronts': 2, 'subject': 2, 'crow': 2, 'slicker': 2, 'underside': 2, 'curling': 2, 'brochure': 2, 'bulldogs': 2, 'duffel': 2, 'vista': 2, 'learns': 2, 'illuminated': 2, 'tightly': 2, 'punting': 2, 'broadly': 2, 'brow': 2, 'designs': 2, 'illustration': 2, 'winning': 2, 'brunettes': 2, 'john': 2, 'mouthing': 2, 'antenna': 2, 'recliner': 2, 'better': 2, 'tote': 2, 'refreshment': 2, 'attentive': 2, 'traversing': 2, 'cheese': 2, 'mustard': 2, 'challenge': 2, 'save': 2, 'backseat': 2, 'sunsets': 2, 'cartoon': 2, 'stubby': 2, 'freestyle': 2, 'perches': 2, 'clips': 2, 'punctured': 2, 'dachshunds': 2, 'collecting': 2, 'utensils': 2, 'quarterpipe': 2, 'girder': 2, 'pails': 2, 'lunchbox': 2, 'zig': 2, 'evergreen': 2, 'outrun': 2, 'thriller': 2, 'latte': 2, 'mcdonalds': 2, 'stopping': 2, 'furnace': 2, 'v': 2, 'hook': 2, 'ornaments': 2, 'gauges': 2, 'prances': 2, 'cattails': 2, 'metro': 2, 'cannot': 2, 'accross': 2, 'trooper': 2, 'aig': 2, 'demolished': 2, 'blower': 2, 'fours': 2, 'powered': 2, 'medal': 2, 'parasailors': 2, 'bangs': 2, 'begs': 2, 'backless': 2, 'quiet': 2, 'pencil': 2, 'smirks': 2, '3rd': 2, 'constructed': 2, 'alcohol': 2, '21': 2, 'frightened': 2, 'measured': 2, 'stepped': 2, 'piste': 2, 'stringed': 2, 'defend': 2, 'rockstar': 2, 'energy': 2, 'charging': 2, 'tanned': 2, 'mp3': 2, 'best': 2, 'stacked': 2, 'amazing': 2, 'murals': 2, 'bonfire': 2, 'groomed': 2, 'propelled': 2, 'sunbathers': 2, 'pastry': 2, 'baked': 2, 'separate': 2, 'interviewing': 2, 'blur': 2, 'dale': 2, 'jr': 2, 'mountaintops': 2, 'surfboarding': 2, 'flaps': 2, 'soled': 2, 'tax': 2, 'hate': 2, 'eleven': 2, 'photography': 2, 'snuggles': 2, 'protectors': 2, 'boarded': 2, 'abandon': 2, 'windmill': 2, 'smilling': 2, 'grasping': 2, 'enthusiastically': 2, 'slippery': 2, 'tantrum': 2, 'fairies': 2, 'tatooed': 2, 'chow': 2, 'vie': 2, 'nerf': 2, 'sheperd': 2, 'sprawled': 2, 'yell': 2, 'minimal': 2, 'hooker': 2, 'teeter': 2, 'totter': 2, 'samsung': 2, 'bracelets': 2, 'amounts': 2, 'netted': 2, 'pensive': 2, 'donning': 2, 'donuts': 2, 'gal': 2, 'mouthguards': 2, 'refuse': 2, 'thermos': 2, 'roping': 2, 'grips': 2, 'veteran': 2, 'greets': 2, 'load': 2, 'vessel': 2, 'sparsely': 2, 'quilted': 2, 'whips': 2, 'bits': 2, 'puffs': 2, 'shorter': 2, 'anime': 2, 'precariously': 2, 'flamboyant': 2, 'sledder': 2, 'canned': 2, 'brook': 2, 'steered': 2, 'humans': 2, 'comforting': 2, 'wintery': 2, 'tinted': 2, 'daring': 2, 'reclines': 2, 'investigates': 2, 'sheepdog': 2, 'headphone': 2, 'keeper': 2, 'kickflip': 2, 'prey': 2, 'wets': 2, 'icing': 2, 'ill': 2, 'dip': 2, 'dodgeball': 2, 'lock': 2, 'queen': 2, 'alter': 2, 'homebase': 2, 'scrimmage': 2, 'uncrowded': 2, 'cascading': 2, 'cbs': 2, 'knocked': 2, 'yong': 2, 'pyrotechnics': 2, 'squeezes': 2, 'indians': 2, 'amuseument': 2, 'features': 2, 'dancefloor': 2, 'paneled': 2, 'juggle': 2, 'innertubes': 2, 'state': 2, 'album': 2, 'peoples': 2, 'containers': 2, 'waders': 2, 'skins': 2, 'doggy': 2, 'retail': 2, 'frosting': 2, 'tone': 2, 'staff': 2, 'ou': 2, 'ponytails': 2, 'swam': 2, 'crocks': 2, 'frilly': 2, 'princess': 2, 'entertain': 2, 'website': 2, 'fanny': 2, 'ing': 2, 'kit': 2, 'sparrow': 2, 'salt': 2, 'possible': 2, 'sequins': 2, 'colonial': 2, 'necks': 2, 'patchy': 2, 'canadian': 2, 'victorian': 2, 'times': 2, 'snowstorm': 2, 'whipping': 2, 'bookshelf': 2, 'dozes': 2, 'news': 2, 'pots': 2, 'found': 2, 'amount': 2, 'sending': 2, 'scarfs': 2, 'headwrap': 2, 'towing': 2, 'choir': 2, 'hosed': 2, 'climbed': 2, 'uggs': 2, 'locked': 2, 'banjo': 2, 'gettin': 2, 'ghostbusters': 2, 'section': 2, 'browse': 2, 'bun': 2, 'stuntman': 2, 'backbends': 2, 'finley': 2, 'castles': 2, 'carabiner': 2, 'skyscrapers': 2, 'changing': 2, 'information': 2, 'marquee': 2, 'enforcement': 2, 'crafts': 2, 'elf': 2, 'ripping': 2, 'treading': 2, 'squad': 2, 'forwards': 2, 'possibly': 2, 'fishermen': 2, 'aerobics': 2, 'wristwatch': 2, 'yorkshire': 2, 'chief': 2, 'portland': 2, 'oregon': 2, 'certificates': 2, 'root': 2, 'smell': 2, 'swordsman': 2, 'yarn': 2, 'garment': 2, 'mosque': 2, 'sinking': 2, 'hers': 2, 'tribal': 2, 'cruiser': 2, 'longhaired': 2, 'poorly': 2, 'bicycling': 2, 'toyota': 2, 'adjust': 2, 'snoopy': 2, 'recording': 2, 'taping': 2, 'judges': 2, 'dear': 2, 'controls': 2, 'dumping': 2, 'barack': 2, 'filming': 2, 'bloom': 2, 'badges': 2, 'scrubland': 2, 'stunning': 2, 'olympic': 2, 'steal': 2, 'attentively': 2, 'guides': 2, 'placid': 2, 'cannonball': 2, 'smu': 2, 'sveral': 2, 'studded': 2, 'bagpipers': 2, 'approached': 2, 'toes': 2, 'budweiser': 2, 'whack': 2, 'bursting': 2, 'plow': 2, 'steadies': 2, 'roman': 2, 'oddly': 2, 'designed': 2, '11': 2, 'excitement': 2, 'dressing': 2, 'arrangement': 2, 'youths': 2, 'flippers': 2, 'entitled': 2, 'ant': 2, 'amplifier': 2, 'interior': 2, 'puzzled': 2, 'bullhorn': 2, 'hods': 2, 'pups': 2, 'pockets': 2, 'spitting': 2, 'spits': 2, 'corral': 2, 'bell': 2, 'doorbell': 2, 'ferris': 2, 'giants': 2, 'dobermans': 2, 'kawasaki': 2, 'crag': 2, 'handrails': 2, 'tried': 2, 'sombody': 2, 'scrubbing': 2, 'un': 2, 'featuring': 2, 'draws': 2, 'terminal': 2, 'drug': 2, 'astroturf': 2, 'texting': 2, 'swimmies': 2, 'simpsons': 2, 'flows': 2, 'cylinder': 2, 'cascades': 2, 'poling': 2, 'flea': 2, 'sandpit': 2, 'eastpak': 2, 'peeing': 2, 'urinating': 2, 'tubes': 2, 'herds': 2, 'binocular': 2, 'columned': 2, 'workshop': 2, 'again': 2, 'reviewing': 2, 'took': 2, 'crest': 2, 'engaging': 2, 'sips': 2, 'crooked': 2, 'amnesty': 2, 'international': 2, 'attending': 2, 'kiosk': 2, 'knocks': 2, 'israel': 2, 'genetic': 2, 'freak': 2, 'intercept': 2, 'canada': 2, 'ritz': 2, 'nylon': 2, 'fork': 2, 'english': 2, 'gay': 2, 'cobblestones': 2, 'west': 2, 'nutcracker': 2, 'shawls': 2, 'stiped': 2, 'andy': 2, 'slices': 2, 'fedex': 2, 'removes': 2, 'bullet': 2, 'disguises': 2, 'hissing': 2, 'breasts': 2, 'grimaces': 2, 'slouching': 2, 'stumps': 2, 'referees': 2, '58': 2, 'skydivers': 2, 'conditions': 2, 'mushing': 2, 'timeout': 2, 'beats': 2, 'sniffed': 2, 'outcrop': 2, 'bongo': 2, 'pinwheel': 2, 'marshy': 2, 'least': 2, 'sailboats': 2, 'parasurfer': 2, 'brake': 2, 'mule': 2, 'showgirl': 2, 'historic': 2, 'israeli': 2, 'accessories': 2, 'got': 2, 'albert': 2, 'goodbye': 2, 'patrick': 2, 'shamrocks': 2, 'lawnmower': 2, 'rugs': 2, 'slinky': 2, 'videotapes': 2, 'rite': 2, 'broadway': 2, 'buckled': 2, 'smear': 2, 'san': 2, 'diego': 2, 'golfers': 2, 'holder': 2, 'sunflowers': 2, 'croquette': 1, 'chainmail': 1, 'codpiece': 1, 'grenade': 1, 'fluorescent': 1, 'somone': 1, 'solitude': 1, 'aquos': 1, 'index': 1, 'warily': 1, 'investigating': 1, 'ractrack': 1, 'shelton': 1, 'waded': 1, 'bruised': 1, 'rental': 1, 'bookcase': 1, 'videos': 1, 'muxzzled': 1, 'riwal': 1, 'eatery': 1, 'playes': 1, 'stringless': 1, 'electricity': 1, 'childern': 1, 'dooorway': 1, 'trolley': 1, 'badly': 1, 'overnight': 1, 'thre': 1, 'consulting': 1, 'looker': 1, 'spill': 1, 'zipping': 1, 'slips': 1, 'word': 1, 'antics': 1, 'poof': 1, 'woolen': 1, 'dirtbikes': 1, 'snowpile': 1, 'hurridly': 1, 'bluejean': 1, 'chinatown': 1, 'crowns': 1, 'daschunds': 1, 'picure': 1, 'referring': 1, 'periods': 1, 'harpsichord': 1, 'pianist': 1, 'boe': 1, 'walkng': 1, 'palestinian': 1, 'majestically': 1, 'sunflower': 1, 'dirtbike': 1, 'endzone': 1, 'launching': 1, 'vaulter': 1, 'cinderblock': 1, 'brighly': 1, 'sliiding': 1, 'floaters': 1, 'wipeout': 1, 'painters': 1, 'heroes': 1, 'propeller': 1, 'rakes': 1, 'ate': 1, 'devotion': 1, 'nfl': 1, 'signage': 1, 'blader': 1, 'teeshirt': 1, 'locker': 1, 'slimy': 1, 'blowup': 1, 'onrushing': 1, 'queens': 1, 'formula': 1, 'grouped': 1, 'intervening': 1, 'coverall': 1, 'bandmates': 1, 'associated': 1, 'panes': 1, 'disturbed': 1, 'pastor': 1, 'boogieboard': 1, 'netting': 1, 'throat': 1, 'joking': 1, 'illustrated': 1, 'corrugated': 1, 'casterol': 1, 'branding': 1, '22': 1, '29': 1, 'drap': 1, 'dump': 1, 'sleeding': 1, 'statefarmcom': 1, 'camps': 1, 'facepaintings': 1, 'x': 1, 'fisheye': 1, 'stirring': 1, 'teamates': 1, 'headless': 1, 'onlooker': 1, 'pavillion': 1, 'cultural': 1, 'lecturer': 1, '30': 1, 'acts': 1, 'tattoed': 1, 'gliders': 1, 'enjoyable': 1, 'agile': 1, 'manuals': 1, 'backside': 1, 'riverside': 1, 'steamboat': 1, 'riverboat': 1, 'medatative': 1, 'meditational': 1, 'bigs': 1, 'adobe': 1, 'fitls': 1, 'emerald': 1, 'burned': 1, 'activities': 1, 'mucky': 1, 'flaring': 1, 'paisley': 1, 'yuong': 1, 'europe': 1, 'domed': 1, 'aligator': 1, 'shares': 1, 'dolls': 1, 'write': 1, 'fellows': 1, 'toll': 1, 'pipes': 1, 'fluorecent': 1, 'kill': 1, 'weirmeiner': 1, 'jumpinjg': 1, 'stonesign': 1, 'welcomes': 1, 'welcome': 1, 'proud': 1, 'fetched': 1, 'adventures': 1, 'graffited': 1, 'housekeeping': 1, 'waitresses': 1, 'footbride': 1, 'currents': 1, 'bigg': 1, 'neither': 1, 'rallies': 1, 'lamb': 1, 'skewed': 1, 'sucker': 1, 'jailbird': 1, 'fliers': 1, 'fawkes': 1, 'vans': 1, 'funky': 1, 'cemetery': 1, 'buff': 1, 'bulky': 1, 'grip': 1, 'rather': 1, 'roading': 1, 'beijing': 1, 'chutes': 1, 'whites': 1, 'railling': 1, 'knoll': 1, 'overshirt': 1, 'chrome': 1, 'flown': 1, 'bedroll': 1, 'slouched': 1, 'withdrawing': 1, 'brwon': 1, 'bakery': 1, 'shoulderbag': 1, 'trotted': 1, 'retangular': 1, 'coconut': 1, 'flotation': 1, 'adorn': 1, 'hdr': 1, 'cathcer': 1, 'bystander': 1, 'fatigue': 1, 'turbulent': 1, 'loops': 1, 'aerobatic': 1, 'coiled': 1, 'archways': 1, 'arches': 1, 'attrative': 1, 'skimply': 1, 'barbwire': 1, 'cavort': 1, 'contestant': 1, 'dishtowel': 1, 'pillowcase': 1, 'cresting': 1, 'exciting': 1, 'flakes': 1, 'mothers': 1, 'inertia': 1, 'pausing': 1, 'hulk': 1, 'even': 1, 'mushroom': 1, 'shaving': 1, 'reflected': 1, 'colt': 1, 'convert': 1, 'twenties': 1, 'distored': 1, 'flashlight': 1, 'linet': 1, 'crime': 1, 'oceanside': 1, 'via': 1, 'blooming': 1, 'jumphouse': 1, 'collapsing': 1, 'hoodoos': 1, 'explores': 1, 'paraskier': 1, 'paragliding': 1, 'pouting': 1, 'colorings': 1, 'fisher': 1, 'intensely': 1, 'deeper': 1, 'flexibility': 1, 'joker': 1, 'suprised': 1, 'sloppy': 1, 'sticker': 1, 'flickr': 1, 'dobbermen': 1, 'chrismas': 1, 'perfors': 1, 'rust': 1, 'seller': 1, 'mad': 1, 'strainer': 1, 'terrior': 1, 'figurine': 1, 'hillock': 1, 'powdery': 1, 'swirls': 1, 'addressing': 1, 'diverse': 1, 'add': 1, 'recyclable': 1, 'ourdoors': 1, 'perfoms': 1, 'overhangs': 1, 'inscribed': 1, 'redhead': 1, 'sands': 1, 'bog': 1, 'binky': 1, 'takedown': 1, 'prestends': 1, 'spotting': 1, 'keeling': 1, 'bathes': 1, 'rapels': 1, 'treetops': 1, 'slanted': 1, 'mountin': 1, 'snowscapes': 1, 'concentrating': 1, 'swipes': 1, 'blockers': 1, 'cocked': 1, 'advantage': 1, 'pullovers': 1, 'tbe': 1, 'buddha': 1, 'breezeway': 1, 'greenpeace': 1, 'barrow': 1, 'replaced': 1, 'decked': 1, 'razzling': 1, 'broach': 1, 'antiquated': 1, 'pearl': 1, 'emerge': 1, 'snakeskin': 1, 'alls': 1, 'preoccupied': 1, 'lawnchair': 1, 'backstrokes': 1, 'tones': 1, 'craw': 1, 'ceramic': 1, 'joggers': 1, 'streambed': 1, 'playgym': 1, 'robust': 1, 'bespectacled': 1, 'lavendar': 1, 'notepad': 1, 'soaks': 1, 'fishemen': 1, 'streetway': 1, 'caring': 1, 'kneeled': 1, 'circled': 1, 'outfir': 1, 'pugs': 1, 'snare': 1, 'regularly': 1, 'geishas': 1, 'breaststroke': 1, 'progressively': 1, 'backround': 1, 'born': 1, 'razer': 1, 'boods': 1, 'sleep': 1, 'patriotic': 1, 'pursed': 1, 'custom': 1, 'allowed': 1, 'steeple': 1, 'bigwheels': 1, 'concealed': 1, 'mobility': 1, 'citizen': 1, 'acroos': 1, 'carreis': 1, 'client': 1, 'perm': 1, 'tear': 1, 'foreign': 1, 'ypoung': 1, 'ependent': 1, 'trumped': 1, 'hunt': 1, 'beckons': 1, 'helments': 1, 'forth': 1, 'rangers': 1, 'ranger': 1, 'flexable': 1, 'ballplayers': 1, 'scans': 1, 'offensive': 1, 'notre': 1, 'dame': 1, 'persues': 1, 'chute': 1, 'organizing': 1, 'cheerful': 1, 'drumming': 1, 'banging': 1, 'pans': 1, 'wields': 1, 'lapel': 1, 'leopard': 1, 'convenience': 1, 'slushies': 1, 'conifers': 1, 'violently': 1, 'actions': 1, 'drills': 1, 'pilar': 1, 'enlarged': 1, 'carpenters': 1, 'samples': 1, 'seafood': 1, 'delivering': 1, 'clever': 1, 'parody': 1, 'nodding': 1, 'beanches': 1, 'smoked': 1, 'knelt': 1, 'armoire': 1, 'idyllic': 1, 'flautist': 1, 'neatly': 1, 'woody': 1, 'bowled': 1, 'finishes': 1, 'anticipation': 1, 'shortstop': 1, 'lapse': 1, 'bareback': 1, 'la': 1, 'quinta': 1, 'anticipating': 1, 'restrain': 1, 'minding': 1, 'kiyaking': 1, 'gin': 1, 'cornfield': 1, 'slurps': 1, 'targeting': 1, 'shite': 1, 'accompanies': 1, 'maneuvering': 1, 'mate': 1, 'exhibt': 1, 'sisters': 1, 'hallways': 1, 'stoppie': 1, 'stange': 1, 'eachothers': 1, 'entry': 1, 'domes': 1, 'orbs': 1, 'shirtness': 1, 'batons': 1, 'juggler': 1, 'disabled': 1, 'spills': 1, 'nibbles': 1, 'looming': 1, 'seidwalk': 1, 'discouraged': 1, 'based': 1, 'communications': 1, 'touchline': 1, 'may': 1, 'noticable': 1, 'hauling': 1, 'pumped': 1, 'bended': 1, 'strength': 1, 'wounded': 1, 'wounds': 1, 'fleecy': 1, 'responders': 1, 'woamn': 1, 'obscene': 1, 'overshadowed': 1, 'outlines': 1, 'jumpsuites': 1, 'delivery': 1, 'easels': 1, 'intot': 1, 'zips': 1, 'sportwoman': 1, 'sportman': 1, 'demonstrate': 1, 'verizon': 1, 'standind': 1, 'schools': 1, 'siluettes': 1, 'surveying': 1, 'indigo': 1, 'wipe': 1, 'furocious': 1, 'snowpacked': 1, 'enforcment': 1, 'gigantic': 1, 'firework': 1, 'litlle': 1, 'jostles': 1, 'perked': 1, 'stiffing': 1, 'meditating': 1, 'antennae': 1, 'snacks': 1, 'aids': 1, 'lazily': 1, 'landform': 1, 'biohazard': 1, 'gurnee': 1, 'awkward': 1, 'brige': 1, 'sprawling': 1, 'snorkel': 1, 'k2': 1, 'crafted': 1, 'equiment': 1, 'rottweiller': 1, 'squating': 1, 'seek': 1, 'entwined': 1, 'gesticulates': 1, 'wal': 1, 'unifrom': 1, 'vampires': 1, 'goers': 1, 'duel': 1, 'alotment': 1, 'thoroughly': 1, 'noise': 1, 'maker': 1, 'noisemaker': 1, 'popper': 1, 'strawberry': 1, 'flashes': 1, 'vandalized': 1, 'flirts': 1, 'pleople': 1, 'panasonic': 1, 'downsteps': 1, 'arbor': 1, 'atrium': 1, 'ability': 1, 'fastened': 1, 'element': 1, 'sweeping': 1, 'operate': 1, 'boredom': 1, 'imagery': 1, 'crucifixion': 1, 'christ': 1, 'crucified': 1, 'sponsors': 1, 'piggybacking': 1, 'maracas': 1, 'hurricanes': 1, 'naval': 1, 'lookers': 1, 'multistory': 1, 'hamming': 1, 'outcroping': 1, 'jersay': 1, 'boned': 1, 'physiques': 1, 'fro': 1, 'allow': 1, 'min': 1, 'egyptian': 1, 'egytian': 1, 'kayer': 1, 'taped': 1, 'silky': 1, 'telegraph': 1, 'landfill': 1, 'constructions': 1, 'suite': 1, 'footwork': 1, 'atomic': 1, 'indicating': 1, 'cheerfully': 1, 'preservers': 1, 'faithful': 1, 'dense': 1, 'zepra': 1, 'spays': 1, '625': 1, 'swishing': 1, 'barettes': 1, 'hairclips': 1, 'edges': 1, 'armstand': 1, 'punts': 1, 'lampost': 1, 'stretchy': 1, 'charm': 1, 'clothesline': 1, 'granny': 1, 'panties': 1, 'underpants': 1, 'wrinkley': 1, 'largley': 1, 'vendors': 1, 'organic': 1, 'outise': 1, 'grainy': 1, 'colapsable': 1, 'peeping': 1, 'napping': 1, 'heap': 1, 'korean': 1, 'aluminum': 1, 'ques': 1, 'underbrush': 1, 'windowed': 1, 'circles': 1, 'embedded': 1, 'hindu': 1, 'citizens': 1, 'dodging': 1, 'cutoff': 1, 'congratulate': 1, 'waaves': 1, 'silohuetted': 1, 'civillians': 1, 'busstop': 1, 'settings': 1, 'glassess': 1, 'snowpants': 1, 'tentatively': 1, 'thrashed': 1, 'scent': 1, 'pamphlets': 1, 'cause': 1, 'vibrating': 1, 'recline': 1, 'coached': 1, 'tho': 1, 'vapour': 1, 'shape': 1, 'pedalling': 1, 'rangler': 1, 'khakis': 1, 'mail': 1, 'brief': 1, 'overhear': 1, 'emerged': 1, 'ruggers': 1, 'trace': 1, 'middleaged': 1, 'suns': 1, 'outlined': 1, 'shorthaired': 1, 'headlight': 1, 'scarily': 1, 'shag': 1, 'helped': 1, 'bridesmaid': 1, 'highchair': 1, 'jello': 1, 'neclace': 1, 'gontaga': 1, 'videocameras': 1, 'atmosphere': 1, 'saucer': 1, 'oout': 1, 'proximity': 1, 'interlocking': 1, 'consumer': 1, 'turnaround': 1, 'entertaining': 1, 'neckless': 1, 'further': 1, 'advances': 1, 'gators': 1, 'fleeces': 1, 'peoplw': 1, 'tulip': 1, 'grss': 1, 'squeak': 1, 'relection': 1, 'aross': 1, 'adjustments': 1, 'photograhi': 1, 'ruined': 1, 'swatting': 1, 'palid': 1, 'swallow': 1, 'hoof': 1, 'bronze': 1, 'stomachs': 1, 'extravagent': 1, 'thw': 1, 'passage': 1, 'ascend': 1, 'trudge': 1, 'handbags': 1, 'attemping': 1, 'venue': 1, 'buggies': 1, 'woolly': 1, 'zaftig': 1, 'freefall': 1, 'coordinator': 1, 'someones': 1, 'bouncey': 1, 'patched': 1, 're': 1, 'biscuit': 1, 'milkbone': 1, 'ti': 1, 'chi': 1, 'areas': 1, 'hanna': 1, 'montana': 1, 'frizzy': 1, 'rung': 1, 'sunlit': 1, 'overfilled': 1, 'shabby': 1, 'butting': 1, 'tawny': 1, 'weave': 1, 'youngle': 1, 'swirling': 1, 'bonnet': 1, 'lkievely': 1, 'tinsel': 1, 'smiled': 1, 'cello': 1, 'shrowded': 1, 'expanding': 1, 'tourquoise': 1, 'perfom': 1, 'rippled': 1, 'littering': 1, 'astonishment': 1, 'demonstarting': 1, 'outfitted': 1, '4x4': 1, 'plantains': 1, 'collides': 1, 'stoplight': 1, 'masses': 1, 'impact': 1, 'compact': 1, 'feamle': 1, 'lassos': 1, 'witnesses': 1, 'dealth': 1, 'signpost': 1, 'rappeling': 1, 'athelete': 1, 'pf': 1, 'internet': 1, 'leaguer': 1, 'ohio': 1, 'dragons': 1, 'year': 1, 'slumped': 1, 'dandylions': 1, 'champ': 1, 'rafael': 1, 'weimaraners': 1, 'forceful': 1, 'dresswear': 1, 'gaurd': 1, 'grassland': 1, 'grond': 1, 'widows': 1, 'contemplating': 1, 'thought': 1, 'peddled': 1, 'youg': 1, 'motley': 1, 'divers': 1, 'sweating': 1, 'congregated': 1, 'fancily': 1, 'styled': 1, 'matchin': 1, 'thousand': 1, 'hundred': 1, 'thirty': 1, 'projector': 1, 'jazz': 1, 'saxaphones': 1, 'loses': 1, 'sailboarder': 1, 'completed': 1, 'childing': 1, 'oh': 1, 'labelled': 1, 'overflowing': 1, 'spoke': 1, 'osme': 1, 'meetinghall': 1, 'blays': 1, 'slighty': 1, 'tilting': 1, 'lifeboat': 1, 'rushed': 1, 'nestled': 1, 'clemson': 1, 'quarry': 1, 'dries': 1, 'bobbed': 1, '97': 1, 'sweatpants': 1, 'powerlines': 1, 'pats': 1, 'saddled': 1, 'hippie': 1, 'toddles': 1, 'hawaiin': 1, 'freddy': 1, 'krueger': 1, 'widow': 1, 'grownups': 1, 'slush': 1, 'drips': 1, 'gaurdian': 1, 'breaker': 1, '33': 1, 'procession': 1, 'deserted': 1, 'mermaid': 1, 'fton': 1, 'throughwindow': 1, 'needlepoint': 1, 'tending': 1, 'abarrotes': 1, 'narby': 1, 'physical': 1, 'contact': 1, 'extinguishes': 1, 'carjack': 1, 'mudfight': 1, 'ash': 1, 'messanger': 1, 'wallride': 1, 'sphere': 1, 'rips': 1, 'cleavage': 1, 'tatoo': 1, 'seperate': 1, 'bleached': 1, 'pac': 1, 'recoiling': 1, 'philadelphia': 1, 'phillie': 1, 'contracption': 1, 'hoses': 1, 'goucho': 1, 'moustaches': 1, 'caged': 1, 'belays': 1, 'competeition': 1, 'hairdo': 1, 'enjoyment': 1, 'flume': 1, 'focused': 1, 'encircling': 1, 'counters': 1, 'pizzeria': 1, 'buzzes': 1, 'goalies': 1, 'ax': 1, 'bunchh': 1, 'bohemian': 1, 'lilly': 1, 'climing': 1, 'hop': 1, 'operators': 1, 'gondoliers': 1, 'venice': 1, 'experiences': 1, 'excites': 1, 'wizard': 1, 'macintosh': 1, 'rodent': 1, 'defecating': 1, 'pooping': 1, 'provocative': 1, 'multilple': 1, 'crows': 1, '80': 1, 'adventurer': 1, 'beckham': 1, 'shook': 1, 'dominant': 1, 'ashen': 1, 'flats': 1, 'mainly': 1, 'rattan': 1, 'fed': 1, 'baggy': 1, 'ashy': 1, 'impersonator': 1, 'tongee': 1, 'gith': 1, 'dhe': 1, 'rode': 1, 'jogged': 1, 'fiesty': 1, 'foul': 1, 'scowling': 1, 'wooly': 1, 'buggys': 1, 'bunnies': 1, 'rabbits': 1, 'bangles': 1, 'safron': 1, 'surges': 1, 'jaw': 1, 'median': 1, 'bumpers': 1, 'seaside': 1, 'torch': 1, 'noce': 1, 'pullup': 1, 'sewn': 1, 'emblazoned': 1, 'pasts': 1, 'belongs': 1, 'staue': 1, 'goofing': 1, 'applauding': 1, 'clapped': 1, 'furiously': 1, 'sparking': 1, 'bland': 1, 'exits': 1, 'deposited': 1, 'entertainers': 1, 'swarmed': 1, 'swarm': 1, 'goggled': 1, 'backing': 1, 'fortess': 1, 'payfully': 1, 'sledders': 1, 'bulldozer': 1, 'exposure': 1, 'reflects': 1, 'screening': 1, 'c': 1, 'novel': 1, 'wierd': 1, 'paddock': 1, 'mma': 1, 'muscles': 1, 'bolts': 1, 'equpitment': 1, 'equpiment': 1, 'haystacks': 1, 'retreived': 1, 'htting': 1, 'cubicle': 1, 'windsurf': 1, 'spacious': 1, 'emty': 1, 'romantically': 1, 'romantic': 1, 'underfoot': 1, 'persue': 1, 'anticipates': 1, 'exterior': 1, 'synch': 1, 'filed': 1, 'fishers': 1, 'apparantly': 1, 'stockings': 1, 'chunky': 1, 'ripped': 1, 'lounges': 1, 'bting': 1, 'reeling': 1, 'ilks': 1, 'ouside': 1, 'longish': 1, 'circus': 1, 'levels': 1, 'mullet': 1, 'heights': 1, 'bang': 1, 'punkish': 1, 'crudely': 1, 'howls': 1, 'dipping': 1, 'brandishes': 1, 'harly': 1, 'davison': 1, 'moter': 1, 'yellowish': 1, 'waterskiis': 1, 'garmet': 1, 'spiritual': 1, 'landmark': 1, 'positions': 1, 'torii': 1, 'indy': 1, 'pits': 1, 'lakefront': 1, 'applebee': 1, 'dave': 1, 'buster': 1, 'arriving': 1, 'bandages': 1, 'fishscales': 1, 'anciet': 1, 'shave': 1, 'encouraged': 1, 'banister': 1, '75': 1, 'powerful': 1, 'guessing': 1, 'verbal': 1, 'exchange': 1, 'perplexed': 1, 'nussle': 1, 'cushions': 1, 'gorge': 1, 'stockcar': 1, 'become': 1, 'seeing': 1, 'elementary': 1, 'protesting': 1, 'raling': 1, 'occured': 1, 'engulf': 1, 'mogul': 1, 'vision': 1, 'tells': 1, 'secret': 1, 'tobaggons': 1, 'saucers': 1, 'disks': 1, 'fuchsia': 1, 'emphatically': 1, 'expressing': 1, 'opinion': 1, 'pleadingly': 1, 'flocking': 1, 'unspooled': 1, 'infants': 1, 'rottwieler': 1, 'maids': 1, 'lowering': 1, 'teases': 1, 'energetic': 1, 'moniter': 1, 'winters': 1, 'panda': 1, 'gotten': 1, 'riverwater': 1, 'inflating': 1, 'skipped': 1, 'busying': 1, 'streetpole': 1, 'schoolyard': 1, 'classmates': 1, 'creams': 1, 'producing': 1, 'scraping': 1, 'lopes': 1, 'redish': 1, 'met': 1, 'orders': 1, 'resaurant': 1, 'determination': 1, 'footpath': 1, 'coutryside': 1, 'argues': 1, 'simultaneously': 1, 'decide': 1, 'churning': 1, 'res': 1, 'iove': 1, 'message': 1, 'jubilant': 1, 'entire': 1, 'paneling': 1, 'readied': 1, 'remax': 1, 'wetland': 1, 'fronds': 1, 'reson': 1, 'notices': 1, 'ducky': 1, 'onesie': 1, 'dreary': 1, 'visiting': 1, 'wuth': 1, 'collapses': 1, 'll': 1, 'bean': 1, 'manuever': 1, 'faint': 1, 'hooked': 1, 'terrorizes': 1, 'sponges': 1, 'diagram': 1, 'anatomy': 1, 'tell': 1, 'fortune': 1, 'negotiates': 1, 'patroling': 1, 'snarly': 1, 'multicoloured': 1, 'ls': 1, 'nametags': 1, 'gambling': 1, 'gorup': 1, 'ceremonial': 1, 'tassel': 1, 'stoic': 1, 'stubbled': 1, 'vigorous': 1, 'bring': 1, 'clasped': 1, 'childrens': 1, 'plan': 1, 'anouther': 1, 'diry': 1, 'jars': 1, 'hapy': 1, 'attacks': 1, 'passin': 1, 'cork': 1, 'burst': 1, 'experimenter': 1, 'breathes': 1, 'obscures': 1, 'discovers': 1, 'suggestive': 1, 'picnickers': 1, 'sunbathing': 1, 'wedgie': 1, 'warning': 1, 'hackey': 1, 'easily': 1, 'passageway': 1, 'packs': 1, 'christmastime': 1, 'strides': 1, 'sill': 1, 'vents': 1, 'clowds': 1, 'mold': 1, 'backlegs': 1, 'ot': 1, 'turtle': 1, 'unamused': 1, 'arid': 1, 'gingerbread': 1, 'downpour': 1, 'distressed': 1, 'comforter': 1, 'unner': 1, 'collision': 1, 'overturn': 1, 'deciding': 1, 'backgound': 1, 'entertainer': 1, 'curtsey': 1, 'lagging': 1, 'checker': 1, 'stocky': 1, 'husk': 1, 'armchair': 1, 'ladys': 1, 'roiling': 1, 'guiutarist': 1, 'soapy': 1, 'cleaned': 1, 'kiddy': 1, 'whild': 1, 'occassion': 1, 'showroom': 1, 'coopers': 1, 'daft': 1, 'hoolahoops': 1, 'viewfinder': 1, 'goaltender': 1, 'cautious': 1, 'checkstand': 1, 'aprons': 1, 'coast': 1, 'convention': 1, 'glide': 1, 'gelled': 1, 'wrist': 1, 'confronting': 1, 'tilling': 1, 'thatch': 1, 'portrate': 1, 'foyer': 1, 'catc': 1, 'recieve': 1, 'cornstalks': 1, 'roasted': 1, 'astro': 1, 'competes': 1, 'elbows': 1, 'muti': 1, 'pigs': 1, 'sewer': 1, 'sum': 1, 'competiting': 1, 'competiton': 1, 'seater': 1, 'panelling': 1, 'hardscape': 1, 'objest': 1, 'strokes': 1, 'cadet': 1, 'properly': 1, 'convoy': 1, 'interrupts': 1, 'goals': 1, 'swirl': 1, 'protector': 1, 'grouchy': 1, 'trekkies': 1, 'draping': 1, 'tucked': 1, 'trundles': 1, 'mechanisms': 1, 'oppsite': 1, 'arguing': 1, 'eyese': 1, 'tickled': 1, 'pompoms': 1, 'neckties': 1, 'foil': 1, 'east': 1, 'ox': 1, 'seedoo': 1, 'takeing': 1, 'chimes': 1, 'vaults': 1, 'cavern': 1, 'bathed': 1, 'goldfish': 1, 'sheepdogs': 1, 'mastif': 1, 'seaguls': 1, 'gren': 1, 'somersaults': 1, 'cartwheeling': 1, 'depicts': 1, 'litttle': 1, 'toothpaste': 1, 'handwritten': 1, 'gentle': 1, 'fairly': 1, 'whie': 1, 'sheltered': 1, 'judgement': 1, 'acrobat': 1, 'entangles': 1, 'collegiate': 1, 'articles': 1, 'cleared': 1, 'beaten': 1, 'headscarves': 1, 'classical': 1, 'homerun': 1, 'safe': 1, 'fails': 1, 'remaining': 1, 'wants': 1, 'last': 1, 'quaint': 1, 'teeing': 1, 'virtual': 1, 'projected': 1, 'joined': 1, 'social': 1, 'partake': 1, 'worm': 1, 'evident': 1, 'magizine': 1, 'pamphlet': 1, 'ballerina': 1, 'backstage': 1, 'eyeshadow': 1, 'clifftop': 1, 'crewmen': 1, 'supervise': 1, 'ships': 1, 'bullfighting': 1, 'matador': 1, 'bullrider': 1, '17': 1, 'peircings': 1, 'fadora': 1, 'mussels': 1, 'classes': 1, 'here': 1, 'girlfriends': 1, 'forms': 1, 'intricate': 1, 'butts': 1, 'handlers': 1, 'tonge': 1, 'critter': 1, 'pice': 1, 'machinery': 1, 'busk': 1, 'react': 1, 'speakers': 1, 'midjump': 1, 'colourfully': 1, 'jewels': 1, 'basett': 1, 'biscut': 1, 'fiveteen': 1, 'flatscreen': 1, 'packaged': 1, 'gifts': 1, 'sang': 1, 'doge': 1, 'frisbie': 1, 'uniquely': 1, 'desserts': 1, 'emits': 1, 'flare': 1, 'let': 1, 'sweatshir': 1, 'signature': 1, 'document': 1, 'signed': 1, 'romps': 1, 'hustle': 1, 'multiracial': 1, 'obscure': 1, 'arc': 1, 'karts': 1, 'leotards': 1, 'windsailing': 1, 'less': 1, 'developed': 1, 'government': 1, 'collarless': 1, 'courthouse': 1, 'windboarder': 1, 'windboard': 1, 'beg': 1, 'composed': 1, 'gauzey': 1, 'giggling': 1, 'bundle': 1, 'twho': 1, 'passifier': 1, 'clergy': 1, 'obese': 1, 'heather': 1, 'foldable': 1, 'geological': 1, 'jets': 1, 'adoring': 1, 'rustic': 1, 'counry': 1, 'solo': 1, 'tale': 1, 'pitted': 1, 'outstreched': 1, 'cavorts': 1, 'egg': 1, 'headresses': 1, 'bridges': 1, 'alertly': 1, 'cradling': 1, 'peolple': 1, 'looms': 1, 'sitts': 1, 'zag': 1, 'offf': 1, 'apex': 1, 'layered': 1, 'motioning': 1, 'mcdonald': 1, 'monitoring': 1, 'gradual': 1, 'waterproof': 1, 'claws': 1, 'hoist': 1, 'scrolled': 1, 'patterns': 1, 'bulging': 1, 'apartments': 1, 'roofs': 1, 'powerwashing': 1, 'outward': 1, 'blitz': 1, 'crocheted': 1, 'pagent': 1, 'attempted': 1, 'conical': 1, 'lightsaber': 1, 'ont': 1, 'drooping': 1, 'lesson': 1, 'waterproofs': 1, 'bundles': 1, 'zombies': 1, 'everybody': 1, 'pinscher': 1, 'squeamish': 1, 'reacting': 1, 'spurting': 1, 'crushed': 1, 'encripted': 1, 'snaps': 1, 'captures': 1, 'cathing': 1, 'entertains': 1, 'wearhing': 1, 'furred': 1, 'paralell': 1, 'medow': 1, 'kildare': 1, 'creeping': 1, 'lifevest': 1, 'browm': 1, 'courts': 1, 'hungry': 1, 'weilding': 1, 'gnome': 1, 'coveralls': 1, 'breaded': 1, 'shimp': 1, 'facefirst': 1, 'skeptically': 1, 'modifications': 1, 'continues': 1, 'temporary': 1, 'donations': 1, 'blindfold': 1, 'mouthpiece': 1, 'miniskirts': 1, 'schoolgirl': 1, 'schoolboy': 1, 'production': 1, 'walled': 1, 'science': 1, 'prefabricated': 1, 'reds': 1, 'yellows': 1, 'additional': 1, 'patrons': 1, 'siren': 1, 'sillhouetted': 1, 'fribee': 1, 'islamic': 1, 'holy': 1, 'tramples': 1, 'trampled': 1, 'whoa': 1, 'hoofs': 1, 'stumbling': 1, 'mandolin': 1, 'plucking': 1, 'chello': 1, 'rebound': 1, 'contained': 1, 'tabloid': 1, 'behinf': 1, 'beack': 1, 'ee': 1, 'roundabout': 1, 'charged': 1, 'mishap': 1, 'prarie': 1, 'coyotes': 1, 'dryed': 1, 'nipples': 1, 'peirced': 1, 'aside': 1, 'ipods': 1, 'create': 1, 'tapped': 1, 'horseriders': 1, 'rounded': 1, 'hapily': 1, 'unshaven': 1, 'models': 1, 'clustered': 1, 'communal': 1, 'darts': 1, 'cafeteria': 1, 'silverware': 1, 'repair': 1, 'doughnut': 1, 'dimlight': 1, 'total': 1, 'reported': 1, 'defying': 1, 'gravity': 1, 'redwood': 1, 'sequoia': 1, 'situated': 1, 'chidl': 1, 'latter': 1, 'rasing': 1, 'adornment': 1, 'eyepatch': 1, 'snowdrift': 1, 'earnhardt': 1, 'jewish': 1, 'disgusted': 1, 'everything': 1, 'giong': 1, 'excess': 1, 'bartender': 1, 'handcrank': 1, 'marlins': 1, 'identifier': 1, 'retriving': 1, 'produces': 1, 'heating': 1, 'dacshund': 1, 'soles': 1, 'jumpy': 1, 'variety': 1, 'folks': 1, 'my': 1, 'buddy': 1, 'dizzy': 1, 'breastfeeding': 1, 'suckles': 1, 'yacht': 1, 'barge': 1, 'spiking': 1, 'buss': 1, 'london': 1, 'ferrett': 1, 'scalling': 1, 'anti': 1, 'dont': 1, 'me': 1, 'bro': 1, 'skidded': 1, 'bandanas': 1, 'cloths': 1, 'sweashirt': 1, 'torwards': 1, 'spashes': 1, 'yamaha': 1, 'eagerly': 1, 'key': 1, 'hp': 1, 'endorsement': 1, 'antoher': 1, 'recital': 1, 'metropolitain': 1, 'hatchback': 1, 'russel': 1, 'urge': 1, 'magnificant': 1, 'ponytailed': 1, 'jetskiing': 1, 'footrace': 1, 'nest': 1, 'celtics': 1, 'huskey': 1, 'pressing': 1, 'stylish': 1, 'signer': 1, 'pressure': 1, 'payer': 1, 'jaket': 1, 'confront': 1, 'chaperone': 1, 'kong': 1, 'tomatos': 1, 'groupe': 1, 'smilely': 1, 'watersports': 1, 'seperated': 1, 'scowls': 1, 'gallopsing': 1, 'striding': 1, 'pedigree': 1, 'aloft': 1, 'cringes': 1, 'oval': 1, 'pane': 1, 'barbeque': 1, 'beaked': 1, 'agains': 1, 'colred': 1, '1950s': 1, 'bones': 1, 'beached': 1, 'ladie': 1, 'hte': 1, 'garland': 1, 'brazilian': 1, 'lei': 1, 'slipping': 1, 'telephot': 1, 'lenses': 1, 'tripods': 1, 'shorline': 1, 'brought': 1, 'pastel': 1, 'pastels': 1, 'retreiving': 1, 'scratch': 1, 'forelegs': 1, 'sprinkles': 1, 'sprinking': 1, 'hoddie': 1, 'swimcap': 1, 'amazed': 1, 'scanner': 1, 'paced': 1, 'helplessly': 1, 'coping': 1, 'speedskater': 1, 'fireside': 1, 'jumpropes': 1, 'containig': 1, 'rainstorm': 1, 'burns': 1, 'outline': 1, 'unified': 1, 'expansive': 1, 'dice': 1, 'bodyless': 1, 'turbaned': 1, 'spokesmodels': 1, 'hotrod': 1, 'zebra': 1, 'burnished': 1, 'dealing': 1, 'flanked': 1, 'force': 1, 'padel': 1, 'riverrafting': 1, 'challange': 1, 'adopted': 1, 'waterhole': 1, 'glacial': 1, 'lilies': 1, 'eluding': 1, 'subaru': 1, 'suspension': 1, 'bicyler': 1, 'parachutist': 1, 'unfurled': 1, 'fold': 1, 'celebratory': 1, 'streched': 1, 'suburbs': 1, 'pavilion': 1, 'minnie': 1, 'engraved': 1, 'names': 1, 'twisty': 1, 'contemporary': 1, 'corporate': 1, 'accends': 1, 'lifeguards': 1, 'motorcross': 1, 'savanah': 1, 'tipped': 1, 'miles': 1, 'lakes': 1, 'blog': 1, 'collapsed': 1, 'wisconsin': 1, 'styrofoam': 1, 'tanktops': 1, 'cosplayers': 1, 'actors': 1, 'inspect': 1, 'wisks': 1, 'mesa': 1, 'outfut': 1, 'guidewire': 1, 'springer': 1, 'gra': 1, 'icey': 1, 'hedge': 1, 'stoney': 1, 'gnarly': 1, 'relatively': 1, 'mosaic': 1, 'belaying': 1, 'propelling': 1, 'hatted': 1, 'frown': 1, 'celebrities': 1, 'musicans': 1, 'posh': 1, 'nudges': 1, 'stoll': 1, 'shadowy': 1, 'consoling': 1, 'witches': 1, 'fastest': 1, 'bikina': 1, 'saroog': 1, 'sarongs': 1, 'coliding': 1, 'swinsuit': 1, 'armed': 1, 'impress': 1, 'splatter': 1, 'splattered': 1, 'graffti': 1, 'twos': 1, 'monkeys': 1, 'springtime': 1, 'rested': 1, 'twenty': 1, 'fourth': 1, 'payphone': 1, 'pay': 1, 'coloful': 1, 'holiday': 1, 'tabs': 1, 'taps': 1, 'joyful': 1, 'histerically': 1, 'exercised': 1, 'athletics': 1, 'punt': 1, 'county': 1, 'snorkeling': 1, 'scubba': 1, 'traveler': 1, 'junction': 1, 'windshield': 1, 'pause': 1, 'uptop': 1, 'docking': 1, 'explorer': 1, 'starlet': 1, 'hankerchief': 1, 'pompadour': 1, 'maps': 1, 'directory': 1, 'reviews': 1, 'organized': 1, 'smoky': 1, 'resembling': 1, 'locomotives': 1, 'lionist': 1, 'cougar': 1, 'masquerade': 1, 'severe': 1, 'dart': 1, 'converge': 1, 'choice': 1, 'pencils': 1, 'decorate': 1, 'hardly': 1, 'canes': 1, 'cacti': 1, 'fiels': 1, 'retrive': 1, 'clibing': 1, 'falcon': 1, 'persian': 1, 'gazed': 1, 'knocking': 1, 'isolated': 1, 'santas': 1, 'gandhi': 1, 'ghandi': 1, 'skying': 1, 'cubby': 1, 'cheeked': 1, 'aquatic': 1, 'ashtray': 1, 'dodge': 1, 'decortive': 1, 'expressway': 1, 'donates': 1, 'panhandler': 1, 'handout': 1, 'knuckle': 1, 'tuft': 1, 'pensively': 1, 'thinks': 1, 'bleak': 1, 'bowing': 1, 'locks': 1, 'knight': 1, 'holes': 1, 'king': 1, 'stork': 1, 'stamds': 1, 'pring': 1, 'lacross': 1, 'oppenents': 1, 'snowshovel': 1, 'layup': 1, 'sillouhette': 1, 'peacoat': 1, 'avoids': 1, 'bodyboarder': 1, 'holing': 1, 'somehow': 1, 'mic': 1, 'question': 1, 'swept': 1, 'gatorade': 1, 'furious': 1, 'explosive': 1, 'effects': 1, 'explosions': 1, '50': 1, 'gust': 1, 'flurry': 1, 'laborador': 1, 'coyote': 1, 'walkways': 1, 'spaced': 1, 'maple': 1, 'prepairing': 1, 'coppery': 1, 'jacks': 1, 'm': 1, 'bushels': 1, 'established': 1, 'nightlife': 1, 'fairgrounds': 1, 'farris': 1, 'potao': 1, 'dupar': 1, 'swear': 1, 'floatlys': 1, 'foliaged': 1, 'roped': 1, 'granite': 1, 'blasted': 1, 'grownup': 1, 'underpass': 1, 'bikins': 1, 'wodden': 1, 'baskers': 1, 'poll': 1, 'mushrooms': 1, 'broad': 1, 'algae': 1, 'petterned': 1, 'billiards': 1, 'cue': 1, 'busines': 1, 'closeout': 1, 'underhang': 1, 'enterance': 1, 'fooling': 1, 'lifesavers': 1, 'creamy': 1, 'motorcrossing': 1, '157': 1, 'professionally': 1, 'vibrant': 1, 'vat': 1, 'discovery': 1, 'playy': 1, 'perforced': 1, 'masonry': 1, 'wispy': 1, 'daylight': 1, 'operated': 1, 'smokestacks': 1, 'hanglider': 1, 'rectangle': 1, 'rackets': 1, 'manager': 1, 'stages': 1, 'landscaped': 1, 'escorts': 1, 'shetland': 1, 'currently': 1, 'summersault': 1, 'meanders': 1, 'canooers': 1, 'tikes': 1, 'stared': 1, 'measuring': 1, 'rodents': 1, 'stingray': 1, 'mittened': 1, 'grating': 1, 'drainpipe': 1, 'grate': 1, 'receiver': 1, 'shading': 1, 'protecting': 1, 'paraphernalia': 1, 'distribute': 1, 'promting': 1, 'seventh': 1, 'cin': 1, 'weating': 1, 'cleats': 1, 'serveral': 1, 'congregation': 1, 'worshipping': 1, 'worshippers': 1, 'waterbed': 1, 'slingshot': 1, 'fowl': 1, 'pees': 1, 'grand': 1, 'caribbean': 1, 'doo': 1, 'seawall': 1, 'churns': 1, 'halway': 1, 'du': 1, 'casque': 1, 'obligatoire': 1, 'pinches': 1, 'spangles': 1, 'glee': 1, 'surroundings': 1, 'tophats': 1, 'restroom': 1, 'boatful': 1, 'aquestrian': 1, 'bordering': 1, 'autumnal': 1, 'bumble': 1, 'assemble': 1, 'agency': 1, 'spouse': 1, 'embroidered': 1, 'chipmunk': 1, 'ducking': 1, 'treefilled': 1, 'swarming': 1, 'intended': 1, 'mine': 1, 'preserves': 1, 'eyeing': 1, 'passenager': 1, 'forcing': 1, 'smashed': 1, 'cappedhills': 1, 'win': 1, 'kinds': 1, 'softdrinks': 1, 'oclock': 1, 'spectating': 1, 'dripping': 1, 'cigerette': 1, 'graffitti': 1, 'surfaces': 1, 'stroke': 1, 'fantastic': 1, 'marvel': 1, 'bookshelves': 1, 'fantasy': 1, 'wax': 1, 'foggyday': 1, 'prow': 1, 'waing': 1, 'antelope': 1, 'wildebeast': 1, 'trips': 1, 'strand': 1, 'strung': 1, 'blasts': 1, 'poncho': 1, 'balconies': 1, 'condominium': 1, 'awaits': 1, 'attampts': 1, 'inflatbale': 1, 'seeking': 1, 'humorous': 1, 'rim': 1, 'steap': 1, 'farward': 1, 'ever': 1, 'since': 1, 'started': 1, 'pinball': 1, 'usual': 1, 'cheeseburger': 1, 'happen': 1, 'outfield': 1, 'samoyads': 1, 'waterspout': 1, 'untouched': 1, 'fireplug': 1, 'thinner': 1, 'shredding': 1, 'headcover': 1, 'ensemble': 1, 'utility': 1, 'kilts': 1, 'lame': 1, 'cuddling': 1, 'cower': 1, 'brownstone': 1, 'sheilding': 1, 'berets': 1, 'waterspouts': 1, 'bubbling': 1, 'comforts': 1, 'treed': 1, 'challenges': 1, 'sunhat': 1, 'unoccupied': 1, 'santana': 1, '57': 1, 'bras': 1, 'sponsorship': 1, 'emblems': 1, 'rights': 1, 'wrestilng': 1, 'rowed': 1, 'circumvents': 1, 'skii': 1, 'steamy': 1, 'rapid': 1, 'kayacker': 1, 'downstream': 1, 'squated': 1, 'am': 1, 'ghost': 1, 'busters': 1, 'ghostbuster': 1, 'impersonators': 1, 'perused': 1, 'herbs': 1, 'racks': 1, 'midpitch': 1, 'profession': 1, 'diferent': 1, 'foreheads': 1, 'advertisment': 1, 'magic': 1, 'magicians': 1, 'magician': 1, 'backstand': 1, 'introduces': 1, 'invention': 1, 'attaches': 1, 'tends': 1, 'lingers': 1, 'seabird': 1, 'remove': 1, 'replaces': 1, 'flees': 1, 'agents': 1, 'shaky': 1, 'slat': 1, 'driftrood': 1, 'battons': 1, 'twome': 1, 'sync': 1, 'practising': 1, 'jersy': 1, 'drumset': 1, 'shreds': 1, 'crustacean': 1, 'halmets': 1, 'kaki': 1, 'shews': 1, 'jewlery': 1, 'braided': 1, 'escalators': 1, 'expressionless': 1, 'responding': 1, 'sunning': 1, 'orangish': 1, 'boas': 1, 'manequins': 1, 'unpainted': 1, 'jomps': 1, 'lunches': 1, 'compound': 1, 'tumbles': 1, 'wrecked': 1, '661': 1, 'wrecks': 1, 'tightropes': 1, 'frayed': 1, 'pawed': 1, 'personal': 1, 'womand': 1, 'twp': 1, 'ciff': 1, 'excersizing': 1, 'fleeing': 1, 'prison': 1, 'zippered': 1, 'navel': 1, 'cuddle': 1, 'then': 1, 'rollskating': 1, 'easy': 1, 'aided': 1, 'midfield': 1, 'lightly': 1, 'orangesunset': 1, 'closer': 1, 'annoyed': 1, 'exercising': 1, 'mortar': 1, 'destination': 1, 'lifevests': 1, 'reveals': 1, 'waterpark': 1, 'caterpillar': 1, 'burbur': 1, 'detector': 1, 'justice': 1, 'kneepads': 1, 'gushing': 1, 'allowing': 1, 'planting': 1, 'diplomas': 1, 'tye': 1, 'programs': 1, 'bums': 1, 'mission': 1, 'stacking': 1, 'dingo': 1, 'fox': 1, 'rustric': 1, 'crotch': 1, 'burgers': 1, 'mono': 1, 'stucco': 1, 'gutarist': 1, 'upfront': 1, 'at&t': 1, 'fins': 1, 'beauty': 1, 'smiff': 1, 'behinds': 1, 'brickwall': 1, 'juming': 1, 'numeral': 1, 'wanders': 1, 'flattened': 1, 'nech': 1, 'planter': 1, 'ion': 1, 'braiding': 1, 'types': 1, 'coral': 1, 'ca': 1, 'zchtv': 1, 'footage': 1, 'protected': 1, 'fending': 1, 'representing': 1, 'beaver': 1, 'goldenrod': 1, 'handstands': 1, 'automobile': 1, 'gains': 1, 'milkweed': 1, 'pods': 1, 'litle': 1, 'bead': 1, 'earphone': 1, 'bluetooth': 1, 'showered': 1, 'screened': 1, 'roofed': 1, 'straggle': 1, 'tupperware': 1, 'hobby': 1, '42': 1, 'browsing': 1, 'snowflake': 1, 'chasseing': 1, 'dons': 1, 'accent': 1, 'worked': 1, 'tailed': 1, 'comfortable': 1, 'buttoned': 1, 'rivers': 1, 'rags': 1, 'district': 1, 'backgrounds': 1, 'hairnet': 1, 'tiretracks': 1, 'tumble': 1, 'rates': 1, 'excavating': 1, 'scientist': 1, 'artifacts': 1, 'paleontologist': 1, 'archeologist': 1, 'soaker': 1, 'doe': 1, 'oak': 1, 'fronr': 1, 'crampons': 1, 'scale': 1, 'overtop': 1, 'angerly': 1, 'glowers': 1, 'touts': 1, 'playroom': 1, 'roots': 1, 'fundraising': 1, 'tripped': 1, 'hooding': 1, 'matt': 1, 'valleys': 1, 'stereo': 1, 'trams': 1, 'contestants': 1, 'erupts': 1, 'firecracker': 1, 'footballs': 1, 'breakdances': 1, 'seahorse': 1, 'gettnig': 1, 'clasp': 1, 'hovered': 1, 'ripstik': 1, 'sidewalks': 1, 'joust': 1, 'horsemen': 1, 'tier': 1, 'submissive': 1, 'girt': 1, 'solicits': 1, 'stirred': 1, 'snowbanks': 1, 'global': 1, 'perfomed': 1, 'waterboard': 1, 'shark': 1, 'seed': 1, 'protruding': 1, 'awe': 1, 'bicylist': 1, 'dimpled': 1, 'ignore': 1, 'dreeds': 1, 'winces': 1, 'wearubg': 1, 'angry': 1, 'spelunker': 1, 'parachuter': 1, 'tread': 1, 'dupont': 1, 'wagging': 1, 'fixtures': 1, 'dirtbed': 1, 'aboriginal': 1, 'chiseling': 1, 'hacking': 1, 'motors': 1, 'secluded': 1, 'shorthair': 1, 'vaste': 1, 'upstream': 1, 'liking': 1, 'nip': 1, 'cds': 1, 'include': 1, 'blank': 1, 'armful': 1, 'windbreaker': 1, 'mike': 1, 'paraglide': 1, 'visits': 1, 'sect': 1, 'plling': 1, 'downriver': 1, 'mommy': 1, 'wii': 1, 'dge': 1, 'foreigners': 1, 'lace': 1, 'moutainside': 1, 'anchored': 1, 'mature': 1, 'competitors': 1, 'bash': 1, 'sushi': 1, 'headwear': 1, 'rellow': 1, 'direct': 1, 'flaggers': 1, 'wavy': 1, 'accelerates': 1, 'dragster': 1, 'speedway': 1, 'spewing': 1, 'mole': 1, 'em': 1, 'whacking': 1, 'aliens': 1, 'cautiously': 1, 'sure': 1, 'unfurling': 1, 'accompanying': 1, 'greenbay': 1, 'packer': 1, 'packers': 1, 'swinger': 1, 'tge': 1, 'thank': 1, 'angel': 1, 'waling': 1, 'repel': 1, 'milling': 1, 'snowploe': 1, 'plows': 1, 'manually': 1, 'id': 1, 'burnt': 1, 'cine': 1, 'rosy': 1, 'nat': 1, 'sifting': 1, 'barrior': 1, 'flares': 1, 'walkman': 1, 'powerboats': 1, 'aboard': 1, 'armor': 1, 'drove': 1, 'motorists': 1, 'atheletes': 1, 'waterline': 1, 'risen': 1, 'strains': 1, 'yougn': 1, 'tucking': 1, 'troll': 1, 'thie': 1, 'griding': 1, 'mannequin': 1, 'fiving': 1, 'enthusiastic': 1, 'wanting': 1, 'jaywalk': 1, 'taxis': 1, 'snowsuits': 1, 'smacker': 1, 'vase': 1, 'mistletoe': 1, 'skiny': 1, 'objective': 1, 'saffron': 1, 'solid': 1, 'sculptures': 1, 'coasts': 1, 'noy': 1, 'cocker': 1, 'spaniels': 1, 'creepy': 1, 'feathery': 1, 'defaced': 1, 'promotional': 1, 'fumble': 1, 'aggresively': 1, 'defender': 1, 'wmoan': 1, 'maintain': 1, 'fingerhold': 1, 'nordic': 1, 'thrust': 1, 'campflauge': 1, 'cami': 1, 'brawl': 1, 'grotto': 1, 'entertainment': 1, 'texts': 1, 'megaphone': 1, 'handkerchiefs': 1, 'adolescents': 1, 'sales': 1, 'merchant': 1, 'movers': 1, 'passers': 1, 'pulleys': 1, 'circling': 1, 'harvest': 1, 'brave': 1, 'saver': 1, 'judo': 1, 'shepherds': 1, 'momma': 1, 'dumpsters': 1, 'sick': 1, 'divind': 1, 'iceburg': 1, 'ump': 1, 'unsual': 1, 'lemons': 1, 'press': 1, 'squeezed': 1, 'tufts': 1, 'uneven': 1, 'withered': 1, 'confronted': 1, 'opposition': 1, 'advance': 1, 'would': 1, 'mansion': 1, 'euro': 1, 'plats': 1, 'chestnut': 1, 'pace': 1, 'gallop': 1, 'proped': 1, 'flyaway': 1, 'fluid': 1, 'backsides': 1, 'faith': 1, 'manmade': 1, 'bullseye': 1, 'archer': 1, 'trows': 1, '10': 1, 'ups': 1, 'operates': 1, 'equipments': 1, 'pincer': 1, 'sprawls': 1, 'preparation': 1, 'propels': 1, 'moutains': 1, 'redskins': 1, 'scare': 1, 'glares': 1, 'docks': 1, 'oiled': 1, 'suntan': 1, 'lotion': 1, 'suntanning': 1, 'honest': 1, 'poems': 1, 'desperate': 1, 'creative': 1, 'garner': 1, 'profusely': 1, 'gleefully': 1, 'scarred': 1, 'playtoy': 1, 'font': 1, 'scattered': 1, 'lost': 1, 'crosslegged': 1, 'ropey': 1, 'partition': 1, 'railgrind': 1, 'spare': 1, 'longeared': 1, 'posign': 1, 'loaves': 1, 'villages': 1, 'scored': 1, 'snowfall': 1, 'companions': 1, 'drak': 1, 'bomber': 1, 'earflaps': 1, 'carpeting': 1, 'peddal': 1, 'national': 1, 'anthem': 1, 'dirtracing': 1, 'grandmother': 1, 'placemats': 1, 'tankini': 1, 'loofa': 1, 'winston': 1, 'resembles': 1, 'sprinkled': 1, 'speach': 1, 'stadning': 1, 'baptized': 1, 'dong': 1, 'bloe': 1, 'weight': 1, 'magenta': 1, 'exception': 1, 'ridable': 1, 'wristband': 1, 'patterened': 1, 'tidal': 1, 'overflow': 1, 'rhododendron': 1, 'firends': 1, 'flamboyantly': 1, 'frisbeen': 1, 'enters': 1, 'amazement': 1, 'acrouss': 1, 'rigging': 1, 'mast': 1, 'canvasses': 1, 'muscled': 1, 'feel': 1, 'angled': 1, 'lanyard': 1, 'wharfs': 1, 'streches': 1, 'bracing': 1, 'visitor': 1, 'contestent': 1, 'bucked': 1, 'togerther': 1, 'quite': 1, 'askance': 1, 'rockets': 1, 'jumpos': 1, 'mutltiple': 1, 'fielder': 1, 'remember': 1, '1915': 1, '1923': 1, 'related': 1, 'heritage': 1, 'nd': 1, 'breakdancer': 1, 'chalked': 1, 'grid': 1, 'curving': 1, 'keffiyahs': 1, 'bottomed': 1, 'tigger': 1, 'evade': 1, 'tw': 1, 'mocks': 1, 'moms': 1, 'caches': 1, 'patrollers': 1, 'minerature': 1, 'supervising': 1, 'arranged': 1, 'respectively': 1, 'cements': 1, 'bannister': 1, 'conch': 1, 'convienance': 1, 'joint': 1, 'slurpees': 1, 'convienience': 1, 'excercises': 1, 'shored': 1, 'regains': 1, 'composure': 1, 'loooking': 1, 'nubby': 1, 'content': 1, 'surronded': 1, 'threshold': 1, 'struck': 1, 'shin': 1, 'purchased': 1, 'cashier': 1, 'checkout': 1, 'calculate': 1, 'volkswagen': 1, 'admired': 1, 'beetle': 1, 'coupe': 1, 'autos': 1, 'south': 1, 'kickboxer': 1, 'kickbox': 1, 'reared': 1, 'rotating': 1, 'boatload': 1, 'fear': 1, 'croc': 1, 'healthy': 1, 'joyfully': 1, 'playstation': 1, 'sandcastles': 1, 'cupcake': 1, 'dribbled': 1, 'piling': 1, 'midstride': 1, 'crooswalk': 1, 'advertizing': 1, 'pee': 1, 'trained': 1, 'braces': 1, 'iceskate': 1, 'lited': 1, 'woooden': 1, 'lilypads': 1, 'lillypads': 1, 'approachs': 1, 'focusing': 1, 'halfpipe': 1, 'spoted': 1, 'sheperds': 1, 'stiars': 1, 'lav': 1, 'kelp': 1, 'sightseers': 1, 'scope': 1, 'cellos': 1, 'violins': 1, 'orchestra': 1, 'quintet': 1, 'whizzes': 1, 'dandilions': 1, 'frustrated': 1, 'frisbree': 1, 'subdivsion': 1, 'cylindrical': 1, 'universal': 1, 'behing': 1, 'thows': 1, 'transport': 1, 'strawberries': 1, 'potrait': 1, 'informal': 1, 'gi': 1, 'ignores': 1, 'masters': 1, 'fend': 1, 'soocerball': 1, 'swordfighting': 1, 'converging': 1, 'raingear': 1, 'apportioned': 1, 'adjusts': 1, 'twist': 1, 'foraging': 1, 'windsurfers': 1, 'modeling': 1, 'catwalk': 1, 'photographic': 1, 'tin': 1, 'conoe': 1, 'tissue': 1, 'exited': 1, 'did': 1, 'booklets': 1, 'telescopes': 1, 'orthodox': 1, 'excersise': 1, 'superhero': 1, 'xmen': 1, 'poppies': 1, 'someplace': 1, 'distnat': 1, 'level': 1, 'mardis': 1, 'suggestively': 1, 'tough': 1, 'cheap': 1, 'reentry': 1, 'hippies': 1, 'backbend': 1, 'interacts': 1, 'exchanges': 1, 'fastens': 1, 'fixes': 1, 'vested': 1, 'zooms': 1, 'chihuahua': 1, 'tak': 1, 'picutre': 1, 'ones': 1, 'verdant': 1, 'bustling': 1, 'typical': 1, 'treck': 1, 'alien': 1, 'attend': 1, 'lakeside': 1, 'fiddle': 1, 'streetlights': 1, 'ther': 1, 'tim': 1, 'hortons': 1, 'otuside': 1, 'judea': 1, 'blame': 1, 'hypocrites': 1, 'againest': 1, 'perfume': 1, 'broen': 1, 'concentration': 1, 'snoring': 1, 'shawled': 1, 'angrily': 1, 'fig': 1, 'aerobatics': 1, 'underneat': 1, 'competitively': 1, 'rollerblade': 1, 'caramel': 1, 'skimming': 1, 'guitarists': 1, 'trombone': 1, 'development': 1, 'cube': 1, 'traveller': 1, 'thread': 1, 'spindle': 1, 'thirds': 1, 'primping': 1, 'kakhi': 1, 'spirit': 1, 'pep': 1, 'nations': 1, 'cowgirls': 1, 'uplifted': 1, 'slaloms': 1, 'bordered': 1, 'gateway': 1, 'crosswalks': 1, 'policeperson': 1, 'assault': 1, 'rif=ding': 1, 'voice': 1, 'glassy': 1, 'scull': 1, 'sponsor': 1, 'motorcycling': 1, 'violinist': 1, 'headress': 1, 'skislope': 1, 'spelling': 1, 'comprised': 1, 'newlywed': 1, 'guests': 1, 'contorted': 1, 'enthusiasts': 1, 'nothing': 1, 'revealed': 1, 'prints': 1, 'factory': 1, 'moutnain': 1, 'implements': 1, 'utilities': 1, 'menacingly': 1, 'grimmaces': 1, 'dine': 1, 'weeping': 1, 'willow': 1, 'pepper': 1, 'queue': 1, 'waitress': 1, 'burka': 1, 'vocabulary': 1, 'blackboard': 1, 'student': 1, 'airtime': 1, 'flighht': 1, 'arrives': 1, 'wishing': 1, 'abdomen': 1, 'midriff': 1, 'shredded': 1, 'caucasion': 1, 'abs': 1, 'unzipping': 1, 'huddles': 1, 'fingertips': 1, 'egde': 1, 'plungles': 1, 'positioned': 1, 'brighty': 1, 'misspelled': 1, 'fanning': 1, 'beaks': 1, 'scrap': 1, 'average': 1, 'everyday': 1, 'takeout': 1, 'afican': 1, 'izod': 1, '93': 1, 'weiner': 1, 'driftwood': 1, 'democrat': 1, 'supporters': 1, 'election': 1, 'chris': 1, 'gregoire': 1, 'palace': 1, 'looling': 1, 'guardsman': 1, 'fourwheeler': 1, 'turquiose': 1, 'gauntlet': 1, 'fitness': 1, 'excercise': 1, 'snowballs': 1, 'glvoes': 1, 'backflips': 1, 'pepco': 1, 'pecks': 1, 'pecking': 1, 'cavorting': 1, 'pastures': 1, 'pinestraw': 1, 'popped': 1, 'alcove': 1, 'streght': 1, 'amoung': 1, 'maintained': 1, 'drab': 1, 'foreround': 1, 'rodderick': 1, 'roddick': 1, 'extravagantly': 1, 'stiffly': 1, 'h': 1, 'ledges': 1, 'mysterious': 1, 'wilbert': 1, 'opportunity': 1, 'hangong': 1, 'coca': 1, 'cola': 1, 'cottage': 1, 'rotary': 1, 'dial': 1, 'faux': 1, 'closeby': 1, 'strolling': 1, 'squabble': 1, 'canoers': 1, 'moutain': 1, 'disney': 1, 'arabic': 1, 'widely': 1, 'hudge': 1, 'populated': 1, 'appearing': 1, 'malnourished': 1, 'oppsing': 1, 'untangles': 1, 'hsirt': 1, 'losing': 1, 'lecturing': 1, 'clue': 1, 'shute': 1, 'debri': 1, 'pub': 1, 'weimeraners': 1, 'vegetable': 1, 'fruits': 1, 'venture': 1, 'portfolio': 1, 'cases': 1, 'buries': 1, 'burrows': 1, 'swetashirts': 1, 'forehand': 1, 'catered': 1, 'buffet': 1, 'disrupt': 1, 'disguise': 1, 'supervision': 1, 'cyclers': 1, 'capture': 1, 'tundra': 1, 'snapshot': 1, 'choreographed': 1, 'jugs': 1, 'origin': 1, 'colecting': 1, 'common': 1, 'readies': 1, 'vike': 1, 'expose': 1, 'corgie': 1, 'wiffle': 1, 'fronmt': 1, 'caucasian': 1, 'gemmed': 1, 'obsured': 1, 'sleve': 1, 'graham': 1, 'blossom': 1, 'separated': 1, 'yet': 1, 'zone': 1, 'surounded': 1, 'soar': 1, 'bohemians': 1, 'prance': 1, 'nowhere': 1, 'dashboard': 1, 'twelve': 1, 'diamond': 1, 'curls': 1, 'vacationing': 1, 'od': 1, 'gothically': 1, 'cosplay': 1, 'pilings': 1, 'porcelain': 1, 'folds': 1, 'dimond': 1, 'interestingly': 1, 'cathedral': 1, 'lamppost': 1, 'jacketed': 1, 'belted': 1, 'cables': 1, 'proudly': 1, 'wrangle': 1, 'skims': 1, 'spokes': 1, 'ruin': 1, 'weas': 1, 'edged': 1, 'highschool': 1, 'defended': 1, 'dramatic': 1, 'attach': 1, 'forearm': 1, 'shortly': 1, 'samll': 1, 'mitsubishi': 1, 'blueish': 1, 'sprinkling': 1, 'roads': 1, 'washer': 1, 'rummages': 1, 'rummage': 1, 'collaborating': 1, 'calming': 1, 'defense': 1, 'tasting': 1, 'forearms': 1, 'measures': 1, 'depth': 1, 'steaks': 1, 'thck': 1, 'pylons': 1, 'shivering': 1, 'shivers': 1, 'anything': 1, 'planked': 1, 'skit': 1, 'dismounts': 1, 'shields': 1, 'unconventional': 1, 'naturally': 1, 'trio': 1, 'grazing': 1, 'popsicles': 1, 'popscicles': 1, 'lollipops': 1, 'popcycles': 1, 'damp': 1, 'swimsuites': 1, '32': 1, 'directing': 1, 'director': 1, 'mama': 1, 'cub': 1, 'felled': 1, 'text': 1, 'sprinkers': 1, 'squeals': 1, 'loader': 1, 'crescent': 1, 'skill': 1, 'rippling': 1, 'ping': 1, 'backback': 1, 'hoists': 1, 'motorcyle': 1, 'jacuzzi': 1, 'laughed': 1, 'playmat': 1, 'sillhouttes': 1, 'climbes': 1, 'extending': 1, 'ivars': 1, 'soccor': 1, 'twins': 1, 'panoramic': 1, 'darked': 1, 'talent': 1, 'shire': 1, 'muffs': 1, 'headline': 1, 'angles': 1, 'smelled': 1, 'gound': 1, 'refugees': 1, 'families': 1, 'streetlamp': 1, 'blades': 1, 'shriner': 1, 'mercury': 1, 'pnc': 1, 'offered': 1, 'happiness': 1, 'glassses': 1, 'dunked': 1, 'sedan': 1, 'dropped': 1, 'boxy': 1, 'cruising': 1, 'yachts': 1, 'pursuing': 1, 'fiddles': 1, 'miscellaneous': 1, 'bmxer': 1, 'rearview': 1, 'streams': 1, '90': 1, 'degrees': 1, 'decoy': 1, 'yerba': 1, 'buena': 1, 'shoelaces': 1, 'clack': 1, 'exposition': 1, 'arranging': 1, 'banana': 1, 'iguanas': 1, 'wrestled': 1, 'reptiles': 1, 'dominance': 1, 'romping': 1, 'shells': 1, 'cereal': 1, 'rainling': 1, 'pipeline': 1, 'catholic': 1, 'exibit': 1, 'kerry': 1, 'mules': 1, 'buddhists': 1, 'smal': 1, 'bad': 1, 'onsie': 1, 'cordoned': 1, 'arrows': 1, 'fayre': 1, 'bobsled': 1, 'hailing': 1, 'signaling': 1, 'needs': 1, 'oriential': 1, 'rectangular': 1, 'wards': 1, 'bathingsuit': 1, 'clowning': 1, 'espana': 1, 'spain': 1, 'pillared': 1, 'blurring': 1, 'pomeranian': 1, 'screeches': 1, 'stock': 1, 'tipping': 1, 'bulletproof': 1, 'proof': 1, 'chemical': 1, 'hilltops': 1, 'actor': 1, 'sections': 1, 'gover': 1, 'ask': 1, 'peddles': 1, 'enviorment': 1, 'zagging': 1, 'lookout': 1, 'viewpoint': 1, 'slowly': 1, 'emotionally': 1, 'spectate': 1, 'swine': 1, 'dozen': 1, 'snowbound': 1, 'texas': 1, 'already': 1, 'shift': 1, 'holey': 1, 'whiffle': 1, 'frowns': 1, 'horizontally': 1, 'fiercely': 1, '528': 1, 'shotput': 1, 'sentence': 1, 'parasurfs': 1, 'snub': 1, 'smells': 1, 'werewolf': 1, 'grassed': 1, 'lantern': 1, 'backset': 1, 'withering': 1, 'scampers': 1, 'jib': 1, 'fixer': 1, 'wetsuite': 1, 'derssed': 1, 'sidwalk': 1, 'wearfing': 1, 'garments': 1, 'peacefully': 1, 'pleasant': 1, 'trendy': 1, 'dimmly': 1, 'canals': 1, 'ganilla': 1, 'shouts': 1, 'footed': 1, 'nitro': 1, 'horro': 1, 'bloodied': 1, 'haloween': 1, 'campaign': 1, 'supporter': 1, 'swordfight': 1, 'dueling': 1, 'israei': 1, 'handkerchief': 1, 'ampitheater': 1, 'memorabilia': 1, 'observer': 1, 'swatted': 1, 'dashes': 1, 'went': 1, 'aveda': 1, 'buys': 1, 'eccentric': 1, 'icicles': 1, 'scores': 1, 'wrings': 1, 'twists': 1, 'wringing': 1, 'djs': 1, 'smacking': 1, 'grayish': 1, 'killer': 1, 'orca': 1, 'seaworld': 1, 'blurs': 1, 'cruisship': 1, 'assorted': 1, 'polar': 1, 'waeribng': 1, 'cheery': 1, 'chilly': 1, 'tychy': 1, 'miasto': 1, 'graying': 1, 'tests': 1, 'tuner': 1, 'dumped': 1, 'wakeboarders': 1, 'sour': 1, 'instructing': 1, 'instructs': 1, 'patricks': 1, 'plenty': 1, 'somersaulting': 1, 'performed': 1, 'slates': 1, 'crochet': 1, 'unexcited': 1, 'plywood': 1, 'apparently': 1, 'pitbulls': 1, 'berries': 1, 'overshadows': 1, 'retrives': 1, 'nursed': 1, 'filiming': 1, 'aerodynamically': 1, 'stepstool': 1, 'thong': 1, 'questioningly': 1, 'alon': 1, 'pitched': 1, 'hulahoop': 1, 'muffler': 1, 'discussion': 1, 'foosball': 1, 'developing': 1, 'nation': 1, 'revel': 1, 'encounters': 1, 'treeless': 1, 'steve': 1, 'nash': 1, 'spash': 1, 'italian': 1, 'literature': 1, 'tandom': 1, 'biek': 1, 'crumb': 1, 'padding': 1, 'collies': 1, 'tobaggan': 1, 'involves': 1, 'presentations': 1, 'slideshow': 1, 'sibling': 1, 'rungs': 1, 'raincoats': 1, 'horned': 1, 'wiht': 1, 'backview': 1, 'suffering': 1, 'serena': 1, 'williams': 1, 'pinkish': 1, 'waterful': 1, 'shocks': 1, 'rash': 1, 'grows': 1, 'deeps': 1, 'coarse': 1, 'ratty': 1, 'neptuno': 1, 'peoople': 1, 'stood': 1, 'beachfront': 1, 'dojo': 1, 'ban': 1, 'swaetshirt': 1, 'greens': 1, 'supermarket': 1, 'lettuce': 1, 'planks': 1, 'possessively': 1, 'froup': 1, 'breaching': 1, 'grasslands': 1, 'checkerboard': 1, 'raked': 1, 'longhorns': 1, 'undone': 1, 'upclose': 1, 'swiftly': 1, 'diveboard': 1, 'identification': 1, 'racedog': 1, 'boarders': 1, 'nervous': 1, 'directed': 1, 'discs': 1, 'purina': 1, 'freesbies': 1, 'woodlands': 1, 'culprit': 1, 'tanning': 1, 'wizards': 1, 'perfect': 1, 'barrette': 1, 'pursing': 1, 'suspiciously': 1, 'heeled': 1, 'corkscrew': 1, 'pebbly': 1, 'collapsable': 1, 'borader': 1, 'encourages': 1, 'smacks': 1, 'aggitates': 1, 'sends': 1, 'whil': 1, 'quick': 1, 'skijoring': 1, 'lesh': 1, 'roosters': 1, 'seas': 1, 'neat': 1, 'golfing': 1, 'fairway': 1, 'examined': 1, 'stickball': 1, 'jaws': 1, 'joins': 1, 'conoes': 1, 'coaches': 1, 'floated': 1, 'sabre': 1, 'wars': 1, 'helemt': 1, 'pontoon': 1, 'ally': 1, 'trackside': 1, 'recreation': 1, 'starbuck': 1, 'corgis': 1, 'pound': 1, 'toppless': 1, 'hr': 1, 'headgear': 1, 'liked': 1, 'bodysurfs': 1, 'pinstriped': 1, 'towarn': 1, 'motocycle': 1, 'inch': 1, 'armour': 1, 'stays': 1, 'sub': 1, 'saharan': 1, 'motivation': 1, 'leader': 1, 'eroded': 1, 'poofy': 1, 'overwhelmed': 1, 'onslaught': 1})\n",
            "Words Start: ['a', 'black', 'dog', 'is', 'running', 'after', 'white', 'in', 'the', 'snow', 'chasing', 'brown', 'through', 'two', 'dogs', 'chase', 'each', 'other', 'across', 'snowy', 'ground', 'play', 'together', 'low', 'lying', 'body', 'of', 'water', 'little', 'baby', 'plays', 'girl', 'next', 'to', 'truck', 'child', 'playing', 'by', 'kid', 'front', 'car', 'with', 'put', 'and', 'ball', 'boy', 'beside', 'has', 'something', 'hot', 'pink', 'its', 'mouth', 'holding', 'hat', 'shirt', 'carrying', 'while', 'walking', 'looking', 'forward', 'along', 'beach', 'wearing', 'collar', 'walks', 'on', 'sand', 'near', 'large', 'ocean', 'red', 'frisbee', 'standing', 'sandy', 'disc', 'flying', 'air', 'catching', 'dropping', 'cyclist', 'helmet', 'riding', 'pavement', 'bike', 'street', 'person', 'down', 'woman', 'wears', 'blue', 'as', 'she', 'goes', 'for', 'ride', 'shade', 'road', 'man', 'dressed']\n",
            "Words End: ['tracksuit', 'swans', 'messy', 'buy', 'cries', 'sumo', 'bug', 'insect', 'tricycles', 'theater', 'native', 'cartwheels', 'cone', 'deer', 'badminton', 'wig', 'hooping', 'railings', 'struggle', 'newborn', 'television', 'used', 'seagull', 'flaming', 'ballet', 'accordion', 'priest', 'easter', 'march', 'turkeys', 'hawaiian', 'carts', 'twin', 'michael', 'jackson', 'pierced', 'bouquet', 'cutout', 'barn', 'words', 'sidelines', 'rocket', 'rubs', 'cutting', 'st', 'french', 'potato', 'chip', 'campfire', 'broom', 'yelling', 'target', 'milk', 'tutus', 'robot', 'statues', 'smelling', 'examine', 'participates', 'mountaineer', 'gazing', 'armenian', 'genocide', 'submerged', 'pretend', 'bale', 'pumpkins', 'magazine', 'kangaroo', 'tongues', 'paintball', 'bookstore', 'map', 'evil', 'punching', 'letters', 'library', 'eagle', 'bee', 'mascot', 'sidecar', 'hamburgers', 'hydrant', 'bowling', 'frying', 'seagulls', 'cannon', 'bitten', 'spotlight', 'binoculars', 'hospital', 'buggy', 'rolled', 'sleeve', 'cigarettes', 'bathrobe', 'hell', 'escalator', 'graduation', 'dolphins']\n",
            "word_map: {'a': 1, 'black': 2, 'dog': 3, 'is': 4, 'running': 5, 'after': 6, 'white': 7, 'in': 8, 'the': 9, 'snow': 10, 'chasing': 11, 'brown': 12, 'through': 13, 'two': 14, 'dogs': 15, 'chase': 16, 'each': 17, 'other': 18, 'across': 19, 'snowy': 20, 'ground': 21, 'play': 22, 'together': 23, 'low': 24, 'lying': 25, 'body': 26, 'of': 27, 'water': 28, 'little': 29, 'baby': 30, 'plays': 31, 'girl': 32, 'next': 33, 'to': 34, 'truck': 35, 'child': 36, 'playing': 37, 'by': 38, 'kid': 39, 'front': 40, 'car': 41, 'with': 42, 'put': 43, 'and': 44, 'ball': 45, 'boy': 46, 'beside': 47, 'has': 48, 'something': 49, 'hot': 50, 'pink': 51, 'its': 52, 'mouth': 53, 'holding': 54, 'hat': 55, 'shirt': 56, 'carrying': 57, 'while': 58, 'walking': 59, 'looking': 60, 'forward': 61, 'along': 62, 'beach': 63, 'wearing': 64, 'collar': 65, 'walks': 66, 'on': 67, 'sand': 68, 'near': 69, 'large': 70, 'ocean': 71, 'red': 72, 'frisbee': 73, 'standing': 74, 'sandy': 75, 'disc': 76, 'flying': 77, 'air': 78, 'catching': 79, 'dropping': 80, 'cyclist': 81, 'helmet': 82, 'riding': 83, 'pavement': 84, 'bike': 85, 'street': 86, 'person': 87, 'down': 88, 'woman': 89, 'wears': 90, 'blue': 91, 'as': 92, 'she': 93, 'goes': 94, 'for': 95, 'ride': 96, 'shade': 97, 'road': 98, 'man': 99, 'dressed': 100, 'purple': 101, 'bandanna': 102, 'smiles': 103, 'at': 104, 'people': 105, 'watching': 106, 'him': 107, 'leather': 108, 'poses': 109, 'camera': 110, 'stands': 111, 'onlookers': 112, 'there': 113, 'men': 114, 't': 115, 'woodland': 116, 'runs': 117, 'some': 118, 'trees': 119, 'young': 120, 'dancing': 121, 'around': 122, 'short': 123, 'sleeved': 124, 'jeans': 125, 'stops': 126, 'smile': 127, 'dress': 128, 'back': 129, 'smiling': 130, 'braids': 131, 'looks': 132, 'green': 133, 'skier': 134, 'yellow': 135, 'jacket': 136, 'airborne': 137, 'above': 138, 'mountains': 139, 'jumps': 140, 'high': 141, 'view': 142, 'skiing': 143, 'very': 144, 'it': 145, 'though': 146, 'he': 147, 'doing': 148, 'ski': 149, 'jump': 150, 'pants': 151, 'appears': 152, 'almost': 153, 'fly': 154, 'into': 155, 'sky': 156, 'photographer': 157, 'over': 158, 'hills': 159, 'natural': 160, 'landscape': 161, 'out': 162, 'rolling': 163, 'tripod': 164, 'lady': 165, 'her': 166, 'set': 167, 'up': 168, 'field': 169, 'record': 170, 'bunch': 171, 'girls': 172, 'cheerleader': 173, 'outfits': 174, 'group': 175, 'cheerleaders': 176, 'parade': 177, 'perform': 178, 'many': 179, 'walk': 180, 'uniforms': 181, 'boat': 182, 'canopy': 183, 'floating': 184, 'calm': 185, 'waters': 186, 'roof': 187, 'middle': 188, 'floats': 189, 'lake': 190, 'catches': 191, 'midair': 192, 'terrier': 193, 'leaping': 194, 'object': 195, 'old': 196, 'sitting': 197, 'an': 198, 'advertisement': 199, 'asian': 200, 'waiting': 201, 'train': 202, 'stop': 203, 'sits': 204, 'station': 205, 'backlit': 206, 'subway': 207, 'umbrella': 208, 'wall': 209, 'blond': 210, 'trail': 211, 'side': 212, 'metal': 213, 'fence': 214, 'coat': 215, 'rural': 216, 'flute': 217, 'parka': 218, 'fenced': 219, 'past': 220, 'enclosed': 221, 'area': 222, 'family': 223, 'including': 224, 'four': 225, 'children': 226, 'pose': 227, 'brick': 228, 'fireplace': 229, 'christmas': 230, 'tree': 231, 'posing': 232, 'happy': 233, 'couples': 234, 'kids': 235, 'picture': 236, 'sweater': 237, 'pointing': 238, 'one': 239, 'arms': 240, 'outstretched': 241, 'finger': 242, 'pointed': 243, 'another': 244, 'stares': 245, 'from': 246, 'behind': 247, 'who': 248, 'his': 249, 'look': 250, 'toward': 251, 'points': 252, 'hallway': 253, 'medium': 254, 'sized': 255, 'small': 256, 'larger': 257, 'grassy': 258, 'big': 259, 'tall': 260, 'grass': 261, 'three': 262, 'flowers': 263, 'stuffed': 264, 'kitten': 265, 'garden': 266, 'among': 267, 'toy': 268, 'cat': 269, 'long': 270, 'raises': 271, 'stripes': 272, 'signs': 273, 'african': 274, 'american': 275, 'building': 276, 'space': 277, 'orange': 278, 'backwards': 279, 'wet': 280, 'stick': 281, 'shore': 282, 'bounds': 283, 'splashing': 284, 'off': 285, 'paws': 286, 'carries': 287, 'are': 288, 'racing': 289, 'race': 290, 'track': 291, 'greyhounds': 292, 'muzzles': 293, 'inside': 294, 'curb': 295, 'dirt': 296, 'run': 297, 'baseball': 298, 'pitcher': 299, 'throws': 300, 'player': 301, 'glove': 302, 'male': 303, 'sports': 304, 'outfit': 305, 'tries': 306, 'catch': 307, 'wades': 308, 'rock': 309, 'shallow': 310, 'reach': 311, 'outcropping': 312, 'light': 313, 'paw': 314, 'about': 315, 'ready': 316, 'swim': 317, '5': 318, 'school': 319, 'cross': 320, 'takes': 321, 'outside': 322, 'photograph': 323, 'taking': 324, 'pictures': 325, 'women': 326, 'stand': 327, 'jean': 328, 'hand': 329, 'chest': 330, 'nose': 331, 'writing': 332, 'hands': 333, 'video': 334, 'store': 335, 'denim': 336, 'full': 337, 'house': 338, 'sunglasses': 339, 'flowered': 340, 'sit': 341, 'nearby': 342, 'left': 343, 'adults': 344, 'display': 345, 'case': 346, 'digital': 347, 'glass': 348, 'sides': 349, 'holds': 350, 'sideways': 351, 'arm': 352, 'snowboarder': 353, 'ramp': 354, 'snowboard': 355, 'performing': 356, 'trick': 357, 'making': 358, 'icy': 359, 'mountain': 360, 'climbers': 361, 'line': 362, 'their': 363, 'way': 364, 'covered': 365, 'background': 366, 'facing': 367, 'jumping': 368, 'hoop': 369, 'ring': 370, 'using': 371, 'laptop': 372, 'cellphone': 373, 'against': 374, 'use': 375, 'bench': 376, 'electronic': 377, 'races': 378, 'number': 379, '6': 380, 'greyhound': 381, 'muzzle': 382, 'six': 383, 'tan': 384, 'gold': 385, 'edge': 386, 'cliff': 387, 'rest': 388, 'overlooking': 389, 'top': 390, 'couple': 391, 'wooded': 392, 'them': 393, 'lined': 394, 'friends': 395, 'forest': 396, 'path': 397, 'woods': 398, 'sponsored': 399, 'smoking': 400, 'tires': 401, 'drives': 402, 'rain': 403, 'driving': 404, 'advertising': 405, 'lights': 406, 'bearded': 407, 'whilst': 408, 'bicycle': 409, 'bicycles': 410, 'hair': 411, 'gives': 412, 'peace': 413, 'sign': 414, 'cap': 415, 'crowd': 416, 'waving': 417, 'flags': 418, 'passing': 419, 'under': 420, 'bridge': 421, 'or': 422, 'banners': 423, 'gets': 424, 'pull': 425, 'chair': 426, 'meal': 427, 'table': 428, 'cafe': 429, 'restaurant': 430, 'getting': 431, 'grabs': 432, 'seat': 433, 'racket': 434, 'round': 435, 'handle': 436, 'backyard': 437, 'lawn': 438, 'clothing': 439, 'yard': 440, 'toys': 441, 'enjoy': 442, 'themselves': 443, 'wind': 444, 'blows': 445, 'trampoline': 446, 'blonde': 447, 'bicyclists': 448, 'between': 449, 'wire': 450, 'cyclists': 451, 'country': 452, 'desert': 453, 'bikers': 454, 'dry': 455, 'land': 456, 'climbing': 457, 'steep': 458, 'hill': 459, 'climbs': 460, 'great': 461, 'spins': 462, 'merry': 463, 'go': 464, 'watch': 465, 'spinning': 466, 'playground': 467, 'that': 468, 'skirt': 469, 'golden': 470, 'retriever': 471, 'towards': 472, 'away': 473, 'bus': 474, 'door': 475, 'vehicle': 476, 'window': 477, 'passengers': 478, 'hiding': 479, 'peeking': 480, 'striped': 481, 'peeks': 482, 'hold': 483, 'drinks': 484, 'point': 485, 'bottles': 486, 'several': 487, 'this': 488, 'photo': 489, 'border': 490, 'collie': 491, 'bed': 492, 'tennis': 493, 'climber': 494, 'peak': 495, 'rocks': 496, 'snowcapped': 497, 'hikes': 498, 'following': 499, 'surrounded': 500, 'distance': 501, 'smoke': 502, 'comes': 503, 'starts': 504, 'broken': 505, 'during': 506, 'racetrack': 507, 'bull': 508, 'leaves': 509, 'bags': 510, 'cave': 511, 'luggage': 512, 'backpacks': 513, 'ice': 514, 'visible': 515, 'hole': 516, 'starting': 517, 'steps': 518, 'murky': 519, 'putting': 520, 'legs': 521, 'pond': 522, 'busy': 523, 'city': 524, 'silhouette': 525, 'buildings': 526, 'crowded': 527, 'shot': 528, 'turned': 529, 'chases': 530, 'interacting': 531, 'opposite': 532, 'direction': 533, 'fetch': 534, 'also': 535, 'outdoors': 536, 'cameras': 537, 'third': 538, 'river': 539, 'skis': 540, 'backdrop': 541, 'face': 542, 'rope': 543, 'swimming': 544, 'creek': 545, 'playfully': 546, 'rolls': 547, 'int': 548, 'chewing': 549, 'resting': 550, 'game': 551, 'elderly': 552, 'straw': 553, 'alone': 554, 'park': 555, 'haired': 556, 'gray': 557, 'dark': 558, 'beard': 559, 'guitar': 560, 'head': 561, 'ceiling': 562, 'wide': 563, 'drink': 564, 'upward': 565, 'reading': 566, 'book': 567, 'scarf': 568, 'older': 569, 'reads': 570, 'motocross': 571, 'motorcycle': 572, 'tight': 573, 'turn': 574, 'motorbike': 575, 'bright': 576, 'someone': 577, 'fall': 578, 'rider': 579, 'sleeping': 580, 'makeshift': 581, 'hanging': 582, 'mountainside': 583, 'skateboarder': 584, 'skateboard': 585, 'landing': 586, 'teenage': 587, 'flipping': 588, 'tricks': 589, 'colored': 590, 'protest': 591, 'banner': 592, 'staring': 593, 'religious': 594, 'sidewalk': 595, 'poster': 596, 'image': 597, 'teddy': 598, 'bear': 599, 'showing': 600, 'stool': 601, 'trying': 602, 'sell': 603, 'animals': 604, 'selling': 605, 'various': 606, 'vendor': 607, 's': 608, 'inflatable': 609, 'guy': 610, 'navy': 611, 'shorts': 612, 'pool': 613, 'football': 614, 'trunks': 615, 'strange': 616, 'diving': 617, 'plastic': 618, 'slide': 619, 'equipment': 620, 'fire': 621, 'day': 622, 'juggling': 623, 'flames': 624, 'bubbles': 625, 'float': 626, 'popping': 627, 'right': 628, 'shoes': 629, 'sandals': 630, 'tug': 631, 'war': 632, 'pulling': 633, 'end': 634, 'rottweiler': 635, 'surf': 636, 'wave': 637, 'surfers': 638, 'surfing': 639, 'attempting': 640, 'sea': 641, 'huge': 642, 'sunset': 643, 'rides': 644, 'biking': 645, 'muddy': 646, 'slope': 647, 'crossing': 648, 'heads': 649, 'night': 650, 'phone': 651, 'talks': 652, 'pile': 653, 'rail': 654, 'stunt': 655, 'soccer': 656, 'uniform': 657, 'kick': 658, 'match': 659, 'watches': 660, 'knee': 661, 'female': 662, 'break': 663, 'driver': 664, 'wheel': 665, 'thumbs': 666, 'before': 667, 'polka': 668, 'boots': 669, 'purse': 670, 'followed': 671, 'redheaded': 672, 'pedestrians': 673, 'barefoot': 674, 'cute': 675, 'puppy': 676, 'fetches': 677, 'chew': 678, 'fluffy': 679, 'rings': 680, 'dock': 681, 'laughing': 682, 'bucket': 683, 'splashes': 684, 'pier': 685, 'beneath': 686, 'dances': 687, 'room': 688, 'balloons': 689, 'floor': 690, 'socks': 691, 'wooden': 692, 'strewn': 693, 'strings': 694, 'confetti': 695, 'wood': 696, 'mother': 697, 'both': 698, 'ledge': 699, 'scales': 700, 'tent': 701, 'being': 702, 'enter': 703, 'fishing': 704, 'setting': 705, 'hut': 706, 'tarp': 707, 'structure': 708, 'surface': 709, 'few': 710, 'foam': 711, 'statue': 712, 'hats': 713, 'take': 714, 'photos': 715, 'new': 716, 'get': 717, 'taken': 718, 'fight': 719, 'jersey': 720, 'leaps': 721, 'dives': 722, 'going': 723, 'sheer': 724, 'climb': 725, 'pretty': 726, 'flat': 727, 'rocky': 728, 'card': 729, 'what': 730, 'says': 731, 'they': 732, 'bathing': 733, 'suit': 734, 'sprinklers': 735, 'sprinkler': 736, 'sliding': 737, 'paddling': 738, 'kiddie': 739, 'lone': 740, 'flies': 741, 'pulled': 742, 'shovel': 743, 'distorted': 744, 'open': 745, 'ends': 746, 'bottom': 747, 'just': 748, 'reaching': 749, 'lands': 750, 'reaches': 751, 'piano': 752, 'sings': 753, 'preparing': 754, 'sing': 755, 'singer': 756, 'music': 757, 'teeth': 758, 'tongue': 759, 'curly': 760, 'violin': 761, 'walls': 762, 'posters': 763, 'chin': 764, 'be': 765, 'cover': 766, 'teenager': 767, 'struggles': 768, 'carry': 769, 'piggy': 770, 'paved': 771, 'covering': 772, 'facial': 773, 'style': 774, 'headscarf': 775, 'grey': 776, 'bird': 777, 'waves': 778, 'roll': 779, 'crane': 780, 'onto': 781, 'pole': 782, 'swings': 783, 'silver': 784, 'swinging': 785, 'bounces': 786, 'leotard': 787, 'hello': 788, 'kitty': 789, 'does': 790, 'leg': 791, 'splits': 792, 'leans': 793, 'board': 794, 'platform': 795, 'skateboarders': 796, 'eating': 797, 'seeds': 798, 'eats': 799, 'stone': 800, 'friend': 801, 'helps': 802, 'help': 803, 'how': 804, 'helping': 805, 'sweatshirt': 806, 'not': 807, 'far': 808, 'racer': 809, 'slightly': 810, 'competition': 811, 'circuit': 812, 'cheering': 813, 'grabbing': 814, 'ankle': 815, 'oklahoma': 816, 'score': 817, 'fans': 818, 'cheer': 819, 'athlete': 820, 'performs': 821, 'herself': 822, 'bar': 823, 'upside': 824, 'attempts': 825, 'feet': 826, 'do': 827, 'flip': 828, 'blocks': 829, 'concrete': 830, 'carefully': 831, 'crosses': 832, 'reflection': 833, 'caught': 834, 'pajamas': 835, 'hall': 836, 'hardwood': 837, 'lit': 838, 'toddler': 839, 'chairs': 840, 'folding': 841, 'row': 842, 'talking': 843, 'spotted': 844, 'fast': 845, 'seated': 846, 'stadium': 847, 'event': 848, 'packed': 849, 'indoor': 850, 'duck': 851, 'pet': 852, 'chased': 853, 'parachute': 854, 'lifting': 855, 'attached': 856, 'parasail': 857, 'suspended': 858, 'silhouetted': 859, 'sunny': 860, 'gliding': 861, 'sun': 862, 'skateboarding': 863, 'slides': 864, 'railing': 865, 'shines': 866, 'skating': 867, 'stair': 868, 'stairs': 869, 'have': 870, 'pillow': 871, 'having': 872, 'skates': 873, 'neck': 874, 'flight': 875, 'boats': 876, 'boxing': 877, 'boxers': 878, 'fighting': 879, 'box': 880, 'faces': 881, 'funny': 882, 'makes': 883, 'glasses': 884, 'rusty': 885, 'barks': 886, 'barking': 887, 'bark': 888, 'hose': 889, 'pouring': 890, 'drinking': 891, 'which': 892, 'squirted': 893, 'wings': 894, 'swimsuits': 895, 'lacrosse': 896, 'players': 897, 'control': 898, 'team': 899, 'breaks': 900, 'hit': 901, 'hockey': 902, 'type': 903, 'sport': 904, 'like': 905, 'sticks': 906, 'guys': 907, 'all': 908, 'saying': 909, 'free': 910, 'dinner': 911, 'cement': 912, 'skate': 913, 'prepares': 914, 'hugging': 915, 'embracing': 916, 'hugs': 917, 'basketball': 918, 'make': 919, 'basket': 920, 'goal': 921, 'dunking': 922, 'grinding': 923, 'snowboarding': 924, 'homemade': 925, 'made': 926, 'piece': 927, 'log': 928, 'mid': 929, 'leap': 930, 'clear': 931, 'wrapped': 932, 'tape': 933, 'brother': 934, 'superman': 935, 'cape': 936, 'cast': 937, 'material': 938, 'airplane': 939, 'plane': 940, 'jet': 941, 'seen': 942, 'windows': 943, 'headfirst': 944, 'digging': 945, 'brush': 946, 'digs': 947, 'greenhouse': 948, 'tools': 949, 'work': 950, 'fishes': 951, 'rod': 952, 'boys': 953, 'living': 954, 'shown': 955, 'wear': 956, 'shirts': 957, 'computer': 958, 'eat': 959, 'cream': 960, 'adult': 961, 'sat': 962, 'world': 963, 'college': 964, 'tackled': 965, 'jerseys': 966, 'tackle': 967, 'tackling': 968, 'uniformed': 969, 'try': 970, 'fan': 971, 'painting': 972, 'paint': 973, 'within': 974, 'painted': 975, 'bleachers': 976, '3': 977, 'spots': 978, 'palm': 979, 'stretch': 980, 'framed': 981, 'sale': 982, 'fallen': 983, 'dead': 984, 'church': 985, 'bicyclist': 986, 'aerial': 987, 'scooter': 988, 'suburban': 989, 'neighborhood': 990, 'center': 991, 'rollerblades': 992, 'rollerblader': 993, 'narrow': 994, 'roller': 995, 'grinds': 996, 'rollerblading': 997, 'participate': 998, 'martial': 999, 'arts': 1000, 'tournament': 1001, 'mat': 1002, 'protective': 1003, 'gear': 1004, 'helmets': 1005, 'sparring': 1006, 'skateboards': 1007, 'skater': 1008, 'flag': 1009, 'headband': 1010, 'vest': 1011, 'indoors': 1012, 'handrail': 1013, 'foot': 1014, 'indian': 1015, 'crossed': 1016, 'folded': 1017, 'outdoor': 1018, 'market': 1019, 'shining': 1020, 'graffiti': 1021, 'plank': 1022, 'no': 1023, 'gallery': 1024, 'naked': 1025, 'individuals': 1026, 'chinese': 1027, 'ladies': 1028, 'bank': 1029, 'lockers': 1030, 'retrieves': 1031, 'seaweed': 1032, 'coming': 1033, 'mossy': 1034, 'hiker': 1035, 'descends': 1036, 'hiking': 1037, 'leafy': 1038, 'gravel': 1039, 'amidst': 1040, 'autumn': 1041, 'others': 1042, 'laugh': 1043, 'touching': 1044, 'laughs': 1045, 'fun': 1046, 'splash': 1047, 'wading': 1048, 'bald': 1049, 'drag': 1050, 'dresses': 1051, 'matching': 1052, 'jewelry': 1053, 'clothes': 1054, 'formally': 1055, 'jumped': 1056, 'somthing': 1057, 'walkway': 1058, 'biker': 1059, 'moving': 1060, 'furry': 1061, 'doorway': 1062, 'leading': 1063, 'plants': 1064, 'patio': 1065, 'potted': 1066, 'hairy': 1067, 'step': 1068, 'cars': 1069, 'start': 1070, 'beginning': 1071, 'see': 1072, 'racers': 1073, 'sled': 1074, 'harness': 1075, 'pack': 1076, 'winter': 1077, 'soaking': 1078, 'wheelie': 1079, 'terrain': 1080, 'turquoise': 1081, 'guard': 1082, 'thin': 1083, 'markings': 1084, 'ears': 1085, 'throwing': 1086, 'tossing': 1087, 'touches': 1088, 'challenging': 1089, 'teams': 1090, 'quickly': 1091, 'meadow': 1092, 'surfer': 1093, 'crashing': 1094, 'follows': 1095, 'surfboard': 1096, 'laying': 1097, 'power': 1098, 'lines': 1099, 'unusual': 1100, 'shaped': 1101, 'stump': 1102, 'referee': 1103, 'breaking': 1104, 'artist': 1105, 'canvas': 1106, 'overalls': 1107, 'travels': 1108, 'heavily': 1109, 'german': 1110, 'shepherd': 1111, 'leash': 1112, 'leashed': 1113, 'falling': 1114, 'backward': 1115, 'urban': 1116, 'partially': 1117, 'guitars': 1118, 'suits': 1119, 'electric': 1120, 'musicians': 1121, 'curtain': 1122, 'frolics': 1123, 'mini': 1124, 'heels': 1125, 'lap': 1126, 'steers': 1127, 'suv': 1128, 'jeep': 1129, 'cords': 1130, 'flock': 1131, 'birds': 1132, 'mural': 1133, 'pigeons': 1134, 'bride': 1135, 'held': 1136, 'polo': 1137, 'hides': 1138, 'bushes': 1139, 'cushion': 1140, 'spider': 1141, 'patterned': 1142, 'base': 1143, 'close': 1144, 'dirty': 1145, 'crashes': 1146, 'beyond': 1147, 'relaxing': 1148, 'trench': 1149, 'waits': 1150, 'sculpture': 1151, 'bikini': 1152, 'pulls': 1153, 'skaters': 1154, 'rink': 1155, 'scenery': 1156, 'bounce': 1157, 'filled': 1158, 'bouncy': 1159, 'castle': 1160, 'swimsuit': 1161, 'tightrope': 1162, 'students': 1163, 'quad': 1164, 'but': 1165, 'falls': 1166, 'practice': 1167, 'asking': 1168, 'daughter': 1169, 'nice': 1170, 'pretending': 1171, 'biting': 1172, 'limb': 1173, 'branch': 1174, 'gnawing': 1175, 'plant': 1176, 'uncut': 1177, 'barrel': 1178, 'buckets': 1179, 'poles': 1180, 'seven': 1181, 'teenagers': 1182, 'wait': 1183, 'gate': 1184, 'puts': 1185, 'raised': 1186, 'newspaper': 1187, 'kitchen': 1188, 'owner': 1189, 'retrieving': 1190, 'paper': 1191, 'tile': 1192, 'sharing': 1193, 'kiss': 1194, 'cold': 1195, 'kissing': 1196, 'passes': 1197, 'teen': 1198, 'coats': 1199, 'tunnel': 1200, 'halloween': 1201, 'eyes': 1202, 'crawls': 1203, 'beautiful': 1204, 'tube': 1205, 'leads': 1206, 'trucks': 1207, 'five': 1208, 'obstacle': 1209, 'course': 1210, 'elephant': 1211, 'fabric': 1212, 'trunk': 1213, 'colorful': 1214, 'foliage': 1215, 'swimmers': 1216, 'move': 1217, 'mud': 1218, 'puddle': 1219, 'identical': 1220, 'shirtless': 1221, 'couch': 1222, 'disk': 1223, 'show': 1224, 'wheeled': 1225, 'tricycle': 1226, 'i': 1227, 'wheeler': 1228, '2': 1229, 'wheels': 1230, 'carnival': 1231, 'worker': 1232, 'animal': 1233, 'amongst': 1234, 'camping': 1235, 'patches': 1236, 'ditch': 1237, 'dribbles': 1238, 'moves': 1239, 'coffee': 1240, 'gather': 1241, 'home': 1242, 'outstreached': 1243, 'begins': 1244, 'plaid': 1245, 'print': 1246, 'rescue': 1247, 'follow': 1248, 'lead': 1249, 'flips': 1250, 'canoe': 1251, 'still': 1252, 'fat': 1253, 'traveling': 1254, 'stepping': 1255, 'shaggy': 1256, 'foreground': 1257, 'inline': 1258, 'public': 1259, 'dust': 1260, 'sooners': 1261, 'lifted': 1262, 'same': 1263, 'athletes': 1264, 'waterfall': 1265, 'waterfalls': 1266, 'guns': 1267, 'shooting': 1268, 'beige': 1269, 'athletic': 1270, 'coach': 1271, 'whistle': 1272, 'beagle': 1273, 'hound': 1274, 'costumes': 1275, 'frame': 1276, 'unseen': 1277, 'multi': 1278, 'excited': 1279, 'audience': 1280, 'parallel': 1281, 'kicks': 1282, 'opponent': 1283, 'compete': 1284, 'karate': 1285, 'clapping': 1286, 'ran': 1287, 'lift': 1288, 'skiers': 1289, 'shakes': 1290, 'shaking': 1291, 'hoodie': 1292, 'horse': 1293, 'jockey': 1294, 'performer': 1295, 'stage': 1296, 'spectators': 1297, 'onstage': 1298, 'presentation': 1299, 'aged': 1300, 'dj': 1301, 'deck': 1302, 'younger': 1303, 'tables': 1304, 'meet': 1305, 'goggles': 1306, 'swimmer': 1307, 'apple': 1308, 'place': 1309, 'bag': 1310, 'backpack': 1311, 'tattoo': 1312, 'clown': 1313, 'practicing': 1314, 'craft': 1315, 'clearing': 1316, 'rough': 1317, 'collars': 1318, 'extreme': 1319, 'backpacker': 1320, 'uses': 1321, 'camcorder': 1322, 'tosses': 1323, 'empty': 1324, 'bottle': 1325, 'volleyball': 1326, 'talk': 1327, 'bikinis': 1328, 'marketplace': 1329, 'ascending': 1330, 'horizontal': 1331, 'shephard': 1332, 'pair': 1333, 'spray': 1334, 'fountain': 1335, 'splashed': 1336, 'sprayed': 1337, 'closeup': 1338, 'screen': 1339, 'life': 1340, 'unicycle': 1341, 'lining': 1342, 'location': 1343, 'ferry': 1344, 'shoreline': 1345, 'dolphin': 1346, 'pokes': 1347, 'shoe': 1348, 'corner': 1349, 'shoulders': 1350, 'these': 1351, 'noses': 1352, 'numbers': 1353, 'dune': 1354, 'watercraft': 1355, 'swing': 1356, 'gestures': 1357, 'closed': 1358, 'poodle': 1359, 'sticking': 1360, 'butt': 1361, 'tattooed': 1362, 'gentleman': 1363, 'half': 1364, 'relaxes': 1365, 'where': 1366, 'parked': 1367, 'topless': 1368, 'snowbank': 1369, 'kneeling': 1370, 'ridge': 1371, 'pass': 1372, 'sleeveless': 1373, 'floppy': 1374, 'cigarette': 1375, 'blanket': 1376, 'hooded': 1377, 'pipe': 1378, 'bubble': 1379, 'machine': 1380, 'bites': 1381, 'blowing': 1382, 'swims': 1383, 'chunk': 1384, 'throw': 1385, 'snowball': 1386, 'grins': 1387, 'paddles': 1388, 'vertical': 1389, 'approaching': 1390, 'backs': 1391, 'tank': 1392, 'official': 1393, 'marsh': 1394, 'mostly': 1395, 'swampy': 1396, 'rubber': 1397, 'chickens': 1398, 'chicken': 1399, 'eight': 1400, 'balls': 1401, 'bottoms': 1402, 'pushes': 1403, 'stroller': 1404, 'brunette': 1405, 'pushing': 1406, 'carriage': 1407, 'females': 1408, 'hurdle': 1409, 'lay': 1410, 'hang': 1411, 'snowman': 1412, 'flower': 1413, 'figure': 1414, 'costume': 1415, 'party': 1416, 'branches': 1417, 'bends': 1418, 'alley': 1419, 'cowboy': 1420, 'lasso': 1421, 'twirling': 1422, 'cart': 1423, '(': 1424, 'carrier': 1425, 'rack': 1426, 'shopping': 1427, 'apart': 1428, 'camouflage': 1429, 'blow': 1430, 'sledding': 1431, 'snowsuit': 1432, 'town': 1433, 'leashes': 1434, 'strap': 1435, 'bite': 1436, 'strip': 1437, 'steering': 1438, 'ship': 1439, 'cloudy': 1440, 'skies': 1441, 'sailboat': 1442, 'van': 1443, 'rv': 1444, 'snake': 1445, 'lean': 1446, 'different': 1447, 'tub': 1448, 'bin': 1449, 'multicolored': 1450, 'pen': 1451, 'enclosure': 1452, 'pit': 1453, 'cut': 1454, 'balloon': 1455, 'only': 1456, 'underwear': 1457, 'marker': 1458, 'drawing': 1459, 'safety': 1460, 'neon': 1461, 'ribbons': 1462, 'competing': 1463, 'covers': 1464, 'vests': 1465, 'you': 1466, 'hi': 1467, 'viz': 1468, 'jackets': 1469, 'returns': 1470, 'horses': 1471, 'pony': 1472, 'stare': 1473, 'shop': 1474, 'mall': 1475, 'food': 1476, 'court': 1477, 'art': 1478, 'turns': 1479, 'ladder': 1480, 'headed': 1481, 'frozen': 1482, 'treat': 1483, 'eyed': 1484, 'ahead': 1485, 'ropes': 1486, 'harnesses': 1487, 'balance': 1488, 'dangling': 1489, 'straps': 1490, 'kicking': 1491, 'miami': 1492, 'university': 1493, 'scene': 1494, 'classic': 1495, 'well': 1496, 'thrown': 1497, 'dancers': 1498, 'dance': 1499, 'prepare': 1500, 'hillside': 1501, 'kayaker': 1502, 'kayaking': 1503, 'kayak': 1504, 'spot': 1505, 'warmly': 1506, 'blurry': 1507, 'tag': 1508, 'touch': 1509, 'riders': 1510, 'shoulder': 1511, 'motorbikes': 1512, 'farm': 1513, 'sheep': 1514, 'goat': 1515, 'twisting': 1516, 'graffitied': 1517, 'stars': 1518, 'bridal': 1519, 'wedding': 1520, 'ten': 1521, 'groom': 1522, 'except': 1523, 'bridesmaids': 1524, 'lays': 1525, 'rests': 1526, 'stream': 1527, 'eye': 1528, 'bush': 1529, 'photographs': 1530, 'canal': 1531, 'muzzled': 1532, '#': 1533, 'numbered': 1534, 'police': 1535, 'officer': 1536, 'cop': 1537, 'fake': 1538, 'mask': 1539, 'tie': 1540, 'handing': 1541, 'papers': 1542, 'button': 1543, 'business': 1544, 'attire': 1545, 'straight': 1546, 'profile': 1547, 'blurred': 1548, 'bicycler': 1549, 'pop': 1550, 'weather': 1551, 'happily': 1552, 'candles': 1553, 'lighting': 1554, 'ridden': 1555, 'colors': 1556, 'eastern': 1557, 'give': 1558, 'chubby': 1559, 'peers': 1560, 'device': 1561, 'pad': 1562, 'writes': 1563, 'motorcyclist': 1564, 'speed': 1565, 'sharp': 1566, 'crouches': 1567, 'bmx': 1568, 'without': 1569, 'boarding': 1570, 'trotting': 1571, 'stuck': 1572, 'trails': 1573, 'pine': 1574, '4': 1575, 'atv': 1576, 'drive': 1577, 'net': 1578, 'logo': 1579, 'skimpy': 1580, 'soaring': 1581, 'underneath': 1582, 'batting': 1583, 'sweaters': 1584, 'wade': 1585, 'military': 1586, 'speak': 1587, 'crying': 1588, 'bending': 1589, 'checking': 1590, 'owners': 1591, 'hits': 1592, 'bat': 1593, 'catcher': 1594, 'teal': 1595, 'japanese': 1596, 'aqua': 1597, 'handstand': 1598, 'lab': 1599, 'below': 1600, 'headphones': 1601, 'elevator': 1602, 'kites': 1603, 'range': 1604, 'camel': 1605, 'camels': 1606, 'trots': 1607, 'agility': 1608, 'lifts': 1609, 'bare': 1610, 'launches': 1611, 'asleep': 1612, 'machines': 1613, 'money': 1614, 'sleeps': 1615, 'maroon': 1616, 'dalmation': 1617, 'chews': 1618, 'hind': 1619, 'counter': 1620, 'handbag': 1621, 'grocery': 1622, 'corn': 1623, 'cob': 1624, 'ear': 1625, 'listens': 1626, 'mobile': 1627, 'come': 1628, 'sets': 1629, 'horizon': 1630, 'cricket': 1631, 'participating': 1632, 'opposing': 1633, 'pick': 1634, 'watermelon': 1635, 'picking': 1636, 'watermelons': 1637, 'flowery': 1638, 'stretches': 1639, 'circle': 1640, 'brightly': 1641, 'teaches': 1642, 'festival': 1643, 'stretching': 1644, 'legged': 1645, 'plain': 1646, 'puppies': 1647, 'grab': 1648, 'share': 1649, 'alongside': 1650, 'sister': 1651, 'mickey': 1652, 'mouse': 1653, 'professional': 1654, 'plate': 1655, 'guarding': 1656, 'runner': 1657, 'softball': 1658, 'block': 1659, 'khaki': 1660, 'pours': 1661, 'wine': 1662, 'dim': 1663, 'belongings': 1664, 'time': 1665, 'lot': 1666, 'choppy': 1667, 'wetsuit': 1668, 'rapids': 1669, 'rafting': 1670, 'leaving': 1671, 'single': 1672, 'exhibit': 1673, 'class': 1674, 'porch': 1675, 'clad': 1676, 'scantily': 1677, 'dish': 1678, 'towel': 1679, 'cloth': 1680, 'boxer': 1681, 'pitbull': 1682, 'uphill': 1683, 'watched': 1684, 'sneakers': 1685, 'highway': 1686, 'item': 1687, 'ribbon': 1688, 'horns': 1689, 'goats': 1690, 'earphones': 1691, 'surprised': 1692, 'foggy': 1693, 'strollers': 1694, 'pushed': 1695, 'first': 1696, 'bikes': 1697, 'fist': 1698, 'gloves': 1699, 'giant': 1700, 'circular': 1701, 'perched': 1702, 'opening': 1703, 'atop': 1704, 'kneels': 1705, 'sprays': 1706, 'push': 1707, 'policeman': 1708, 'crosswalk': 1709, 'traffic': 1710, 'chainsaw': 1711, 'carving': 1712, 'reflective': 1713, 'racquet': 1714, 'parking': 1715, 'printed': 1716, 'attempt': 1717, 'goalie': 1718, 'dreadlocks': 1719, 'bowls': 1720, 'liquid': 1721, 'mug': 1722, 'beer': 1723, 'lunges': 1724, 'balcony': 1725, 'bouncing': 1726, 'hopping': 1727, 'magazines': 1728, 'fruit': 1729, 'fish': 1730, 'paddle': 1731, 'father': 1732, 'tossed': 1733, 'monkey': 1734, 'bars': 1735, 'jungle': 1736, 'gym': 1737, 'mess': 1738, 'skiiers': 1739, 'dachshund': 1740, 'cones': 1741, 'pads': 1742, 'really': 1743, 'wand': 1744, 'speaks': 1745, 'cup': 1746, 'birthday': 1747, 'celebrate': 1748, 'cake': 1749, 'seating': 1750, 'parents': 1751, 'skull': 1752, 'motorcycles': 1753, 'motorcyclists': 1754, 'interesting': 1755, 'formations': 1756, 'licking': 1757, 'sniffing': 1758, 'desk': 1759, 'meeting': 1760, 'decorated': 1761, 'gathered': 1762, 'clouds': 1763, 'lots': 1764, 'raising': 1765, 'parachuting': 1766, 'ponytail': 1767, 'lips': 1768, 'tool': 1769, 'rug': 1770, 'carpet': 1771, 'checkered': 1772, 'tiger': 1773, 'picnic': 1774, 'hike': 1775, 'barren': 1776, 'bend': 1777, 'gymnastics': 1778, 'seashore': 1779, 'shows': 1780, 'fur': 1781, 'balances': 1782, 'staircase': 1783, 'scaling': 1784, 'repels': 1785, 'studio': 1786, 'tops': 1787, 'clothed': 1788, 'soars': 1789, 'slopes': 1790, 'surrounding': 1791, 'smaller': 1792, 'tail': 1793, 'sniffs': 1794, 'batman': 1795, 'makeup': 1796, 'giving': 1797, 'celebrating': 1798, 'beret': 1799, 'mustache': 1800, 'conversation': 1801, 'joy': 1802, 'chain': 1803, 'transportation': 1804, 'camo': 1805, 'photographed': 1806, 'sort': 1807, 'darkened': 1808, 'bowl': 1809, 'hikers': 1810, 'backpacking': 1811, 'santa': 1812, 'sofa': 1813, 'claus': 1814, 'mirror': 1815, 'surfs': 1816, 'leaning': 1817, 'stairway': 1818, 'garbage': 1819, 'can': 1820, 'trash': 1821, 'cans': 1822, 'trashcan': 1823, 'cardboard': 1824, 'boxes': 1825, 'instrument': 1826, 'musician': 1827, 'tire': 1828, 'rocking': 1829, 'deep': 1830, 'overhang': 1831, 'motion': 1832, 'storefront': 1833, 'entrance': 1834, 'jagged': 1835, 'peach': 1836, 'enjoys': 1837, 'stomach': 1838, 'wakeboard': 1839, 'read': 1840, 'pacifier': 1841, 'sucking': 1842, 'soldier': 1843, 'shaved': 1844, 'boulder': 1845, 'approaches': 1846, 'shower': 1847, 'bounding': 1848, 'museum': 1849, 'zip': 1850, 'cable': 1851, 'arena': 1852, 'jumpsuit': 1853, 'bent': 1854, 'dunes': 1855, 'jockeys': 1856, 'wrestle': 1857, 'warm': 1858, 'intersection': 1859, 'beanie': 1860, 'blocked': 1861, 'members': 1862, 'fetching': 1863, 'teammate': 1864, 'quarterback': 1865, 'summit': 1866, 'rails': 1867, 'robes': 1868, 'tattoos': 1869, 'bow': 1870, 'picks': 1871, 'airport': 1872, 'overhead': 1873, 'amusement': 1874, 'arcade': 1875, 'blindfolded': 1876, 'o': 1877, 'written': 1878, 'burning': 1879, 'handles': 1880, 'wheelbarrow': 1881, 'whose': 1882, 'been': 1883, 'turning': 1884, 'tiny': 1885, 'cow': 1886, 'necklace': 1887, 'cliffs': 1888, 'band': 1889, 'dollar': 1890, 'instruments': 1891, 'underwater': 1892, 'fingers': 1893, 'chocolate': 1894, 'act': 1895, 'silly': 1896, 'gas': 1897, 'curve': 1898, 'placed': 1899, 'union': 1900, 'jack': 1901, 'backstroke': 1902, 'bunny': 1903, 'hurdles': 1904, 'tri': 1905, 'tents': 1906, 'barrier': 1907, 'fuzzy': 1908, 'creature': 1909, 'cups': 1910, 'marathon': 1911, 'runners': 1912, 'lane': 1913, 'rag': 1914, 'dried': 1915, 'containing': 1916, 'cage': 1917, 'crawling': 1918, 'pieces': 1919, 'gymnast': 1920, 'gymnasium': 1921, 'infant': 1922, 'squatting': 1923, 'beverage': 1924, 'overweight': 1925, 'yawning': 1926, 'homeless': 1927, 'sheet': 1928, 'serve': 1929, 'portrait': 1930, 'action': 1931, 'boardwalk': 1932, 'too': 1933, 'village': 1934, 'masks': 1935, 'embrace': 1936, 'garb': 1937, 'enjoying': 1938, 'downhill': 1939, 'cats': 1940, 'pug': 1941, 'mohawk': 1942, 'drums': 1943, 'feathers': 1944, 'drum': 1945, 'tussle': 1946, 'deflated': 1947, 'string': 1948, 'traditional': 1949, 'elaborate': 1950, 'butterfly': 1951, 'order': 1952, 'posed': 1953, 'stretched': 1954, 'jogging': 1955, 'pitch': 1956, 'rugby': 1957, 'hula': 1958, 'hoops': 1959, 'maneuvers': 1960, 'link': 1961, 'nap': 1962, 'tutu': 1963, 'waterskiing': 1964, 'waterskier': 1965, 'colourful': 1966, 'snowboards': 1967, 'snowmobile': 1968, 'blocking': 1969, 'army': 1970, 'fairy': 1971, 'raft': 1972, 'innertube': 1973, 'monument': 1974, 'mountaintop': 1975, 'pyramid': 1976, 'directions': 1977, 'mouths': 1978, 'lies': 1979, 'coaster': 1980, 'flipped': 1981, 'multiple': 1982, 'motorized': 1983, 'wheelchair': 1984, 'shops': 1985, 'distant': 1986, 'things': 1987, 'floral': 1988, 'wagon': 1989, 'umbrellas': 1990, 'beads': 1991, 'pigtails': 1992, 'heart': 1993, 'fancy': 1994, 'robe': 1995, 'bagpipe': 1996, 'medieval': 1997, 'similar': 1998, 'trumpet': 1999, 'marching': 2000, 'fog': 2001, 'balancing': 2002, 'tractor': 2003, 'squirrel': 2004, 'melting': 2005, 'mound': 2006, 'heavy': 2007, 'rowboat': 2008, 'rowing': 2009, 'boards': 2010, 'boogie': 2011, 'ninja': 2012, 'security': 2013, 'series': 2014, 'beam': 2015, 'snowboarders': 2016, 'dancer': 2017, 'batter': 2018, 'dusk': 2019, 'wakeboarder': 2020, 'emerges': 2021, 'dandelion': 2022, 'toe': 2023, 'spread': 2024, 'tourists': 2025, 'injured': 2026, 'color': 2027, 'fireworks': 2028, 'sparklers': 2029, 'kisses': 2030, 'scarves': 2031, 'cows': 2032, 'herding': 2033, 'skeleton': 2034, 'sail': 2035, 'thumb': 2036, 'ticket': 2037, 'dimly': 2038, 'singing': 2039, 'club': 2040, 'guitarist': 2041, 'microphone': 2042, 'listening': 2043, 'chopsticks': 2044, 'pot': 2045, 'pan': 2046, 'items': 2047, 'kilt': 2048, 'own': 2049, 'soda': 2050, 'sunlight': 2051, 'patch': 2052, 'shades': 2053, 'shadow': 2054, 'post': 2055, 'fellow': 2056, 'formal': 2057, 'skinned': 2058, 'filling': 2059, 'watery': 2060, 'canoeing': 2061, 'casting': 2062, 'playful': 2063, 'construction': 2064, 'works': 2065, 'breed': 2066, 'smooth': 2067, 'mans': 2068, 'handed': 2069, 'so': 2070, 'waterway': 2071, 'crowds': 2072, 'complete': 2073, 'finish': 2074, 'benches': 2075, 'checked': 2076, 'descending': 2077, 'knees': 2078, 'crouching': 2079, 'position': 2080, 'stripped': 2081, 'skips': 2082, 'located': 2083, 'midst': 2084, 'part': 2085, 'overlooks': 2086, 'valley': 2087, 'forested': 2088, 'european': 2089, 'stoop': 2090, 'trainer': 2091, 'wrestling': 2092, 'candy': 2093, 'cloud': 2094, 'footballers': 2095, 'baseman': 2096, 'league': 2097, 'rival': 2098, 'second': 2099, 'hangs': 2100, 'flannel': 2101, 'streets': 2102, 'progress': 2103, 'cameraman': 2104, 'pounces': 2105, 'males': 2106, 'fashioned': 2107, 'cowboys': 2108, 'rodeo': 2109, 'bucking': 2110, 'fisherman': 2111, 'kayaks': 2112, 'pitching': 2113, 'practices': 2114, 'trains': 2115, 'husky': 2116, 'emerging': 2117, 'opens': 2118, 'horseback': 2119, 'cooking': 2120, 'bound': 2121, 'lime': 2122, 'corgi': 2123, 'concert': 2124, 'parasailing': 2125, 'workers': 2126, 'jar': 2127, 'knit': 2128, 'feathered': 2129, 'caution': 2130, 'feather': 2131, 'purses': 2132, 'cheek': 2133, 'tracks': 2134, 'pale': 2135, 'tugging': 2136, 'playhouse': 2137, 'shoot': 2138, 'diner': 2139, 'played': 2140, '8': 2141, 'huddle': 2142, 'musical': 2143, 'juggles': 2144, 'growling': 2145, 'doberman': 2146, 'buried': 2147, 'stones': 2148, 'pointy': 2149, 'appear': 2150, 'navigates': 2151, 'himself': 2152, 'greenery': 2153, 'smokes': 2154, 'whist': 2155, 'tags': 2156, 'creating': 2157, 'crash': 2158, 'clings': 2159, 'rear': 2160, 'retrieve': 2161, 'summer': 2162, 'hitting': 2163, 'canyon': 2164, 'pauses': 2165, 'skinny': 2166, 'jumper': 2167, 'bungee': 2168, 'weeds': 2169, 'rainbow': 2170, 'kite': 2171, 'raincoat': 2172, 'juice': 2173, 'fencing': 2174, 'bundled': 2175, 'characters': 2176, 'billboard': 2177, 'itself': 2178, 'everywhere': 2179, 'monster': 2180, 'plaza': 2181, 'square': 2182, 'training': 2183, 'asphalt': 2184, 'sprints': 2185, 'leaf': 2186, 'casts': 2187, 'shrubs': 2188, 'belly': 2189, 'lush': 2190, 'countryside': 2191, 'peaks': 2192, 'teaching': 2193, 'officers': 2194, 'policemen': 2195, 'speaking': 2196, 'formation': 2197, 'bumpy': 2198, 'hay': 2199, 'pumpkin': 2200, 'star': 2201, 'sparkler': 2202, 'skirts': 2203, 'shoveling': 2204, 'shovels': 2205, 'hear': 2206, 'windsurfing': 2207, 'snack': 2208, 'caps': 2209, 'speeds': 2210, 'overpass': 2211, 'attention': 2212, 'tropical': 2213, 'resort': 2214, 'human': 2215, 'doll': 2216, 'driven': 2217, 'boarder': 2218, 'geese': 2219, 'lower': 2220, 'carpeted': 2221, 'awning': 2222, 'licks': 2223, 'extended': 2224, 'fencers': 2225, 'fair': 2226, 'upper': 2227, 'driveway': 2228, 'amid': 2229, 'bearing': 2230, 'gazes': 2231, 'slalom': 2232, 'poodles': 2233, 'interact': 2234, 'bone': 2235, 'artificial': 2236, 'hard': 2237, 'wrestler': 2238, 'wrestlers': 2239, 'tuxedos': 2240, 'pathway': 2241, 'son': 2242, 'expression': 2243, 'mountainous': 2244, 'mom': 2245, 'jesus': 2246, 'carried': 2247, 'wakeboarding': 2248, 'song': 2249, 'serious': 2250, 'dribbling': 2251, 'rafts': 2252, 'bread': 2253, 'glides': 2254, 'babies': 2255, 'beak': 2256, 'vehicles': 2257, 'newspapers': 2258, 'community': 2259, 'rollerskating': 2260, 'headdress': 2261, 'earrings': 2262, 'kayakers': 2263, 'canoes': 2264, 'courtyard': 2265, 'baskets': 2266, 'houses': 2267, 'boston': 2268, 'shiny': 2269, 'soft': 2270, 'wilderness': 2271, 'nature': 2272, 'spraying': 2273, 'squirting': 2274, 'telescope': 2275, 'gun': 2276, 'office': 2277, 'screams': 2278, 'rows': 2279, 'scenic': 2280, 'morning': 2281, 'skyline': 2282, 'bracelet': 2283, 'sleeves': 2284, 'stall': 2285, 'cartwheel': 2286, 'sleds': 2287, 'dragging': 2288, 'pirate': 2289, 'hug': 2290, 'costumed': 2291, 'spout': 2292, 'doors': 2293, 'seattle': 2294, 'observes': 2295, 'hotel': 2296, 'flops': 2297, 'tied': 2298, 'hardhat': 2299, 'surfboarder': 2300, 'dyed': 2301, 'industrial': 2302, 'helmeted': 2303, 'good': 2304, 'reddish': 2305, 'necked': 2306, 'knife': 2307, 'tails': 2308, 'lipstick': 2309, 'afternoon': 2310, 'reflecting': 2311, 'checks': 2312, 'operating': 2313, 'scuba': 2314, 'diver': 2315, 'travel': 2316, 'sporting': 2317, 'bib': 2318, 'toss': 2319, 'labrador': 2320, 'called': 2321, 'lounge': 2322, 'lamp': 2323, 'working': 2324, 'stripe': 2325, 'glacier': 2326, 'marked': 2327, 'gathering': 2328, 'computers': 2329, 'dot': 2330, 'lens': 2331, 'shirted': 2332, 'romp': 2333, 'tulips': 2334, 'incline': 2335, 'crouched': 2336, 'multicolor': 2337, 'individual': 2338, 'crown': 2339, 'festive': 2340, 'dumps': 2341, 'test': 2342, 'performance': 2343, 'bigger': 2344, 'halter': 2345, 'handlebars': 2346, 'observing': 2347, 'paints': 2348, 'belt': 2349, 'kicked': 2350, 'fairground': 2351, 'oar': 2352, 'wild': 2353, 'motor': 2354, 'campsite': 2355, 'storm': 2356, 'drain': 2357, 'love': 2358, 'pets': 2359, 'apartment': 2360, 'cycling': 2361, 'muscular': 2362, 'sword': 2363, 'puffy': 2364, 'logs': 2365, 'swords': 2366, 'spiderman': 2367, 'furniture': 2368, 'collide': 2369, 'tumbling': 2370, 'posts': 2371, 'sails': 2372, 'sailing': 2373, 'earring': 2374, 'jogs': 2375, 'sad': 2376, 'size': 2377, 'either': 2378, 'nighttime': 2379, 'wrestles': 2380, 'examining': 2381, 'soaked': 2382, 'overlook': 2383, 'telephone': 2384, 'booth': 2385, 'apron': 2386, 'squirts': 2387, 'strapped': 2388, 'droplets': 2389, 'performers': 2390, 'dive': 2391, 'ollie': 2392, 'terriers': 2393, 'parasails': 2394, 'windsurfer': 2395, 'was': 2396, 'built': 2397, 'bay': 2398, 'themed': 2399, 'cleaning': 2400, 'container': 2401, 'pail': 2402, 'slip': 2403, 'golf': 2404, 'petting': 2405, 'toddlers': 2406, 'playpen': 2407, 'padded': 2408, 'jogger': 2409, 'bath': 2410, 'bathtub': 2411, 'bathroom': 2412, 'florida': 2413, 'rally': 2414, 'tackles': 2415, 'residential': 2416, 'fountains': 2417, 'railroad': 2418, 'firefighter': 2419, 'hood': 2420, 'fireman': 2421, 'youth': 2422, 'camp': 2423, 'thick': 2424, 'feeding': 2425, 'eyebrows': 2426, 'moustache': 2427, 'tee': 2428, 'raise': 2429, 'cellphones': 2430, 'change': 2431, 'exercise': 2432, 'form': 2433, 'apples': 2434, 'gondola': 2435, 'skyscraper': 2436, 'stunts': 2437, 'bulldog': 2438, 'movie': 2439, 'squats': 2440, 'member': 2441, 'self': 2442, 'cheeks': 2443, 'seats': 2444, 'ramps': 2445, 'squirt': 2446, 'diaper': 2447, 'begging': 2448, 'gathers': 2449, 'firetruck': 2450, 'fields': 2451, 'hilly': 2452, 'attacking': 2453, 'check': 2454, 'striking': 2455, 'haircut': 2456, 'tv': 2457, 'obstacles': 2458, 'obama': 2459, 'keep': 2460, 'misty': 2461, 'chalk': 2462, 'trailing': 2463, 'treads': 2464, 'bears': 2465, 'expressions': 2466, 'stuff': 2467, 'shoots': 2468, 'marble': 2469, 'attack': 2470, 'stove': 2471, 'speeding': 2472, 'domino': 2473, 'pizza': 2474, 'galloping': 2475, 'recently': 2476, 'drenched': 2477, 'pattern': 2478, 'cobblestone': 2479, 'sacks': 2480, 'saddle': 2481, 'shed': 2482, 'port': 2483, 'much': 2484, 'tights': 2485, 'books': 2486, 'footballer': 2487, 'if': 2488, 'spoon': 2489, 'oriental': 2490, 'pinata': 2491, 'contest': 2492, 'stores': 2493, 'brindle': 2494, 'forehead': 2495, 'paperwork': 2496, 'pro': 2497, 'dusty': 2498, 'glider': 2499, 'pelican': 2500, 'toilet': 2501, 'drummer': 2502, 'gown': 2503, 'firing': 2504, 'puck': 2505, 'site': 2506, 'swan': 2507, 'coloring': 2508, 'evening': 2509, 'floaties': 2510, 'cards': 2511, 'vine': 2512, 'harbor': 2513, 'bagpipes': 2514, 'plates': 2515, 'alike': 2516, 'ducks': 2517, 'starring': 2518, 'suited': 2519, 'colorfully': 2520, 'tiled': 2521, 'helicopter': 2522, 'garage': 2523, 'numerous': 2524, 'clean': 2525, 'waist': 2526, 'aiming': 2527, 'defending': 2528, 'crossbones': 2529, 'tracksuit': 2530, 'swans': 2531, 'messy': 2532, 'buy': 2533, 'cries': 2534, 'sumo': 2535, 'bug': 2536, 'insect': 2537, 'tricycles': 2538, 'theater': 2539, 'native': 2540, 'cartwheels': 2541, 'cone': 2542, 'deer': 2543, 'badminton': 2544, 'wig': 2545, 'hooping': 2546, 'railings': 2547, 'struggle': 2548, 'newborn': 2549, 'television': 2550, 'used': 2551, 'seagull': 2552, 'flaming': 2553, 'ballet': 2554, 'accordion': 2555, 'priest': 2556, 'easter': 2557, 'march': 2558, 'turkeys': 2559, 'hawaiian': 2560, 'carts': 2561, 'twin': 2562, 'michael': 2563, 'jackson': 2564, 'pierced': 2565, 'bouquet': 2566, 'cutout': 2567, 'barn': 2568, 'words': 2569, 'sidelines': 2570, 'rocket': 2571, 'rubs': 2572, 'cutting': 2573, 'st': 2574, 'french': 2575, 'potato': 2576, 'chip': 2577, 'campfire': 2578, 'broom': 2579, 'yelling': 2580, 'target': 2581, 'milk': 2582, 'tutus': 2583, 'robot': 2584, 'statues': 2585, 'smelling': 2586, 'examine': 2587, 'participates': 2588, 'mountaineer': 2589, 'gazing': 2590, 'armenian': 2591, 'genocide': 2592, 'submerged': 2593, 'pretend': 2594, 'bale': 2595, 'pumpkins': 2596, 'magazine': 2597, 'kangaroo': 2598, 'tongues': 2599, 'paintball': 2600, 'bookstore': 2601, 'map': 2602, 'evil': 2603, 'punching': 2604, 'letters': 2605, 'library': 2606, 'eagle': 2607, 'bee': 2608, 'mascot': 2609, 'sidecar': 2610, 'hamburgers': 2611, 'hydrant': 2612, 'bowling': 2613, 'frying': 2614, 'seagulls': 2615, 'cannon': 2616, 'bitten': 2617, 'spotlight': 2618, 'binoculars': 2619, 'hospital': 2620, 'buggy': 2621, 'rolled': 2622, 'sleeve': 2623, 'cigarettes': 2624, 'bathrobe': 2625, 'hell': 2626, 'escalator': 2627, 'graduation': 2628, 'dolphins': 2629, '<unk>': 2630, '<start>': 2631, '<end>': 2632, '<pad>': 0}\n",
            "base_filename :  flickr8k_5_cap_per_img_5_min_word_freq\n",
            " *** Split Change *** : TRAIN\n",
            "\n",
            "Reading TRAIN images and captions, storing to file...\n",
            "\n",
            "captions when len(imcaps[i]) GT> captions_per_image: [['a', 'black', 'dog', 'is', 'running', 'after', 'a', 'white', 'dog', 'in', 'the', 'snow'], ['two', 'dogs', 'chase', 'each', 'other', 'across', 'the', 'snowy', 'ground'], ['two', 'dogs', 'running', 'through', 'a', 'low', 'lying', 'body', 'of', 'water'], ['black', 'dog', 'chasing', 'brown', 'dog', 'through', 'snow'], ['two', 'dogs', 'play', 'together', 'in', 'the', 'snow']]\n",
            "Encoding captions start stat count: 1 start Length: [] enc_captions: [] len(captions): 5\n",
            "Encoding captions End stat count: 1 end Length: [14] enc_captions: [[2631, 1, 2, 3, 4, 5, 6, 1, 7, 3, 8, 9, 10, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "Encoding captions End stat count: 1 end Length: [14, 11] enc_captions: [[2631, 1, 2, 3, 4, 5, 6, 1, 7, 3, 8, 9, 10, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 14, 15, 16, 17, 18, 19, 9, 20, 21, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "Encoding captions End stat count: 1 end Length: [14, 11, 12] enc_captions: [[2631, 1, 2, 3, 4, 5, 6, 1, 7, 3, 8, 9, 10, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 14, 15, 16, 17, 18, 19, 9, 20, 21, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 14, 15, 5, 13, 1, 24, 25, 26, 27, 28, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "Encoding captions End stat count: 1 end Length: [14, 11, 12, 9] enc_captions: [[2631, 1, 2, 3, 4, 5, 6, 1, 7, 3, 8, 9, 10, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 14, 15, 16, 17, 18, 19, 9, 20, 21, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 14, 15, 5, 13, 1, 24, 25, 26, 27, 28, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 2, 3, 11, 12, 3, 13, 10, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "Encoding captions End stat count: 1 end Length: [14, 11, 12, 9, 9] enc_captions: [[2631, 1, 2, 3, 4, 5, 6, 1, 7, 3, 8, 9, 10, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 14, 15, 16, 17, 18, 19, 9, 20, 21, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 14, 15, 5, 13, 1, 24, 25, 26, 27, 28, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 2, 3, 11, 12, 3, 13, 10, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 14, 15, 22, 23, 8, 9, 10, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "captions when len(imcaps[i]) GT> captions_per_image: [['a', 'little', 'baby', 'plays', 'croquet'], ['the', 'little', 'boy', 'is', 'playing', 'with', 'a', 'croquet', 'hammer', 'and', 'ball', 'beside', 'the', 'car'], ['a', 'little', 'girl', 'plays', 'croquet', 'next', 'to', 'a', 'truck'], ['the', 'child', 'is', 'playing', 'croquette', 'by', 'the', 'truck'], ['the', 'kid', 'is', 'in', 'front', 'of', 'a', 'car', 'with', 'a', 'put', 'and', 'a', 'ball']]\n",
            "captions when len(imcaps[i]) GT> captions_per_image: [['a', 'brown', 'dog', 'in', 'the', 'snow', 'has', 'something', 'hot', 'pink', 'in', 'its', 'mouth'], ['a', 'brown', 'dog', 'in', 'the', 'snow', 'holding', 'a', 'pink', 'hat'], ['a', 'dog', 'with', 'something', 'pink', 'in', 'its', 'mouth', 'is', 'looking', 'forward'], ['a', 'dog', 'is', 'carrying', 'something', 'pink', 'in', 'its', 'mouth', 'while', 'walking', 'through', 'the', 'snow'], ['a', 'brown', 'dog', 'is', 'holding', 'a', 'pink', 'shirt', 'in', 'the', 'snow']]\n",
            "captions when len(imcaps[i]) GT> captions_per_image: [['a', 'brown', 'dog', 'wearing', 'a', 'black', 'collar', 'running', 'across', 'the', 'beach'], ['the', 'large', 'brown', 'dog', 'is', 'running', 'on', 'the', 'beach', 'by', 'the', 'ocean'], ['a', 'brown', 'dog', 'is', 'running', 'along', 'a', 'beach'], ['brown', 'dog', 'running', 'on', 'the', 'beach'], ['a', 'dog', 'walks', 'on', 'the', 'sand', 'near', 'the', 'water']]\n",
            "captions when len(imcaps[i]) GT> captions_per_image: [['the', 'black', 'dog', 'is', 'dropping', 'a', 'red', 'disc', 'on', 'a', 'beach'], ['dog', 'catching', 'a', 'red', 'frisbee'], ['a', 'black', 'and', 'white', 'dog', 'with', 'a', 'red', 'frisbee', 'standing', 'on', 'a', 'sandy', 'beach'], ['a', 'dog', 'with', 'a', 'red', 'frisbee', 'flying', 'in', 'the', 'air'], ['a', 'dog', 'drops', 'a', 'red', 'disc', 'on', 'a', 'beach']]\n",
            "captions when len(imcaps[i]) GT> captions_per_image: [['a', 'woman', 'wears', 'a', 'red', 'helmet', 'and', 'blue', 'shirt', 'as', 'she', 'goes', 'for', 'a', 'bike', 'ride', 'in', 'the', 'shade'], ['a', 'cyclist', 'wearing', 'a', 'red', 'helmet', 'is', 'riding', 'on', 'the', 'pavement'], ['person', 'in', 'blue', 'shirt', 'and', 'red', 'helmet', 'riding', 'bike', 'down', 'the', 'road'], ['a', 'person', 'on', 'a', 'bike', 'wearing', 'a', 'red', 'helmet', 'riding', 'down', 'a', 'street'], ['a', 'girl', 'is', 'riding', 'a', 'bike', 'on', 'the', 'street', 'while', 'wearing', 'a', 'red', 'helmet']]\n",
            "captions when len(imcaps[i]) GT> captions_per_image: [['a', 'man', 'on', 'the', 'street', 'wearing', 'leather', 'chaps', 'and', 'a', 'chainmail', 'codpiece'], ['a', 'man', 'dressed', 'in', 'a', 'purple', 'shirt', 'and', 'red', 'bandanna', 'smiles', 'at', 'the', 'people', 'watching', 'him'], ['there', 'is', 'a', 'man', 'in', 'a', 'purple', 'shirt', 'leather', 'chaps', 'and', 'a', 'red', 'bandanna', 'standing', 'near', 'other', 'men'], ['a', 'man', 'wearing', 'a', 'purple', 'shirt', 'and', 'black', 'leather', 'chaps', 'poses', 'for', 'the', 'camera'], ['man', 'dressed', 'in', 'leather', 'chaps', 'and', 'purple', 'shirt', 'stands', 'in', 'front', 'of', 'onlookers']]\n",
            "captions when len(imcaps[i]) GT> captions_per_image: [['a', 'young', 'boy', 'is', 'dancing', 'around'], ['a', 'young', 'boy', 'with', 'a', 'red', 'short', 'sleeved', 'shirt', 'and', 'jeans', 'runs', 'by', 'some', 'trees'], ['a', 'boy', 'wearing', 'a', 'red', 't', 'shirt', 'is', 'running', 'through', 'woodland'], ['a', 'child', 'runs', 'near', 'some', 'trees'], ['the', 'little', 'boy', 'in', 'the', 'red', 'shirt', 'stops', 'to', 'smile', 'for', 'the', 'camera']]\n",
            "captions when len(imcaps[i]) GT> captions_per_image: [['the', 'girl', 'is', 'holding', 'a', 'green', 'ball'], ['a', 'young', 'girl', 'wearing', 'white', 'looks', 'at', 'the', 'camera', 'as', 'she', 'plays'], ['a', 'girl', 'in', 'a', 'white', 'dress'], ['a', 'little', 'girl', 'in', 'white', 'is', 'looking', 'back', 'at', 'the', 'camera', 'while', 'carrying', 'a', 'water', 'grenade'], ['a', 'smiling', 'young', 'girl', 'in', 'braids', 'is', 'playing', 'ball']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6000/6000 [00:46<00:00, 129.54it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "***Total Count***: 6000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 11/1000 [00:00<00:08, 109.89it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " *** Split Change *** : VAL\n",
            "\n",
            "Reading VAL images and captions, storing to file...\n",
            "\n",
            "captions when len(imcaps[i]) GT> captions_per_image: [['two', 'small', 'children', 'in', 'red', 'shirts', 'playing', 'on', 'a', 'skateboard'], ['two', 'girls', 'play', 'on', 'a', 'skateboard', 'in', 'a', 'courtyard'], ['two', 'young', 'children', 'on', 'a', 'skateboard', 'going', 'across', 'a', 'sidewalk'], ['two', 'people', 'play', 'on', 'a', 'long', 'skateboard'], ['the', 'boy', 'laying', 'face', 'down', 'on', 'a', 'skateboard', 'is', 'being', 'pushed', 'along', 'the', 'ground', 'by', 'another', 'boy']]\n",
            "Encoding captions start stat count: 1 start Length: [] enc_captions: [] len(captions): 5\n",
            "Encoding captions End stat count: 1 end Length: [12] enc_captions: [[2631, 14, 256, 226, 8, 72, 957, 37, 67, 1, 585, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "Encoding captions End stat count: 1 end Length: [12, 11] enc_captions: [[2631, 14, 256, 226, 8, 72, 957, 37, 67, 1, 585, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 14, 172, 22, 67, 1, 585, 8, 1, 2265, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "Encoding captions End stat count: 1 end Length: [12, 11, 12] enc_captions: [[2631, 14, 256, 226, 8, 72, 957, 37, 67, 1, 585, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 14, 172, 22, 67, 1, 585, 8, 1, 2265, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 14, 120, 226, 67, 1, 585, 723, 19, 1, 595, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "Encoding captions End stat count: 1 end Length: [12, 11, 12, 9] enc_captions: [[2631, 14, 256, 226, 8, 72, 957, 37, 67, 1, 585, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 14, 172, 22, 67, 1, 585, 8, 1, 2265, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 14, 120, 226, 67, 1, 585, 723, 19, 1, 595, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 14, 105, 22, 67, 1, 270, 585, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "Encoding captions End stat count: 1 end Length: [12, 11, 12, 9, 19] enc_captions: [[2631, 14, 256, 226, 8, 72, 957, 37, 67, 1, 585, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 14, 172, 22, 67, 1, 585, 8, 1, 2265, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 14, 120, 226, 67, 1, 585, 723, 19, 1, 595, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 14, 105, 22, 67, 1, 270, 585, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 9, 46, 1097, 542, 88, 67, 1, 585, 4, 702, 1695, 62, 9, 21, 38, 244, 46, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "captions when len(imcaps[i]) GT> captions_per_image: [['a', 'young', 'boy', 'jumps', 'off', 'a', 'rock', 'in', 'the', 'forest'], ['a', 'boy', 'in', 'a', 'blue', 'top', 'is', 'jumping', 'off', 'some', 'rocks', 'in', 'the', 'woods'], ['child', 'in', 'blue', 'and', 'grey', 'shirt', 'jumping', 'off', 'hill', 'in', 'the', 'woods'], ['a', 'boy', 'jumps', 'off', 'a', 'tan', 'rock'], ['a', 'boy', 'jumps', 'up', 'in', 'a', 'field', 'in', 'the', 'woods']]\n",
            "captions when len(imcaps[i]) GT> captions_per_image: [['the', 'woman', 'is', 'leading', 'a', 'dog', 'through', 'an', 'obstacle', 'course'], ['a', 'woman', 'is', 'guiding', 'a', 'brown', 'dog', 'around', 'an', 'obstacle', 'course'], ['a', 'small', 'tan', 'and', 'white', 'dog', 'and', 'trainer', 'running', 'an', 'obstacle', 'course'], ['a', 'lady', 'walking', 'her', 'dog', 'through', 'an', 'obstacle', 'course', 'while', 'other', 'people', 'are', 'in', 'the', 'background'], ['a', 'woman', 'with', 'a', 'hat', 'is', 'leading', 'a', 'small', 'dog', 'through', 'an', 'obstacle', 'course']]\n",
            "captions when len(imcaps[i]) GT> captions_per_image: [['a', 'white', 'dog', 'is', 'watching', 'a', 'black', 'dog', 'jump', 'on', 'a', 'lawn', 'next', 'to', 'a', 'pile', 'of', 'large', 'rocks'], ['a', 'white', 'dog', 'watching', 'a', 'black', 'dog', 'in', 'the', 'air'], ['a', 'dog', 'looks', 'at', 'another', 'dog', 'catching', 'a', 'ball', 'in', 'the', 'air'], ['a', 'big', 'black', 'dog', 'jumps', 'in', 'the', 'air', 'to', 'catch', 'the', 'tennis', 'ball', 'in', 'his', 'mouth'], ['two', 'dogs', 'playing', 'with', 'a', 'tennis', 'ball', 'in', 'the', 'yard']]\n",
            "captions when len(imcaps[i]) GT> captions_per_image: [['two', 'women', 'in', 'bathing', 'suit', 'on', 'large', 'rocks', 'at', 'the', 'ocean'], ['two', 'women', 'in', 'bathing', 'suits', 'climb', 'rock', 'piles', 'by', 'the', 'ocean'], ['two', 'women', 'are', 'climbing', 'over', 'rocks', 'near', 'to', 'the', 'ocean'], ['two', 'women', 'climb', 'on', 'top', 'of', 'rocks', 'in', 'front', 'of', 'the', 'ocean'], ['two', 'woman', 'climbing', 'rocks', 'around', 'the', 'ocean']]\n",
            "captions when len(imcaps[i]) GT> captions_per_image: [['a', 'child', 'sings', 'into', 'a', 'loofa', 'in', 'the', 'bathtub'], ['a', 'child', 'is', 'laying', 'in', 'a', 'bubble', 'bath', 'holding', 'a', 'yellow', 'scrubbing', 'brush', 'up', 'to', 'his', 'mouth', 'as', 'if', 'singing', 'into', 'a', 'microphone'], ['boy', 'in', 'bubble', 'bath', 'yelling', 'or', 'singing', 'into', 'scrubbing', 'brush'], ['a', 'little', 'boy', 'is', 'laying', 'down', 'in', 'a', 'bubble', 'bath'], ['the', 'child', 'is', 'surrounded', 'by', 'bubbles', 'while', 'in', 'the', 'bathtub']]\n",
            "captions when len(imcaps[i]) GT> captions_per_image: [['a', 'group', 'of', 'people', 'are', 'ice', 'skating', 'in', 'a', 'big', 'city'], ['people', 'using', 'an', 'outdoor', 'ice', 'skating', 'rink'], ['an', 'ice', 'skating', 'park', 'in', 'winter', 'with', 'many', 'people'], ['an', 'outdoor', 'ice', 'skating', 'rink', 'full', 'of', 'people'], ['people', 'skate', 'in', 'an', 'urban', 'outdoor', 'ice', 'skating', 'rink']]\n",
            "captions when len(imcaps[i]) GT> captions_per_image: [['three', 'boys', 'and', 'one', 'girl', 'in', 'a', 'muddy', 'grass', 'field'], ['four', 'children', 'are', 'running', 'and', 'playing', 'tag'], ['group', 'of', 'children', 'play', 'in', 'rural', 'area'], ['four', 'children', 'are', 'playing', 'in', 'a', 'grassy', 'field'], ['four', 'children', 'are', 'playing', 'together', 'outside']]\n",
            "captions when len(imcaps[i]) GT> captions_per_image: [['a', 'little', 'boy', 'skateboards', 'on', 'a', 'cement', 'ramp'], ['a', 'young', 'child', 'wearing', 'a', 'blue', 'helmet', 'rides', 'a', 'skateboard', 'over', 'a', 'ramp'], ['a', 'little', 'baby', 'skateboarding', 'on', 'a', 'small', 'concrete', 'wall'], ['a', 'young', 'skateboarder', 'on', 'a', 'ramp'], ['the', 'little', 'boy', 'in', 'the', 'green', 'jacket', 'is', 'riding', 'a', 'skateboard', 'on', 'a', 'ramp']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:07<00:00, 128.12it/s]\n",
            "  0%|          | 0/1000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "***Total Count***: 1000\n",
            " *** Split Change *** : TEST\n",
            "\n",
            "Reading TEST images and captions, storing to file...\n",
            "\n",
            "captions when len(imcaps[i]) GT> captions_per_image: [['two', 'brown', 'dogs', 'playfully', 'fight', 'in', 'the', 'snow'], ['two', 'brown', 'dogs', 'wrestle', 'in', 'the', 'snow'], ['two', 'dogs', 'playing', 'in', 'the', 'snow'], ['the', 'dogs', 'are', 'in', 'the', 'snow', 'in', 'front', 'of', 'a', 'fence'], ['the', 'dogs', 'play', 'on', 'the', 'snow']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 24/1000 [00:00<00:09, 105.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Encoding captions start stat count: 1 start Length: [] enc_captions: [] len(captions): 5\n",
            "Encoding captions End stat count: 1 end Length: [10] enc_captions: [[2631, 14, 12, 15, 546, 719, 8, 9, 10, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "Encoding captions End stat count: 1 end Length: [10, 9] enc_captions: [[2631, 14, 12, 15, 546, 719, 8, 9, 10, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 14, 12, 15, 1857, 8, 9, 10, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "Encoding captions End stat count: 1 end Length: [10, 9, 8] enc_captions: [[2631, 14, 12, 15, 546, 719, 8, 9, 10, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 14, 12, 15, 1857, 8, 9, 10, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 14, 15, 37, 8, 9, 10, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "Encoding captions End stat count: 1 end Length: [10, 9, 8, 13] enc_captions: [[2631, 14, 12, 15, 546, 719, 8, 9, 10, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 14, 12, 15, 1857, 8, 9, 10, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 14, 15, 37, 8, 9, 10, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 9, 15, 288, 8, 9, 10, 8, 40, 27, 1, 214, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "Encoding captions End stat count: 1 end Length: [10, 9, 8, 13, 8] enc_captions: [[2631, 14, 12, 15, 546, 719, 8, 9, 10, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 14, 12, 15, 1857, 8, 9, 10, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 14, 15, 37, 8, 9, 10, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 9, 15, 288, 8, 9, 10, 8, 40, 27, 1, 214, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2631, 9, 15, 22, 67, 9, 10, 2632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "captions when len(imcaps[i]) GT> captions_per_image: [['the', 'small', 'brown', 'and', 'white', 'dog', 'is', 'in', 'the', 'pool'], ['a', 'brown', 'and', 'white', 'dog', 'swimming', 'towards', 'some', 'in', 'the', 'pool'], ['small', 'dog', 'is', 'paddling', 'through', 'the', 'water', 'in', 'a', 'pool'], ['a', 'dog', 'swims', 'in', 'a', 'pool', 'near', 'a', 'person'], ['a', 'dog', 'in', 'a', 'swimming', 'pool', 'swims', 'toward', 'sombody', 'we', 'cannot', 'see']]\n",
            "captions when len(imcaps[i]) GT> captions_per_image: [['one', 'performer', 'wearing', 'a', 'feathered', 'headdress', 'dancing', 'with', 'another', 'performer', 'in', 'the', 'streets'], ['a', 'man', 'and', 'a', 'woman', 'with', 'feathers', 'on', 'her', 'head', 'dance'], ['a', 'man', 'and', 'a', 'woman', 'wearing', 'decorative', 'costumes', 'and', 'dancing', 'in', 'a', 'crowd', 'of', 'onlookers'], ['a', 'man', 'and', 'a', 'woman', 'in', 'festive', 'costumes', 'dancing'], ['two', 'people', 'are', 'dancing', 'with', 'drums', 'on', 'the', 'right', 'and', 'a', 'crowd', 'behind', 'them']]\n",
            "captions when len(imcaps[i]) GT> captions_per_image: [['three', 'people', 'sit', 'at', 'an', 'outdoor', 'table', 'in', 'front', 'of', 'a', 'building', 'painted', 'like', 'the', 'union', 'jack'], ['three', 'people', 'sit', 'at', 'an', 'outdoor', 'cafe'], ['a', 'couple', 'of', 'people', 'sit', 'outdoors', 'at', 'a', 'table', 'with', 'an', 'umbrella', 'and', 'talk'], ['three', 'people', 'sit', 'at', 'a', 'picnic', 'table', 'outside', 'of', 'a', 'building', 'painted', 'like', 'a', 'union', 'jack'], ['three', 'people', 'are', 'sitting', 'at', 'an', 'outside', 'picnic', 'bench', 'with', 'an', 'umbrella']]\n",
            "captions when len(imcaps[i]) GT> captions_per_image: [['a', 'man', 'is', 'wearing', 'a', 'sooners', 'red', 'football', 'shirt', 'and', 'helmet'], ['a', 'oklahoma', 'sooners', 'football', 'player', 'wearing', 'his', 'jersey', 'number', '28'], ['guy', 'in', 'red', 'and', 'white', 'football', 'uniform'], ['a', 'sooners', 'football', 'player', 'weas', 'the', 'number', '28', 'and', 'black', 'armbands'], ['the', 'american', 'footballer', 'is', 'wearing', 'a', 'red', 'and', 'white', 'strip']]\n",
            "captions when len(imcaps[i]) GT> captions_per_image: [['a', 'brown', 'dog', 'running'], ['a', 'brown', 'dog', 'with', 'its', 'front', 'paws', 'off', 'the', 'ground', 'on', 'a', 'grassy', 'surface', 'near', 'red', 'and', 'purple', 'flowers'], ['a', 'yellow', 'dog', 'is', 'playing', 'in', 'a', 'grassy', 'area', 'near', 'flowers'], ['a', 'dog', 'runs', 'across', 'a', 'grassy', 'lawn', 'near', 'some', 'flowers'], ['a', 'brown', 'dog', 'running', 'over', 'grass']]\n",
            "captions when len(imcaps[i]) GT> captions_per_image: [['two', 'girls', 'are', 'looking', 'past', 'each', 'other', 'in', 'different', 'directions', 'while', 'standing', 'in', 'a', 'crowd'], ['a', 'girl', 'with', 'dark', 'brown', 'hair', 'and', 'eyes', 'in', 'a', 'blue', 'scarf', 'is', 'standing', 'next', 'to', 'a', 'girl', 'in', 'a', 'fur', 'edged', 'coat'], ['the', 'girls', 'were', 'in', 'the', 'crowd'], ['an', 'asian', 'boy', 'and', 'an', 'asian', 'girl', 'are', 'smiling', 'in', 'a', 'crowd', 'of', 'people'], ['two', 'dark', 'haired', 'girls', 'are', 'in', 'a', 'crowd']]\n",
            "captions when len(imcaps[i]) GT> captions_per_image: [['close', 'up', 'of', 'dog', 'in', 'profile', 'with', 'mouth', 'open'], ['the', 'dog', 's', 'mouth', 'is', 'open', 'like', 'he', 'is', 'yawning'], ['dog', 'yawns'], ['brown', 'and', 'white', 'dog', 'yawning'], ['a', 'dog', 'with', 'its', 'mouth', 'opened']]\n",
            "captions when len(imcaps[i]) GT> captions_per_image: [['a', 'black', 'dog', 'on', 'a', 'beach', 'carrying', 'a', 'ball', 'in', 'its', 'mouth'], ['a', 'black', 'dog', 'emerges', 'from', 'the', 'water', 'onto', 'the', 'sand', 'holding', 'a', 'white', 'object', 'in', 'its', 'mouth'], ['the', 'black', 'dog', 'jumps', 'out', 'of', 'the', 'water', 'with', 'something', 'in', 'its', 'mouth'], ['a', 'black', 'dog', 'emerges', 'from', 'the', 'water', 'with', 'a', 'white', 'ball', 'in', 'its', 'mouth'], ['a', 'black', 'dog', 'walking', 'out', 'of', 'the', 'water', 'with', 'a', 'white', 'ball', 'in', 'his', 'mouth']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:07<00:00, 127.66it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "***Total Count***: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9EKZp1E-4D9"
      },
      "source": [
        "class CaptionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_folder, data_name, split, transform=None):\n",
        "        \"\"\"\n",
        "        :param data_folder: folder where data files are stored\n",
        "        :param data_name: base name of processed datasets\n",
        "        :param split: split, one of 'TRAIN', 'VAL', or 'TEST'\n",
        "        :param transform: image transform pipeline\n",
        "        \"\"\"\n",
        "        self.split = split\n",
        "        assert self.split in {'TRAIN', 'VAL', 'TEST'}\n",
        "\n",
        "        # Open hdf5 file where images are stored\n",
        "        self.h = h5py.File(os.path.join(data_folder, self.split + '_IMAGES_' + data_name + '.hdf5'), 'r')\n",
        "        self.imgs = self.h['images']\n",
        "\n",
        "        # Captions per image\n",
        "        self.cpi = self.h.attrs['captions_per_image']  ## If there are 5 different captions for an image, then cpi = 5\n",
        "\n",
        "        # Load encoded captions (completely into memory)\n",
        "        with open(os.path.join(data_folder, self.split + '_CAPTIONS_' + data_name + '.json'), 'r') as j:\n",
        "            self.captions = json.load(j)\n",
        "\n",
        "        # Load caption lengths (completely into memory)\n",
        "        with open(os.path.join(data_folder, self.split + '_CAPLENS_' + data_name + '.json'), 'r') as j:\n",
        "            self.caplens = json.load(j)\n",
        "\n",
        "        # PyTorch transformation pipeline for the image (normalizing, etc.)\n",
        "        self.transform = transform\n",
        "\n",
        "        # Total number of datapoints\n",
        "        self.dataset_size = len(self.captions)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # Remember, the Nth caption corresponds to the (N // captions_per_image)th image\n",
        "        # i indicates captions..For every image there are 5 captions (cpi)...so total images = i / self.cpi\n",
        "        img = torch.FloatTensor(self.imgs[i // self.cpi] / 255.)\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        caption = torch.LongTensor(self.captions[i])\n",
        "\n",
        "        caplen = torch.LongTensor([self.caplens[i]])\n",
        "\n",
        "        if self.split is 'TRAIN':\n",
        "            return img, caption, caplen\n",
        "        else:\n",
        "            # For validation of testing, also return all 'captions_per_image' captions to find BLEU-4 score\n",
        "            # eg: all_captions belonging to 2nd image.\n",
        "            # Here i will be 5,6,7,8,9 (assuming index starts at 0). Let us take i = 8\n",
        "            # so [((8//5)*5):(((8//5)*5) + 5)]  = [(1*5):((1*5)+5)] = [5:10] ie return all captions with idx 5,6,7,8,9 for 2nd image\n",
        "            all_captions = torch.LongTensor(\n",
        "                self.captions[((i // self.cpi) * self.cpi):(((i // self.cpi) * self.cpi) + self.cpi)])\n",
        "            return img, caption, caplen, all_captions\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGM3CRH_-7uW"
      },
      "source": [
        "def init_embedding(embeddings):\n",
        "    \"\"\"\n",
        "    Fills embedding tensor with values from the uniform distribution.\n",
        "    :param embeddings: embedding tensor\n",
        "    \"\"\"\n",
        "    bias = np.sqrt(3.0 / embeddings.size(1))\n",
        "    torch.nn.init.uniform_(embeddings, -bias, bias)\n",
        "\n",
        "'''\n",
        "def load_embeddings(emb_file, word_map):\n",
        "    \"\"\"\n",
        "    Creates an embedding tensor for the specified word map, for loading into the model.\n",
        "    :param emb_file: file containing embeddings (stored in GloVe format)\n",
        "    :param word_map: word map (word map is what we get based on captions dataset)\n",
        "    :return: embeddings in the same order as the words in the word map, dimension of embeddings\n",
        "    \"\"\"\n",
        "\n",
        "    # Find embedding dimension\n",
        "    with open(emb_file, 'r') as f:\n",
        "        emb_dim = len(f.readline().split(' ')) - 1\n",
        "\n",
        "    vocab = set(word_map.keys())\n",
        "\n",
        "    # Create tensor to hold embeddings, initialize\n",
        "    embeddings = torch.FloatTensor(len(vocab), emb_dim)\n",
        "    init_embedding(embeddings)\n",
        "\n",
        "    # Read embedding file\n",
        "    print(\"\\nLoading embeddings...\")\n",
        "    for line in open(emb_file, 'r'):\n",
        "        line = line.split(' ')\n",
        "\n",
        "        emb_word = line[0]\n",
        "        embedding = list(map(lambda t: float(t), filter(lambda n: n and not n.isspace(), line[1:])))\n",
        "\n",
        "        # Ignore word if not in train_vocab\n",
        "        if emb_word not in vocab:\n",
        "            continue\n",
        "\n",
        "        embeddings[word_map[emb_word]] = torch.FloatTensor(embedding)\n",
        "\n",
        "    return embeddings, emb_dim\n",
        "'''\n",
        "\n",
        "def clip_gradient(optimizer, grad_clip):\n",
        "    \"\"\"\n",
        "    Clips gradients computed during backpropagation to avoid explosion of gradients.\n",
        "    :param optimizer: optimizer with the gradients to be clipped\n",
        "    :param grad_clip: clip value\n",
        "    \"\"\"\n",
        "    for group in optimizer.param_groups:\n",
        "        for param in group['params']:\n",
        "            if param.grad is not None:\n",
        "                param.grad.data.clamp_(-grad_clip, grad_clip)\n",
        "\n",
        "\n",
        "def save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "                    bleu4, is_best):\n",
        "    \"\"\"\n",
        "    Saves model checkpoint.\n",
        "    :param data_name: base name of processed dataset\n",
        "    :param epoch: epoch number\n",
        "    :param epochs_since_improvement: number of epochs since last improvement in BLEU-4 score\n",
        "    :param encoder: encoder model\n",
        "    :param decoder: decoder model\n",
        "    :param encoder_optimizer: optimizer to update encoder's weights, if fine-tuning\n",
        "    :param decoder_optimizer: optimizer to update decoder's weights\n",
        "    :param bleu4: validation BLEU-4 score for this epoch\n",
        "    :param is_best: is this checkpoint the best so far?\n",
        "    \"\"\"\n",
        "    state = {'epoch': epoch,\n",
        "             'epochs_since_improvement': epochs_since_improvement,\n",
        "             'bleu-4': bleu4,\n",
        "             'encoder': encoder,\n",
        "             'decoder': decoder,\n",
        "             'encoder_optimizer': encoder_optimizer,\n",
        "             'decoder_optimizer': decoder_optimizer}\n",
        "    filename    = 'checkpoint_' + data_name + '.pth.tar'\n",
        "    pt_filename = 'checkpoint_' + data_name + '.pt'\n",
        "    torch.save(state, filename)\n",
        "    torch.save({\n",
        "                \"encoder\": encoder.state_dict(),\n",
        "                \"decoder\": decoder.state_dict()\n",
        "               }, pt_filename)\n",
        "    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n",
        "    print('epoch :', epoch, 'Checkpoint saved :', filename, pt_filename)\n",
        "    if is_best:\n",
        "        torch.save(state, 'BEST_' + filename)\n",
        "        torch.save({\n",
        "                \"encoder\": encoder.state_dict(),\n",
        "                \"decoder\": decoder.state_dict()\n",
        "                   }, 'BEST_' + pt_filename)        \n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Keeps track of most recent, average, sum, and count of a metric. This class is used for below objects while training & validating:\n",
        "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
        "    data_time = AverageMeter()  # data loading time\n",
        "    losses = AverageMeter()  # loss (per word decoded)\n",
        "    top5accs = AverageMeter()  # top5 accuracy\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, shrink_factor):\n",
        "    \"\"\"\n",
        "    Shrinks learning rate by a specified factor.\n",
        "    :param optimizer: optimizer whose learning rate must be shrunk.\n",
        "    :param shrink_factor: factor in interval (0, 1) to multiply learning rate with.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nDECAYING learning rate.\")\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
        "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n",
        "\n",
        "\n",
        "def accuracy(scores, targets, k):\n",
        "    \"\"\"\n",
        "    Computes top-k accuracy, from predicted and true labels.\n",
        "    :param scores: scores from the model\n",
        "    :param targets: true labels\n",
        "    :param k: k in top-k accuracy\n",
        "    :return: top-k accuracy\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size = targets.size(0)\n",
        "    _, ind = scores.topk(k, 1, True, True)\n",
        "    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n",
        "    correct_total = correct.view(-1).float().sum()  # 0D tensor\n",
        "    return correct_total.item() * (100.0 / batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWm4jqGx9G9y"
      },
      "source": [
        "#### Necessary imports for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-397Jrl-No0"
      },
      "source": [
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "#from models import Encoder, DecoderWithAttention\n",
        "#from datasets import *\n",
        "#from utils import *\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22HMt_r9THF6"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoded_image_size=14):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_image_size = encoded_image_size\n",
        "\n",
        "        resnet = torchvision.models.resnet101(pretrained=True)  # pretrained ImageNet ResNet-101\n",
        "\n",
        "        # Remove linear and pool layers (since we're not doing classification)\n",
        "        modules = list(resnet.children())[:-2]\n",
        "                 \n",
        "        '''\n",
        "        nn.Sequential is a construction which is used when you want to run certain layers sequentially.\n",
        "        Let us say modules = [layer1,layer2,layer3]\n",
        "        x = ... # our input\n",
        "\n",
        "        x = layer1(x)\n",
        "        x = layer2(x)\n",
        "        x = layer3(x)\n",
        "        '''\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "\n",
        "        # Resize image to fixed size to allow input images of variable size\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
        "\n",
        "        self.fine_tune()\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n",
        "        :return: encoded images\n",
        "        \"\"\"\n",
        "        out = self.resnet(images)  # (batch_size, 2048, image_size/32, image_size/32)\n",
        "        out = self.adaptive_pool(out)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
        "        out = out.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 2048) -> (32, 14, 14, 2048)\n",
        "        #print('Encoder Ouput size:', out.size())\n",
        "        return out\n",
        "\n",
        "    def fine_tune(self, fine_tune=True):\n",
        "        \"\"\"\n",
        "        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n",
        "        :param fine_tune: Allow?\n",
        "        \"\"\"\n",
        "        for p in self.resnet.parameters():\n",
        "            p.requires_grad = False\n",
        "        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n",
        "        for c in list(self.resnet.children())[5:]:\n",
        "            for p in c.parameters():\n",
        "                p.requires_grad = fine_tune"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssYVnAd6TR4v"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention Network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
        "        \"\"\"\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param attention_dim: size of the attention network\n",
        "        \"\"\"\n",
        "        super(Attention, self).__init__()\n",
        "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
        "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
        "        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
        "\n",
        "    def forward(self, encoder_out, decoder_hidden, px):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim) [32, 196, 2048]\n",
        "        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim) [32, 512]\n",
        "        :return: attention weighted encoding, weights\n",
        "        \"\"\"\n",
        "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim) \n",
        "                                              # [32, 196, 2048] -> [32, 196, 512] makes encoder_out compatible with attention_dim via nn.linear\n",
        "                                              # for t = 17, batch_size_t = 3 in example, [3, 196, 2048] -> [3, 196, 512]\n",
        "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
        "                                                 # for t = 17, batch_size_t = 3 in example, [3, 512] -> [3, 512] as both attention_dim & decoder_dim = 512\n",
        "        att_temp0 = self.relu(att1 + att2.unsqueeze(1))\n",
        "        \n",
        "        #if px < 2:\n",
        "        #   print('Attention - att_temp0.size() :', att_temp0.size())            \n",
        "\n",
        "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
        "        # for t = 17,  batch_size_t = 3 in example,\n",
        "        #   att2.unsqueeze(1)                                             --> Adding dimension to make [3, 512] --> [3, 1, 512]\n",
        "        #   self.relu(att1 + att2.unsqueeze(1))                           --> [3, 196, 512] + [3, 1, 512] -> [3, 196, 512]\n",
        "        #   self.full_att(self.relu(att1 + att2.unsqueeze(1)))            --> [3, 196, 512] -> [3, 196, 1]\n",
        "        #   self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2) --> Removing dim =2 to give [3, 196]\n",
        "        \n",
        "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
        "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
        "        # for t = 17,  batch_size_t = 3 in example,\n",
        "        #    alpha = self.softmax(att)                       --> [3, 196]. These are weights to be multiplied with encodings to give attention.\n",
        "        #    alpha.unsqueeze(2)                              --> [3, 196, 1]. Adding dimension on dim =2 to enable multiplication with encodings.\n",
        "        #    encoder_out                                     --> [3, 196, 2048]\n",
        "        #    (encoder_out * alpha.unsqueeze(2))              --> [3, 196, 2048]. Multipled alpha. Equilant to multiplying a scalar weight.\n",
        "        #    (encoder_out * alpha.unsqueeze(2)).sum(dim=1)   --> Sums up 196 pixels in dim = 1 to give [3, 2048] \n",
        "\n",
        "        return attention_weighted_encoding, alpha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a-8ok8qTZAG"
      },
      "source": [
        "class DecoderWithAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n",
        "        \"\"\"\n",
        "        :param attention_dim: size of attention network\n",
        "        :param embed_dim: embedding size\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param vocab_size: size of vocabulary\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param dropout: dropout\n",
        "        \"\"\"\n",
        "        super(DecoderWithAttention, self).__init__()\n",
        "\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.attention_dim = attention_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
        "        self.dropout = nn.Dropout(p=self.dropout)\n",
        "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
        "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
        "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
        "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
        "        self.init_weights()  # initialize some layers with the uniform distribution\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
        "        \"\"\"\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def load_pretrained_embeddings(self, embeddings):\n",
        "        \"\"\"\n",
        "        Loads embedding layer with pre-trained embeddings.\n",
        "        :param embeddings: pre-trained embeddings\n",
        "        \"\"\"\n",
        "        self.embedding.weight = nn.Parameter(embeddings)\n",
        "\n",
        "    def fine_tune_embeddings(self, fine_tune=True):\n",
        "        \"\"\"\n",
        "        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n",
        "        :param fine_tune: Allow?\n",
        "        \"\"\"\n",
        "        for p in self.embedding.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "\n",
        "    def init_hidden_state(self, encoder_out, px):\n",
        "        \"\"\"\n",
        "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :return: hidden state, cell state\n",
        "        \"\"\"\n",
        "        mean_encoder_out = encoder_out.mean(dim=1) # will take mean of [32, 196, 2048] against dim=1 giving mean_encoder_out of size [32, 2048]\n",
        "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
        "        # An example how nn.linear works\n",
        "        # >>> m = nn.Linear(20, 30) --> Here self.init_h = nn.Linear(encoder_dim, decoder_dim) where encoder_dim = 2048 & decoder_dim = 512\n",
        "        # >>> input = torch.randn(128, 20)\n",
        "        # >>> output = m(input)     --> Here h = self.init_h(mean_encoder_out) --> we will get h of size [32, 512] i.e. 2048 converted to 512 for batch-size of 32 \n",
        "        # >>> print(output.size())\n",
        "        # torch.Size([128, 30])\n",
        "\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions, caption_lengths, px):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n",
        "        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
        "        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
        "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
        "        \"\"\"\n",
        "        batch_size = encoder_out.size(0)  # torch.Size([32, 14, 14, 2048])...batch_size 32\n",
        "        encoder_dim = encoder_out.size(-1) # encoder_dim = 2048..last dimension of encoder_out\n",
        "        vocab_size = self.vocab_size\n",
        "       \n",
        "        # Flatten image\n",
        "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim) -> (32, 196, 2048)\n",
        "        # enoder_out size is [32, 14, 14, 2048]. While using encoder_out.view, we kept batch_size =32 & encoder_dim = 2048 as such and\n",
        "        # gave -1 inplace of 14, 14. So pytorch converts this -1 to 14*14 = 196 which is num_pixels\n",
        "        # We are converting encoder output to this format because we need to know which pixel/s out of 196 to focus using attention mechanism\n",
        "        num_pixels = encoder_out.size(1) # 196 pixels (14*14)\n",
        "\n",
        "        # Sort input data by decreasing lengths; why? apparent below\n",
        "        # torch.sort will give 2 outputs \n",
        "        # A namedtuple of (values, indices) is returned, where the values are the sorted values and indices are the indices of the elements \n",
        "        # in the original input tensor. Here values -> caption_lengths, indices -> sort_ind\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True) # caption_lengths.size() -> 32 , sort_ind.size() -> 32\n",
        "        encoder_out = encoder_out[sort_ind]  # Sorting corresponding input image encodings based on sorted indices [32, 196, 2048]\n",
        "        encoded_captions = encoded_captions[sort_ind] # Sorting corresponding input caption encodings based on sorted indices [32, 52]\n",
        "                                                      # [32, 52] because 32 is batch size and 52 is the fixed caption length we choose to create above in input dataset.\n",
        "\n",
        "        # Embedding\n",
        "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim) --> [32, 52, 512]\n",
        "\n",
        "        # Initialize LSTM state\n",
        "        h, c = self.init_hidden_state(encoder_out, px)  # (batch_size, decoder_dim) --> h & c of size [32, 512]\n",
        "\n",
        "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
        "        # So, decoding lengths are actual lengths - 1\n",
        "        # Let us say caption_lengths are tensor([20, 19, 19, 18, 16, 15, 14, 14, 13, 13, 13, 13, 13, 13, 13, 12, 12, 12, 12, 11, 11, 11, 10, 10, 10,  9,  9,  9,  8,  8,  8,  7],\n",
        "        # then decode_lengths will be           [19, 18, 18, 17, 15, 14, 13, 13, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 10, 10, 10,  9,  9,  9,  8,  8,  8,  7,  7,  7,  6]\n",
        "        decode_lengths = (caption_lengths - 1).tolist()\n",
        "\n",
        "        # Create tensors to hold word predicion scores and alphas\n",
        "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device) \n",
        "        # predictions are words --> an instance could be [32, 19, 2633]. Here 19 can change based on max caption_length in a particular batch.\n",
        "        # vocab_size -> 2633 -> len(word_map). Word_map we created early while creating input data \n",
        "\n",
        "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device) \n",
        "        # alphas are weights that decide which pixel to focus\n",
        "        # an instance could be [32, 19, 196]. Here 19 can change based on max caption_length in a particular batch.\n",
        "\n",
        "\n",
        "        # At each time-step, decode by\n",
        "        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n",
        "        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
        "        \n",
        "        # This 'for' loop is to ensure that we process images by decreasing caption lengths. This will help to avoid processing <pad>.\n",
        "        # Let us say decode_lengths = [19, 18, 18, 17, 15, 14, 13, 13, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 10, 10, 10, 9, 9, 9, 8, 8, 8, 7, 7, 7, 6]\n",
        "        # Here max(decode_lengths) = 19 so 'for' loop will execute from 0 to 18 & batch_size_t at each time-step 't' will be as follows:\n",
        "        '''\n",
        "        batch_size_t = sum([l > t for l in decode_lengths])\n",
        "        t = 0 till 5 , batch_size_t = 32, all 32 images processed & 32 outputs from previous step is used. Caption_length 19, 18, 18, 17, 15, 14, 13, 13, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 10, 10, 10, 9, 9, 9, 8, 8, 8, 7, 7, 7, 6\n",
        "        t = 6, batch_size_t = 31, only 31 images processed & 31 top outputs from previous step is used. Caption_length 19, 18, 18, 17, 15, 14, 13, 13, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 10, 10, 10, 9, 9, 9, 8, 8, 8, 7, 7, 7\n",
        "        t = 7, batch_size_t = 28, only 28 images processed & 23 top outputs from previous step is used. Caption_length 19, 18, 18, 17, 15, 14, 13, 13, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 10, 10, 10, 9, 9, 9, 8, 8, 8\n",
        "        t = 8, batch_size_t = 25, only 25 images processed & 25 top outputs from previous step is used. Caption_length 19, 18, 18, 17, 15, 14, 13, 13, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 10, 10, 10, 9, 9, 9\n",
        "        t = 9, batch_size_t = 22, only 22 images processed & 22 top outputs from previous step is used. Caption_length 19, 18, 18, 17, 15, 14, 13, 13, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 10, 10, 10\n",
        "        t = 10, batch_size_t = 19, only 19 images processed & 19 top outputs from previous step is used. Caption_length 19, 18, 18, 17, 15, 14, 13, 13, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11\n",
        "        t = 11, batch_size_t = 15, only 15 images processed & 15 top outputs from previous step is used. Caption_length 19, 18, 18, 17, 15, 14, 13, 13, 12, 12, 12, 12, 12, 12, 12\n",
        "        t = 12, batch_size_t = 8, only 8 images processed & 8 top outputs from previous step is used. Caption_length 19, 18, 18, 17, 15, 14, 13\n",
        "        t = 13, batch_size_t = 6, only 6 images processed & 6 top outputs from previous step is used. Caption_length 19, 18, 18, 17, 15, 14\n",
        "        t = 14, batch_size_t = 5, only 5 images processed & 5 top outputs from previous step is used. Caption_length 19, 18, 18, 17, 15\n",
        "        t = 15, batch_size_t = 4, only 4 images processed & 4 top outputs from previous step is used. Caption_length 19, 18, 18, 17\n",
        "        t = 16, batch_size_t = 4, only 4 images processed & 4 top outputs from previous step is used. Caption_length 19, 18, 18, 17\n",
        "        t = 17, batch_size_t = 3, only 3 images processed & 3 top outputs from previous step is used. Caption_length 19, 18, 18\n",
        "        t = 18, batch_size_t = 1, only 1 image processed & 1 top output from previous step is used. Caption_length 19\n",
        "        '''\n",
        "        for t in range(max(decode_lengths)):\n",
        "            \n",
        "            batch_size_t = sum([l > t for l in decode_lengths])\n",
        "          \n",
        "            '''\n",
        "                        attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
        "                                                                h[:batch_size_t], px) \n",
        "            Passing only those image encodings to attention network based on decreasing caption lengths\n",
        "            for example at t = 16, batch_size_t = 4 \n",
        "                           t = 17, batch_size_t = 3. Let us consider 17th time-step.\n",
        "            then encoder_out[:3] which means only 3 images out of 32 that are passed to attention on 17th step Which also means out of \n",
        "            [32, 196, 2048] only [3,196,2048] is passed to self.attention.\n",
        "            Also h[:3] means only first 3 entries from h belonging to 16th time-step will be passed on 17th step for attention processing.\n",
        "            which means out of [32, 512], only [3,512] is passed to self.attention\n",
        "            '''\n",
        "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
        "                                                                h[:batch_size_t], px) \n",
        "            \n",
        "            # for t = 17 in above example, attention_weighted_encoding.size() will be [3, 2048] and alpha will be [3, 196]\n",
        "            # Check Attention(nn.module) to get better sense of dimensions and operation\n",
        "\n",
        "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim) \n",
        "                            # for t = 17 in above example, \n",
        "                            # h[:batch_size_t]                             --> [3, 512], h --> [4, 512] bcoz t = 16 has batch_size_t = 4  \n",
        "                            # self.f_beta(h[:batch_size_t])                --> [3, 2048]\n",
        "                            # self.sigmoid(self.f_beta(h[:batch_size_t]))  --> [3, 2048]\n",
        "                            # This dimension change is to enable gating scalar multiplication with attention_weighted_encoding [3, 2048]\n",
        "            attention_weighted_encoding = gate * attention_weighted_encoding  # for t =17 in above example, [3, 2048]      \n",
        "            h, c = self.decode_step(\n",
        "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
        "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
        "            '''\n",
        "            for t = 17 in above example, \n",
        "               [embeddings[:batch_size_t, t, :] --> embeddings[:3, 17, :] --> [3, 512] where embeddings.size() is [32, 52, 512]\n",
        "               Refer pytorch slicing example given below for more clarity\n",
        "               :3        -> 0,1,2 out of 32 selected from [32,52,512]\n",
        "               17        -> Only 17th row out of 52 rows selected from [32,52,512]\n",
        "               :         -> All 512 columns selected from [32,52,512]\n",
        "               17, :     -> 17th row element for all 512 columns selected. This will give a tensor of size [1, 512]\n",
        "               :3, 17, : -> Means 3 such entries out of 32 is selected giving a tensor of size [3, 512]\n",
        "               torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1)\n",
        "                                                --> torch.cat([[3, 512], [3, 2048]], dim =1) --> [3, 2560]\n",
        "               h[:batch_size_t] that is fed to LSTM will be from t = 16. So h.size() will be [4, 512] bcoz t = 16 has batch_size_t = 4 &\n",
        "               h passed to LSTM in 17th time-step i.e. h[:batch_size_t] will be [3, 512] bcoz t = 17 has batch_size_t = 3.\n",
        "               \n",
        "               Similarly c[:batch_size_t] that is fed to LSTM will be from t = 16. So c.size() will be [4, 512] bcoz t = 16 has \n",
        "               batch_size_t = 4 & c passed to LSTM  in 17th time-step i.e. c[:batch_size_t] will be [3, 512] bcoz t = 17 has batch_size_t = 3.\n",
        "               \n",
        "               To summarize h,c = self.decode_step([3,2560], [3, 512], [3, 512])\n",
        "\n",
        "               self.decode_step is an LSTM cell that will accept weighted encodings, prev prediction from decoder (h) and prev cell state (c).\n",
        "               Output of LSTM cell is h and c.\n",
        "\n",
        "            '''   \n",
        "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size) \n",
        "            # preds are scores over vocabulary. \n",
        "            # for t = 17 in above example, h.size = [3, 512]. After passing through self.fc it will become [3, 2633]     \n",
        "            predictions[:batch_size_t, t, :] = preds\n",
        "            # Here predictions.size() that we created before 'for' loop is [32, 19, 2633]. \n",
        "            # Out of this [0:3, 19, 2633] is replaced by preds we got above\n",
        "            alphas[:batch_size_t, t, :] = alpha\n",
        "            # Here alphas.size() that we created before 'for' loop is [32, 19, 196]. \n",
        "            # Out of this [0:2, 19, 196] is replaced by alpha we got from self.attention\n",
        "            \n",
        "            #if px < 2:\n",
        "            #   print(' Decoder - Size of Predictions, alphas :', predictions.size(), alphas.size())\n",
        "        \n",
        "        #if px < 2:\n",
        "        #   print(' Iteration Ended:', px)\n",
        "        #   print('Size of Predictions, alphas :', predictions.size(), alphas.size())\n",
        "        \n",
        "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfC8KCh4HK-l"
      },
      "source": [
        "# Data parameters\n",
        "data_folder = '/content/data_output'                  # folder with data files saved by create_input_files.py...Create this MANUALLY in colab\n",
        "data_name = 'flickr8k_5_cap_per_img_5_min_word_freq'  # base name shared by data files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P55kTHd5KNBz"
      },
      "source": [
        "# Model parameters\n",
        "emb_dim = 512  # dimension of word embeddings\n",
        "attention_dim = 512  # dimension of attention linear layers\n",
        "decoder_dim = 512  # dimension of decoder RNN\n",
        "#encoder_dim = 512 # dimension from the resnet backbone\n",
        "dropout = 0.5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
        "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_YpEEzgTAlL"
      },
      "source": [
        "# Training parameters\n",
        "start_epoch = 0\n",
        "epochs = 3  # number of epochs to train for (if early stopping is not triggered)\n",
        "epochs_since_improvement = 0  # keeps track of number of epochs since there's been an improvement in validation BLEU\n",
        "batch_size = 32\n",
        "workers = 1  # for data-loading; right now, only 1 works with h5py\n",
        "encoder_lr = 1e-4  # learning rate for encoder if fine-tuning\n",
        "decoder_lr = 4e-4  # learning rate for decoder\n",
        "grad_clip = 5.  # clip gradients at an absolute value of\n",
        "alpha_c = 1.  # regularization parameter for 'doubly stochastic attention', as in the paper\n",
        "best_bleu4 = 0.  # BLEU-4 score right now\n",
        "print_freq = 100  # print training/validation stats every __ batches\n",
        "fine_tune_encoder = False  # fine-tune encoder?\n",
        "checkpoint = None  # path to checkpoint, None if none"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovVJnP4UTjEK"
      },
      "source": [
        "def train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch):\n",
        "    \"\"\"\n",
        "    Performs one epoch's training.\n",
        "    :param train_loader: DataLoader for training data\n",
        "    :param encoder: encoder model\n",
        "    :param decoder: decoder model\n",
        "    :param criterion: loss layer\n",
        "    :param encoder_optimizer: optimizer to update encoder's weights (if fine-tuning)\n",
        "    :param decoder_optimizer: optimizer to update decoder's weights\n",
        "    :param epoch: epoch number\n",
        "    \"\"\"  \n",
        "\n",
        "    decoder.train()  # train mode (dropout and batchnorm is used)\n",
        "    encoder.train()\n",
        "\n",
        "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
        "    data_time = AverageMeter()  # data loading time\n",
        "    losses = AverageMeter()  # loss (per word decoded)\n",
        "    top5accs = AverageMeter()  # top5 accuracy\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    print(' *** Epoch :',epoch)\n",
        "\n",
        "    # Batches\n",
        "    for i, (imgs, caps, caplens) in enumerate(train_loader):\n",
        "        data_time.update(time.time() - start)\n",
        "\n",
        "        # Move to GPU, if available\n",
        "        imgs = imgs.to(device)\n",
        "        caps = caps.to(device)\n",
        "        caplens = caplens.to(device)\n",
        "\n",
        "        # Forward prop.\n",
        "        imgs = encoder(imgs)\n",
        "        scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens, i)\n",
        "        # Output of decoder -> predictions [32, 20, 2633], encoded_captions [32,52], decode_lengths 32, alphas [32,20,196], sort_ind [32]  \n",
        "        # Here 20 could change based on max(decode_lengths)     \n",
        "        \n",
        "        # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
        "        targets = caps_sorted[:, 1:]  # caps_sorted.size -> [32, 52] This means there are 32 rows each having captions of length 52.\n",
        "                                      # So caps_sorted[:, 1:] will give us [32, 51] excluding the first column that represents <start>\n",
        "       \n",
        "        # Remove timesteps that we didn't decode at, or are pads\n",
        "        # pack_padded_sequence is an easy trick to do this. \n",
        "        \n",
        "        # Refer https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch\n",
        "        # Let us say decode_lengths = [22, 21, 19, 19, 18, 16, 14, 14, 13, 13, 13, 12, 12, 12, 12, 12, 12, 11, 11, 11, 10, 10, 10, 9, 9, 8, 8, 7, 7, 7, 6, 5]\n",
        "        # sum of decode_lengths = 383\n",
        "        # Before applying \"pack_padded_sequence\":scores.size() -> [32, 22, 2633], targets.size() -> [32, 51] \n",
        "        scores = pack_padded_sequence(scores, decode_lengths, batch_first=True).data    # ([32, 22, 2633], 383, batch_first=True )\n",
        "        targets = pack_padded_sequence(targets, decode_lengths, batch_first=True).data  # ([32, 51],       383, batch_first=True )\n",
        "        # After applying \"pack_padded_sequence\":\n",
        "        # scores.size() -> [383, 2633]  32*22 = 704 but not all 32 rows need 22 characters, many are pads. So after applying pack-padding,\n",
        "        # we reduced this 704 to 383. You can understand how it works better from stackoverflow link given above.\n",
        "        # targets.size()-> [383] 1632 reduced to 383 via pack-padding.\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(scores, targets)  # loss coming out from nn.CrossEntropyLoss criterion is a single value tensor\n",
        "        \n",
        "        # Add doubly stochastic attention regularization\n",
        "        loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
        "        \n",
        "        # Back prop.\n",
        "        decoder_optimizer.zero_grad()\n",
        "        if encoder_optimizer is not None:\n",
        "            encoder_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients\n",
        "        if grad_clip is not None:\n",
        "            clip_gradient(decoder_optimizer, grad_clip)\n",
        "            if encoder_optimizer is not None:\n",
        "                clip_gradient(encoder_optimizer, grad_clip)\n",
        "\n",
        "        # Update weights\n",
        "        decoder_optimizer.step()\n",
        "        if encoder_optimizer is not None:\n",
        "            encoder_optimizer.step()\n",
        "\n",
        "        # Keep track of metrics\n",
        "        top5 = accuracy(scores, targets, 5)\n",
        "        losses.update(loss.item(), sum(decode_lengths))\n",
        "        top5accs.update(top5, sum(decode_lengths))\n",
        "        batch_time.update(time.time() - start)\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        # Print status\n",
        "        if i % print_freq == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'.format(epoch, i, len(train_loader),\n",
        "                                                                          batch_time=batch_time,\n",
        "                                                                          data_time=data_time, loss=losses,\n",
        "                                                                          top5=top5accs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaLPD8-pj3Tb"
      },
      "source": [
        "## Slicing pytorch example\n",
        "#### torch.rand(3,3,2) --> Means 3 pages each having (4,2) tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzkLRPN_c4A8",
        "outputId": "a0d77233-75dd-458e-db43-4e16341cc4bb"
      },
      "source": [
        "b = torch.rand(3,4,2)\n",
        "print(b)\n",
        "b_ = b[:2, 3, :]  # :2 -> Means 3rd page omitted. Only 1st & 2nd page will be represented\n",
        "                  #  3 -> Means 3rd index row only selected out of 4 indexes i.e. [0,1,2,3]\n",
        "                  # : -> Means all available out of selected 2 columns [0,1] are taken\n",
        "print(b_)\n",
        "print(b.size(), b_.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0.4292, 0.9800],\n",
            "         [0.6874, 0.2835],\n",
            "         [0.3971, 0.9038],\n",
            "         [0.4445, 0.3100]],\n",
            "\n",
            "        [[0.9836, 0.5401],\n",
            "         [0.4187, 0.3589],\n",
            "         [0.1402, 0.5209],\n",
            "         [0.8631, 0.8726]],\n",
            "\n",
            "        [[0.2677, 0.5601],\n",
            "         [0.7570, 0.7538],\n",
            "         [0.1663, 0.0958],\n",
            "         [0.7970, 0.4050]]])\n",
            "tensor([[0.4445, 0.3100],\n",
            "        [0.8631, 0.8726]])\n",
            "torch.Size([3, 4, 2]) torch.Size([2, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwlTgKk9T7l1"
      },
      "source": [
        "def validate(val_loader, encoder, decoder, criterion):\n",
        "    \"\"\"\n",
        "    Performs one epoch's validation.\n",
        "    :param val_loader: DataLoader for validation data.\n",
        "    :param encoder: encoder model\n",
        "    :param decoder: decoder model\n",
        "    :param criterion: loss layer\n",
        "    :return: BLEU-4 score\n",
        "    \"\"\"\n",
        "    decoder.eval()  # eval mode (no dropout or batchnorm)\n",
        "    if encoder is not None:\n",
        "        encoder.eval()\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top5accs = AverageMeter()\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    references = list()  # references (true captions) for calculating BLEU-4 score\n",
        "    hypotheses = list()  # hypotheses (predictions)\n",
        "\n",
        "    # explicitly disable gradient calculation to avoid CUDA memory error\n",
        "    # solves the issue #57\n",
        "    with torch.no_grad():\n",
        "        # Batches\n",
        "        for i, (imgs, caps, caplens, allcaps) in enumerate(val_loader):\n",
        "            \n",
        "            # Move to device, if available\n",
        "            imgs = imgs.to(device)\n",
        "            caps = caps.to(device)\n",
        "            caplens = caplens.to(device)\n",
        "\n",
        "            # Forward prop.\n",
        "            if encoder is not None:\n",
        "                imgs = encoder(imgs)\n",
        "            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens, i)\n",
        "           # scores [32, 19, 2633], caps_sorted [32, 52], decode_lengths 32, alphas [32, 19, 196], sort_ind 32 \n",
        "           # Here 19 could change based on max(decode_lengths)\n",
        " \n",
        "            # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
        "            targets = caps_sorted[:, 1:] # [32, 51]\n",
        "\n",
        "            # Remove timesteps that we didn't decode at, or are pads\n",
        "            # pack_padded_sequence is an easy trick to do this\n",
        "            scores_copy = scores.clone()\n",
        "            scores = pack_padded_sequence(scores, decode_lengths, batch_first=True).data     # size -> [sum of decode_lengths, 2633]\n",
        "            targets = pack_padded_sequence(targets, decode_lengths, batch_first=True).data   # size -> [sum of decode_lengths]\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(scores, targets)\n",
        "\n",
        "            # Add doubly stochastic attention regularization\n",
        "            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
        "\n",
        "            # Keep track of metrics\n",
        "            losses.update(loss.item(), sum(decode_lengths))\n",
        "            top5 = accuracy(scores, targets, 5)\n",
        "            top5accs.update(top5, sum(decode_lengths))\n",
        "            batch_time.update(time.time() - start)\n",
        "\n",
        "            start = time.time()\n",
        "\n",
        "            if i % print_freq == 0:\n",
        "                print('Validation: [{0}/{1}]\\t'\n",
        "                      'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                      'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(i, len(val_loader), batch_time=batch_time,\n",
        "                                                                                loss=losses, top5=top5accs))\n",
        "\n",
        "            # Store references (true captions), and hypothesis (prediction) for each image\n",
        "            # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n",
        "            # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n",
        "\n",
        "            # References\n",
        "            allcaps = allcaps[sort_ind]  # because images were sorted in the decoder \n",
        "            # allcaps -> [32, 5, 52], there are 5 captions for each image. So 32 images in a batch, each having 5 captions of length 52                             \n",
        "\n",
        "            '''\n",
        "            Below logic is to remove <start> -> index 2631 & <pad> -> index 0 from allcaps.\n",
        "            Result is 'references' which is a list having 32 lists inside it. Each of the 32 lists will have 5 captions against it.\n",
        "            Eg. of 'references' as follows:\n",
        "            References need to be in this format for BLEU metrics.This evaluates a generated caption against reference caption(s). \n",
        "            For each generated caption, we will use all N_c captions (here 5) available for that image as the reference captions.\n",
        "\n",
        "            [[[7, 301, 825, 34, 717, 45, 246, 2, 301, 67, 918, 1477, 2632], \n",
        "              [1, 918, 301, 8, 7, 4, 602, 34, 2630, 374, 9, 301, 8, 2, 248, 4, 2251, 9, 45, 2632], \n",
        "              [14, 1090, 288, 37, 918, 92, 239, 99, 4, 404, 472, 9, 369, 2632], \n",
        "              [198, 274, 275, 117, 42, 1, 918, 92, 1, 2630, 306, 34, 714, 9, 45, 246, 107, 2632],  ---> 1st list\n",
        "              [610, 8, 2, 657, 2251, 918, 473, 246, 610, 8, 7, 657, 2632]], \n",
        "              \n",
        "              [[14, 256, 226, 59, 8, 1, 169, 2632], \n",
        "              [14, 256, 226, 180, 473, 8, 1, 169, 2632], \n",
        "              [14, 120, 226, 288, 59, 19, 198, 745, 169, 2632], \n",
        "              [9, 29, 46, 44, 29, 32, 288, 59, 212, 38, 212, 2632],                   ---> 2nd list\n",
        "              [1, 46, 8, 72, 44, 1, 32, 51, 288, 59, 13, 1, 24, 1454, 169, 2632]], \n",
        "              \n",
        "              ....\n",
        "\n",
        "              [[1, 89, 90, 1, 1214, 56, 44, 1, 1666, 27, 1053, 2632], \n",
        "               [9, 89, 42, 1, 2630, 1312, 350, 1, 927, 27, 1191, 8, 166, 343, 329, 2632], \n",
        "               [1, 89, 64, 1053, 350, 1, 927, 27, 1191, 44, 132, 88, 2632],                       ---> 32nd list\n",
        "               [1, 32, 42, 1015, 439, 67, 44, 2630, 67, 166, 329, 94, 13, 2496, 2632], \n",
        "               [9, 165, 8, 9, 1278, 590, 56, 48, 1, 1887, 67, 1, 7, 195, 8, 166, 329, 2632]]] \n",
        "\n",
        "            Length of 'references' keeps increasing by 32 with each iteration. If there are 100 iterations (i = 100), then progression of\n",
        "            'references' will be 0, 32, 64, 96,.....3200.\n",
        "            '''\n",
        "            \n",
        "            for j in range(allcaps.shape[0]):\n",
        "                img_caps = allcaps[j].tolist()\n",
        "                img_captions = list(\n",
        "                    map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<pad>']}],\n",
        "                        img_caps))  # remove <start> and pads                    \n",
        "                references.append(img_captions) \n",
        "\n",
        "            # Hypotheses\n",
        "            _, preds = torch.max(scores_copy, dim=2) \n",
        "            # Returns (value, indices). We are interested in indices here which is \"preds\"\n",
        "            # preds -> [32, 19]...We are taking index corresponding to maximum value out of 2633 for 18 timesteps belonging to each of 32 images. \n",
        "            # Remember that Scores_copy dimension in this example was -> [32, 19, 2633] & 19 can change based on max(decode_lengths)\n",
        "\n",
        "            '''\n",
        "            Let us say preds -> [32, 19]. Here 19 is the max(decode_length). However all the 32 entries won't be having 19 values.\n",
        "            Decode_lengths [19, 17, 17, 17, 16, 16, 15, 14, 14, 14, 13, 13, 13, 13, 12, 12, 12, 11, 11, 11, 10, 10, 10, 10, 9, 9, 9, 8, 8, 7, 7, 7]\n",
        "            From decode_lengths, it is evident that only 1st image has 19 caption length, 2nd, 3rd & 4th has 17 and so on..\n",
        "            So preds value will be as below for 1st (preds[0]), 2nd(preds[1]) & 5th image(preds[4]).\n",
        "            preds[0] -> [1, 99, 8, 1, 89, 2630, 1, 4, 2632, 1, 2630, 2632, 1, 366, 2632, 1, 2630, 1, 2632] -> 19 elements\n",
        "            preds[1] -> [ 1, 99, 4, 8, 1, 44, 67, 1, 7, 3, 7, 3, 4, 9, 169, 169, 2632, 0, 0] -> 19 elements but only 17 has values, 2 are padded with zeroes\n",
        "            preds[4] -> [ 1, 46, 8, 368, 1, 2630, 67, 1, 2632, 1, 2630, 2632, 9, 78, 2632, 2632, 0, 0, 0]\n",
        "                                                                          -> 19 elements but only 16 has values, rest 3 are padded with zeroes\n",
        "            Below logic removes pad i.e. removes 0s and write to temp_preds which will later be written to 'hypotheses'.\n",
        "            temp_preds for 1st, 2nd & 5th will look as below:\n",
        "            temp_preds = [[1, 99, 8, 1, 89, 2630, 1, 4, 2632, 1, 2630, 2632, 1, 366, 2632, 1, 2630, 1, 2632],\n",
        "                          [ 1, 99, 4, 8, 1, 44, 67, 1, 7, 3, 7, 3, 4, 9, 169, 169, 2632],\n",
        "                          ..\n",
        "                          ..\n",
        "                          [ 1, 46, 8, 368, 1, 2630, 67, 1, 2632, 1, 2630, 2632, 9, 78, 2632, 2632]\n",
        "                          ..\n",
        "                          ..]\n",
        "            Length of 'hypotheses' like 'references' before keeps increasing by 32 with each iteration. If there are 100 iterations (i = 100), \n",
        "            then progression of 'hypotheses' will be 0, 32, 64, 96,.....3200.              \n",
        "            '''\n",
        "            preds = preds.tolist()\n",
        "            temp_preds = list()\n",
        "            for j, p in enumerate(preds):\n",
        "                temp_preds.append(preds[j][:decode_lengths[j]])  # remove pads          \n",
        "            preds = temp_preds\n",
        "            hypotheses.extend(preds)\n",
        "            \n",
        "            assert len(references) == len(hypotheses)\n",
        "\n",
        "            #print('Validation -Epoch :', epoch, 'i :', i, 'len(references) : ', len(references), 'len(hypotheses) :', len(hypotheses))\n",
        "\n",
        "        # Calculate BLEU-4 scores. BLEU score is a metric designed for comparing \n",
        "        # naturally generated captions to ground-truth captions of differing length. \n",
        "        bleu4 = corpus_bleu(references, hypotheses)\n",
        "\n",
        "        print(\n",
        "            '\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}, BLEU-4 - {bleu}\\n'.format(\n",
        "                loss=losses,\n",
        "                top5=top5accs,\n",
        "                bleu=bleu4))\n",
        "\n",
        "    return bleu4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_i2ypp_UHwv"
      },
      "source": [
        "    \"\"\"\n",
        "    Training and validation.\n",
        "    \"\"\"\n",
        "\n",
        "    global best_bleu4, epochs_since_improvement, checkpoint, start_epoch, fine_tune_encoder, data_name, word_map\n",
        "    import time\n",
        "\n",
        "    # Read word map\n",
        "    word_map_file = os.path.join(data_folder, 'WORDMAP_' + data_name + '.json')\n",
        "    with open(word_map_file, 'r') as j:\n",
        "        word_map = json.load(j)\n",
        "\n",
        "    # Initialize / load checkpoint\n",
        "    if checkpoint is None:\n",
        "        print('checkpoint')\n",
        "        decoder = DecoderWithAttention(attention_dim=attention_dim,\n",
        "                                       embed_dim=emb_dim,\n",
        "                                       decoder_dim=decoder_dim,                                  \n",
        "                                       vocab_size=len(word_map),\n",
        "                                       dropout=dropout\n",
        "                                       )\n",
        "        decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n",
        "                                             lr=decoder_lr)\n",
        "        encoder = Encoder()\n",
        "        encoder.fine_tune(fine_tune_encoder)\n",
        "        encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
        "                                             lr=encoder_lr) if fine_tune_encoder else None\n",
        "\n",
        "    else:\n",
        "        checkpoint = torch.load(checkpoint)\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
        "        best_bleu4 = checkpoint['bleu-4']\n",
        "        decoder = checkpoint['decoder']\n",
        "        decoder_optimizer = checkpoint['decoder_optimizer']\n",
        "        encoder = checkpoint['encoder']\n",
        "        encoder_optimizer = checkpoint['encoder_optimizer']\n",
        "        if fine_tune_encoder is True and encoder_optimizer is None:\n",
        "            encoder.fine_tune(fine_tune_encoder)\n",
        "            encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
        "                                                 lr=encoder_lr)\n",
        "\n",
        "    # Move to GPU, if available\n",
        "    decoder = decoder.to(device)\n",
        "    encoder = encoder.to(device)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    # Custom dataloaders\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        CaptionDataset(data_folder, data_name, 'TRAIN', transform=transforms.Compose([normalize])),\n",
        "        batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        CaptionDataset(data_folder, data_name, 'VAL', transform=transforms.Compose([normalize])),\n",
        "        batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
        "\n",
        "    # Epochs\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "\n",
        "        # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n",
        "        if epochs_since_improvement == 20:\n",
        "            break\n",
        "        if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n",
        "            adjust_learning_rate(decoder_optimizer, 0.8)\n",
        "            if fine_tune_encoder:\n",
        "                adjust_learning_rate(encoder_optimizer, 0.8)\n",
        "\n",
        "        # One epoch's training\n",
        "        train(train_loader=train_loader,\n",
        "              encoder=encoder,\n",
        "              decoder=decoder,\n",
        "              criterion=criterion,\n",
        "              encoder_optimizer=encoder_optimizer,\n",
        "              decoder_optimizer=decoder_optimizer,\n",
        "              epoch=epoch)\n",
        "\n",
        "        # One epoch's validation\n",
        "        recent_bleu4 = validate(val_loader=val_loader,\n",
        "                                encoder=encoder,\n",
        "                                decoder=decoder,\n",
        "                                criterion=criterion)\n",
        "\n",
        "        # Check if there was an improvement\n",
        "        is_best = recent_bleu4 > best_bleu4\n",
        "        best_bleu4 = max(recent_bleu4, best_bleu4)\n",
        "        if not is_best:\n",
        "            epochs_since_improvement += 1\n",
        "            print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
        "        else:\n",
        "            epochs_since_improvement = 0\n",
        "\n",
        "        # Save checkpoint\n",
        "        save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer,\n",
        "                        decoder_optimizer, recent_bleu4, is_best)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYkUXWVbsn7T"
      },
      "source": [
        "#checkpoint = '/content/BEST_checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar'  # model checkpoint\n",
        "#checkpoint = '/content/gdrive/MyDrive/EVA4P2_S12_ImageCaptioning/BEST_checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pt'\n",
        "checkpoint = '/content/gdrive/MyDrive/EVA4P2_S12_ImageCaptioning/BEST_checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar'\n",
        "#word_map_file = '/content/data_output/WORDMAP_flickr8k_5_cap_per_img_5_min_word_freq.json'  # word map, ensure it's the same the data was encoded with and the model was trained with\n",
        "word_map_file = '/content/gdrive/MyDrive/EVA4P2_S12_ImageCaptioning/WORDMAP_flickr8k_5_cap_per_img_5_min_word_freq.json'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
        "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
        "\n",
        "# Load model\n",
        "checkpoint = torch.load(checkpoint)\n",
        "decoder = checkpoint['decoder']\n",
        "decoder = decoder.to(device)\n",
        "decoder.eval()\n",
        "encoder = checkpoint['encoder']\n",
        "encoder = encoder.to(device)\n",
        "encoder.eval()\n",
        "\n",
        "# Load word map (word2ix)\n",
        "with open(word_map_file, 'r') as j:\n",
        "    word_map = json.load(j)\n",
        "rev_word_map = {v: k for k, v in word_map.items()}\n",
        "vocab_size = len(word_map)\n",
        "\n",
        "# Normalization transform\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uhd1qPfuUyj3"
      },
      "source": [
        "def evaluate(beam_size):\n",
        "    \"\"\"\n",
        "    Evaluation\n",
        "    :param beam_size: beam size at which to generate captions for evaluation\n",
        "    :return: BLEU-4 score\n",
        "    \"\"\"\n",
        "    # DataLoader\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        CaptionDataset(data_folder, data_name, 'TEST', transform=transforms.Compose([normalize])),\n",
        "        batch_size=1, shuffle=True, num_workers=1, pin_memory=True)\n",
        "\n",
        "    # TODO: Batched Beam Search\n",
        "    # Therefore, do not use a batch_size greater than 1 - IMPORTANT!\n",
        "\n",
        "    # Lists to store references (true captions), and hypothesis (prediction) for each image\n",
        "    # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n",
        "    # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n",
        "    references = list()\n",
        "    hypotheses = list()\n",
        "\n",
        "    # For each image\n",
        "    for i, (image, caps, caplens, allcaps) in enumerate(\n",
        "            tqdm(loader, desc=\"EVALUATING AT BEAM SIZE \" + str(beam_size))):\n",
        "\n",
        "        k = beam_size  # All below comments will be for k = 3\n",
        "        if i < 2: \n",
        "           print('****** i ******* :', i, 'k :', k)\n",
        "\n",
        "        # Move to GPU device, if available\n",
        "        image = image.to(device)  # (1, 3, 256, 256)\n",
        "\n",
        "        # Encode\n",
        "        encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim) -> [1, 14, 14, 2048]\n",
        "        enc_image_size = encoder_out.size(1)  # 14\n",
        "        encoder_dim = encoder_out.size(3)     # 2048    \n",
        "\n",
        "        # Flatten encoding\n",
        "        encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim) -> [1, 196, 2048]\n",
        "        num_pixels = encoder_out.size(1) # 196\n",
        "\n",
        "        # We'll treat the problem as having a batch size of k. Here k = 3\n",
        "        encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim) \n",
        "        # torch.expand expands tensor. Let us say (1, 196, 512) and k=3, then torch.expand will expand it to (3, 196, 512)\n",
        "\n",
        "        '''\n",
        "        Tensor to store top k previous words at each step; now they're just <start>. These 'k_prev_words' will be converted as embeddings & fed to LSTM for next word predictions. \n",
        "        Below step is just initializing with <start> to give to LSTM for first time. \n",
        "        Later they will be populated based on LSTM predictions inside while loop as sequence progresses.\n",
        "        (k, 1) --> [3, 1]. Will have only 2631 at begining which belongs to <start>.\n",
        "        '''\n",
        "        k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  \n",
        "        \n",
        "        '''\n",
        "        Tensor to store top k sequences; now they're just <start>. Will have only [2631] at begining which belongs to <start>.\n",
        "        [[2631],\n",
        "         [2631],\n",
        "         [2631]]\n",
        "        '''\n",
        "        seqs = k_prev_words  # (k, 1) --> [3, 1]. \n",
        "\n",
        "        '''\n",
        "        Tensor to store top k sequences' scores; now they're just 0\n",
        "        [[0.],\n",
        "         [0.],\n",
        "         [0.]]\n",
        "        ''' \n",
        "        top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1) --> [3, 1]\n",
        "\n",
        "        # Lists to store completed sequences and scores\n",
        "        complete_seqs = list()\n",
        "        complete_seqs_scores = list()\n",
        "\n",
        "        # Start decoding\n",
        "        step = 1                                          # step will get incremented by 1 inside while loop till all the sequences hit <end>\n",
        "        h, c = decoder.init_hidden_state(encoder_out, i)  #[batch_size, decoder_dim], if k = 3 , then [3, 512]\n",
        "\n",
        "        # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
        "        while True:\n",
        "\n",
        "            embeddings = decoder.embedding(k_prev_words).squeeze(1)  \n",
        "            # (s, embed_dim) -> [3, 512]. We are converting k_prev_words(corresponding word_map indexes) to embeddings          \n",
        "\n",
        "            awe, _ = decoder.attention(encoder_out, h, i)  # (s, encoder_dim), (s, num_pixels). Passing image encoding & previous hidden state to attention.\n",
        "            # Attention will give attention weighted encoding(awe) & alpha(weights).Unlike training here we are interested only in (awe).                                                 \n",
        "\n",
        "            gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
        "            awe = gate * awe  # Passing the awe through sigmoid gate\n",
        "\n",
        "            h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  \n",
        "            # (s, decoder_dim). Calling LSTM. Inputs are embeddings of previously predicted words(k_prev_words), attention weighted encodings(awe), previous h & previous c\n",
        "\n",
        "            scores = decoder.fc(h)  # (s, vocab_size). Gets predictions in the form of scores by passing h through fc layer --> [3, 2633]         \n",
        "            scores = F.log_softmax(scores, dim=1)  # [3, 2633]\n",
        "            # Doing log_softmax on 'scores' across 1st dimension i.e. perform log_softmax operation of 2633 values to widen the gap among these values\n",
        "\n",
        "            # Add\n",
        "            scores = top_k_scores.expand_as(scores) + scores # (s, vocab_size) --> [3, 2633]\n",
        "            # expand_as --> means expand the 'top_k_scores' tensor same as size of 'scores' tensor.\n",
        "            # Here top_k_scores of size [3, 1] is expanded to size of 'scores' which is [3, 2633] and adding cumulatively. From this, top_k_scores are calculated below again.\n",
        "            # This is done to ensure that top scores are kept up-to-date based on latest predictions.        \n",
        "        \n",
        "\n",
        "            # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
        "            # torch.topk(input, k, dim=None, largest=True, sorted=True, *, out=None) -> (Tensor, LongTensor)\n",
        "            # .topk --> (values, indices) is returned, where the indices are the indices of the elements in the original input tensor.\n",
        "            #            values - k largest elements of the given input tensor along a given dimension. If dim is not given, the last dimension of the input is chosen.\n",
        "\n",
        "            if step == 1:\n",
        "                top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
        "                if i < 2:               \n",
        "                   print('  i, step :', i, step , 'top_k_scores, top_k_words :',  top_k_scores, top_k_words)\n",
        "            else:\n",
        "                # Unroll and find top scores, and their unrolled indices\n",
        "                top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  \n",
        "                # (s) --> selecting 'k=3' top-scores from 3 * 2633 flat tensor. \n",
        "                if i < 2:\n",
        "                   print('*****************')                   \n",
        "                   print('  i, step :', i, step , 'top_k_scores, top_k_words :', top_k_scores, top_k_words)\n",
        "                   \n",
        "            # Convert unrolled indices to actual indices of scores\n",
        "            # vocab_size -> 2633 i.e. there are 2633 words in vaocabulary. \n",
        "            # If an index comes as 2634, then that means it is 1st index in 2633. i.e. 2633 + 1 = 2634. This is captured in next_word_inds\n",
        "            # Let us say top_k_words = 2689, 5322,   91, then prev_word_inds = [1, 2, 0] and next_word_inds = [56 (2633+56), 56 (2633*2 +56), 91 (0 + 91)]\n",
        "            prev_word_inds = top_k_words / vocab_size  # (s) --> [3]\n",
        "            next_word_inds = top_k_words % vocab_size  # (s) --> [3]\n",
        "            if i < 2:\n",
        "               print('  i, step :', i, step , 'prev_word_inds :', prev_word_inds, 'next_word_inds :', next_word_inds)       \n",
        "               print('  i, step :', i, step , 'Before cat - seqs.size() :', seqs.size(), 'seqs before cat :', seqs)\n",
        " \n",
        "            '''\n",
        "            Add new words to sequences. Adding 'next_word_inds' that we got based on 'top_k_words' to already existing sequence. Example as below:\n",
        "            [[2631,    56, 99],\n",
        "             [2631,    56, 89],\n",
        "             [2631,    91, 10]]\n",
        "\n",
        "            Here [2631, 2631, 2631] is what we started with. We got [56, 56, 91] on step =1 & [99, 89, 10] on step =2 and so on....  \n",
        "            '''  \n",
        "            seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
        "      \n",
        "            if i < 2:\n",
        "               print('  i, step :', i, step , 'Before cat - seqs.size() :', seqs.size(), 'seqs After cat :', seqs)\n",
        "\n",
        "            '''\n",
        "            Which sequences are incomplete (didn't reach <end>)? Capturing indexes of incomplete sequences. eg: [0, 1, 2] for k =3 means all three sequences are incomplete\n",
        "            as any of them didnt hit <end> so far. If <end> i.e. 2632 is encountered that index is removed from sequence. Let us say '1' hit <end> , then \n",
        "            incomplete_inds will eliminate 1 and keep only [0,2]\n",
        "            '''\n",
        "            incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
        "                               next_word != word_map['<end>']]\n",
        "            if i < 2:\n",
        "               print('  i, step :', i, step ,  'incomplete_inds :', incomplete_inds)            \n",
        "\n",
        "            '''       \n",
        "            List to capture index of completed sequences i.e. sequences which reached <end>. aka 2632\n",
        "            Let us say out of [0, 1, 2], sequence 1 hit <end>. Then 'incomplete_inds' based on above logic will get updated as [0, 2] whereas 'next_word_inds' will\n",
        "            continue to have 3 elements in it. So 'set(range(len(next_word_inds)))' will be {0, 1, 2}. 'set(incomplete_inds)' will {0, 1}.\n",
        "            Difference between these two will exacly give the index of completed sequence ie 1 & will get captured in 'complete_inds' list as [1]\n",
        "            '''\n",
        "            complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds)) \n",
        "\n",
        "            if i < 2:\n",
        "               print('  i, step :', i, step , 'set(range(len(next_word_inds))) & set(incomplete_inds) :', set(range(len(next_word_inds))), set(incomplete_inds))\n",
        "               print('  i, step :', i, step , 'complete_inds :', complete_inds)               \n",
        "\n",
        "            # Set aside complete sequences\n",
        "            if len(complete_inds) > 0:\n",
        "                complete_seqs.extend(seqs[complete_inds].tolist()) # Fetches completed sequence from 'seqs' using 'complete_inds' index and adds it to 'complete_seqs'\n",
        "                complete_seqs_scores.extend(top_k_scores[complete_inds]) # Takes score of completed sequence from 'top_k_scores' and adds it to 'complete_seqs_scores'\n",
        "                if i < 2 : \n",
        "                   print('  i, step :', i, step , 'complete_seqs :', complete_seqs)\n",
        "                   print('  i, step :', i, step , 'complete_seqs_scores :', complete_seqs_scores)\n",
        "                   print('  i, step :', i, step , ' k before minus :', k)\n",
        "\n",
        "            k -= len(complete_inds)  # reduce beam length accordingly. If 'k =3' & one of the sequence hit <end>, then 'k' will be reduced by 1 to make 'k = 2'\n",
        "            if i < 2:\n",
        "               print('  i, step :', i, step , ' k after minus :', k)\n",
        "\n",
        "            # Proceed with incomplete sequences\n",
        "            if k == 0:\n",
        "                if i < 2:\n",
        "                   print('  i, step :', i, step ,'k reached zero , breaking ***')\n",
        "                break\n",
        "            \n",
        "            '''\n",
        "            Updating 'seqs' with 'incomplete_inds'. In otherwords, taking out completed sequences from 'seqs'. Similarly 'h' and 'c' also downsized.\n",
        "            Let us say size of 'seqs' was [3, 13]. This means there are 3 sequences and each sequence has 13 words captured so far.\n",
        "            Let us say all these 3 sequences are incomplete ie. [0, 1, 2] indexes of 'seqs' are incomplete. Now imagine 13th word for 1st index is <end> i.e. 2632.\n",
        "            Since it hit <end>, we will conside this index '1' as complete. Accordingly 'seqs' also needs to be updated as there is no point in further predicting words for \n",
        "            the sequence belonging to index 1 as it reached its <end>. Below step updates 'seqs\" by removing completed indexes, in this example 1. So new size of 'seqs'\n",
        "            will be [2, 13] and there will only be 2 sequences remaining for further processing as they didnt hit <end> so far.\n",
        "            Similarly we need to take out this completed sequence from 'h' & 'c' also as we dont need to feed it again to LSTM for prediction.\n",
        "            'h' will be updated from [3, 512] -> [2, 512]\n",
        "            'c' will be updated from [3, 512] -> [3, 512]\n",
        "            Also we need to remove this from encodings, top_k_scores, next_word_inds also. 'encoder_out' will be updated from [3, 196, 2048] -> [2, 196, 2048].\n",
        "            'top_k_scores' from [3] to [2]. 'next_word_inds' from [3] to [2].\n",
        "            Also we need to update 'k_prev_words' too. Because we are passing embeddings of 'k_prev_words' to LSTM to make predictions. So previous word that belongs to \n",
        "            completed sequence needs to be removed. We do this copying over 'next_word_inds' belonging to 'incomplete_inds [0, 2]' to 'k_prev_words' as below:\n",
        "            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
        "            Accordingly, size of 'k_prev_words' will be updated from [3,1] to [2,1]           \n",
        "            '''\n",
        "            if i < 2 : \n",
        "               print('  i, step :', i, step , 'Before slicing - seqs.size(), seqs:', seqs.size(), seqs)\n",
        "               print('  i, step :', i, step , 'Before slicing - h.size() :', h.size(), 'c.size() :', c.size())\n",
        "               print('  i, step :', i, step , 'Before slicing - encoder_out.size() :', encoder_out.size()) \n",
        "               print('  i, step :', i, step , 'Before slicing - top_k_scores.size() :', top_k_scores.size())\n",
        "               print('  i, step :', i, step , 'Before slicing - size() & k_prev_words :', k_prev_words.size(), k_prev_words)                \n",
        "\n",
        "            seqs = seqs[incomplete_inds]        \n",
        "            h    = h[prev_word_inds[incomplete_inds]]\n",
        "            c    = c[prev_word_inds[incomplete_inds]]\n",
        "            encoder_out  = encoder_out[prev_word_inds[incomplete_inds]]\n",
        "            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
        "            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
        "\n",
        "            if i < 2:\n",
        "               print('  i, step :', i, step , 'After slicing - seqs.size(), seqs:', seqs.size(), seqs)              \n",
        "               print('  i, step :', i, step , 'After slicing - h.size() :', h.size(), 'c.size() :', c.size())  \n",
        "               print('  i, step :', i, step , 'After slicing - encoder_out.size() :', encoder_out.size())\n",
        "               print('  i, step :', i, step , 'After slicing - top_k_scores.size() :', top_k_scores.size())\n",
        "               print('  i, step :', i, step , 'After slicing - size() & k_prev_words :', k_prev_words.size(), k_prev_words)\n",
        "              \n",
        "            # Break if things have been going on too long. Something should be wrong if we didn't hit <end> even after 50 steps. After all we are creating image captions\n",
        "            # here, not writing description of image.\n",
        "            if step > 50:\n",
        "                break\n",
        "            step += 1   # Incrementing step\n",
        "\n",
        "        # complete_seqs_scores      --> This List stores the scores belonging to each sequence for each iteration. Gets reset with each iteration also.\n",
        "        # max(complete_seqs_scores) --> After a particular iteration is completed, takes maximum from 'complete_seqs_scores'.\n",
        "        # j = complete_seqs_scores.index(max(complete_seqs_scores)) --> We are finding index of 'max(complete_seqs_scores)' and capturing it in 'j'.\n",
        " \n",
        "        j = complete_seqs_scores.index(max(complete_seqs_scores))     \n",
        "        if i < 2:\n",
        "           print('i :', i, ' Max Index - j :', j)\n",
        "        \n",
        "        # Using the 'max score index - j' take the sequence of that index from 'complete_seqs' and stores it in 'seq'. \n",
        "        # Please note that 'seq' will get overwritten with each iteration 'i'\n",
        "        seq = complete_seqs[j]    \n",
        "        if i < 2:\n",
        "           print('i :', i, 'seq :', seq)\n",
        "       \n",
        "                 \n",
        "        # References are captured. Same logic as we used in 'validation'. These are ground truths we got from input dataset. All 5 captions per image will be stored for bleu score.\n",
        "        img_caps = allcaps[0].tolist()\n",
        "        img_captions = list(\n",
        "            map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}],\n",
        "                img_caps))  # remove <start> and pads\n",
        "        references.append(img_captions)\n",
        "\n",
        "        # Hypotheses are captured using 'seq' we populated above. These are predictions we got from network.\n",
        "        hypotheses.append([w for w in seq if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}])\n",
        "\n",
        "        assert len(references) == len(hypotheses)\n",
        "\n",
        "    # Calculate BLEU-4 scores\n",
        "    bleu4 = corpus_bleu(references, hypotheses)\n",
        "\n",
        "    return bleu4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P40u7NqXi5K5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdf27094-e7c9-4ebe-a24f-ca8e67aab99f"
      },
      "source": [
        "beam_size = 3\n",
        "print(\"\\nBLEU-4 score @ beam size of %d is %.4f.\" % (beam_size, evaluate(beam_size)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEVALUATING AT BEAM SIZE 3:   0%|          | 0/5000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** i ******* : 0 k : 3\n",
            "  i, step : 0 1 top_k_scores, top_k_words : tensor([-0.2165, -2.4377, -4.0377], device='cuda:0', grad_fn=<TopkBackward>) tensor([ 1,  9, 14], device='cuda:0')\n",
            "  i, step : 0 1 prev_word_inds : tensor([0, 0, 0], device='cuda:0') next_word_inds : tensor([ 1,  9, 14], device='cuda:0')\n",
            "  i, step : 0 1 Before cat - seqs.size() : torch.Size([3, 1]) seqs before cat : tensor([[2631],\n",
            "        [2631],\n",
            "        [2631]], device='cuda:0')\n",
            "  i, step : 0 1 Before cat - seqs.size() : torch.Size([3, 2]) seqs After cat : tensor([[2631,    1],\n",
            "        [2631,    9],\n",
            "        [2631,   14]], device='cuda:0')\n",
            "  i, step : 0 1 incomplete_inds : [0, 1, 2]\n",
            "  i, step : 0 1 set(range(len(next_word_inds))) & set(incomplete_inds) : {0, 1, 2} {0, 1, 2}\n",
            "  i, step : 0 1 complete_inds : []\n",
            "  i, step : 0 1  k after minus : 3\n",
            "  i, step : 0 1 Before slicing - seqs.size(), seqs: torch.Size([3, 2]) tensor([[2631,    1],\n",
            "        [2631,    9],\n",
            "        [2631,   14]], device='cuda:0')\n",
            "  i, step : 0 1 Before slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 0 1 Before slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 0 1 Before slicing - top_k_scores.size() : torch.Size([3])\n",
            "  i, step : 0 1 Before slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[2631],\n",
            "        [2631],\n",
            "        [2631]], device='cuda:0')\n",
            "  i, step : 0 1 After slicing - seqs.size(), seqs: torch.Size([3, 2]) tensor([[2631,    1],\n",
            "        [2631,    9],\n",
            "        [2631,   14]], device='cuda:0')\n",
            "  i, step : 0 1 After slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 0 1 After slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 0 1 After slicing - top_k_scores.size() : torch.Size([3, 1])\n",
            "  i, step : 0 1 After slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[ 1],\n",
            "        [ 9],\n",
            "        [14]], device='cuda:0')\n",
            "*****************\n",
            "  i, step : 0 2 top_k_scores, top_k_words : tensor([-1.3222, -1.5804, -2.6353], device='cuda:0', grad_fn=<TopkBackward>) tensor([ 99, 584,  87], device='cuda:0')\n",
            "  i, step : 0 2 prev_word_inds : tensor([0, 0, 0], device='cuda:0') next_word_inds : tensor([ 99, 584,  87], device='cuda:0')\n",
            "  i, step : 0 2 Before cat - seqs.size() : torch.Size([3, 2]) seqs before cat : tensor([[2631,    1],\n",
            "        [2631,    9],\n",
            "        [2631,   14]], device='cuda:0')\n",
            "  i, step : 0 2 Before cat - seqs.size() : torch.Size([3, 3]) seqs After cat : tensor([[2631,    1,   99],\n",
            "        [2631,    1,  584],\n",
            "        [2631,    1,   87]], device='cuda:0')\n",
            "  i, step : 0 2 incomplete_inds : [0, 1, 2]\n",
            "  i, step : 0 2 set(range(len(next_word_inds))) & set(incomplete_inds) : {0, 1, 2} {0, 1, 2}\n",
            "  i, step : 0 2 complete_inds : []\n",
            "  i, step : 0 2  k after minus : 3\n",
            "  i, step : 0 2 Before slicing - seqs.size(), seqs: torch.Size([3, 3]) tensor([[2631,    1,   99],\n",
            "        [2631,    1,  584],\n",
            "        [2631,    1,   87]], device='cuda:0')\n",
            "  i, step : 0 2 Before slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 0 2 Before slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 0 2 Before slicing - top_k_scores.size() : torch.Size([3])\n",
            "  i, step : 0 2 Before slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[ 1],\n",
            "        [ 9],\n",
            "        [14]], device='cuda:0')\n",
            "  i, step : 0 2 After slicing - seqs.size(), seqs: torch.Size([3, 3]) tensor([[2631,    1,   99],\n",
            "        [2631,    1,  584],\n",
            "        [2631,    1,   87]], device='cuda:0')\n",
            "  i, step : 0 2 After slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 0 2 After slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 0 2 After slicing - top_k_scores.size() : torch.Size([3, 1])\n",
            "  i, step : 0 2 After slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[ 99],\n",
            "        [584],\n",
            "        [ 87]], device='cuda:0')\n",
            "*****************\n",
            "  i, step : 0 3 top_k_scores, top_k_words : tensor([-3.1585, -3.2368, -3.6158], device='cuda:0', grad_fn=<TopkBackward>) tensor([   8,    4, 2637], device='cuda:0')\n",
            "  i, step : 0 3 prev_word_inds : tensor([0, 0, 1], device='cuda:0') next_word_inds : tensor([8, 4, 4], device='cuda:0')\n",
            "  i, step : 0 3 Before cat - seqs.size() : torch.Size([3, 3]) seqs before cat : tensor([[2631,    1,   99],\n",
            "        [2631,    1,  584],\n",
            "        [2631,    1,   87]], device='cuda:0')\n",
            "  i, step : 0 3 Before cat - seqs.size() : torch.Size([3, 4]) seqs After cat : tensor([[2631,    1,   99,    8],\n",
            "        [2631,    1,   99,    4],\n",
            "        [2631,    1,  584,    4]], device='cuda:0')\n",
            "  i, step : 0 3 incomplete_inds : [0, 1, 2]\n",
            "  i, step : 0 3 set(range(len(next_word_inds))) & set(incomplete_inds) : {0, 1, 2} {0, 1, 2}\n",
            "  i, step : 0 3 complete_inds : []\n",
            "  i, step : 0 3  k after minus : 3\n",
            "  i, step : 0 3 Before slicing - seqs.size(), seqs: torch.Size([3, 4]) tensor([[2631,    1,   99,    8],\n",
            "        [2631,    1,   99,    4],\n",
            "        [2631,    1,  584,    4]], device='cuda:0')\n",
            "  i, step : 0 3 Before slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 0 3 Before slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 0 3 Before slicing - top_k_scores.size() : torch.Size([3])\n",
            "  i, step : 0 3 Before slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[ 99],\n",
            "        [584],\n",
            "        [ 87]], device='cuda:0')\n",
            "  i, step : 0 3 After slicing - seqs.size(), seqs: torch.Size([3, 4]) tensor([[2631,    1,   99,    8],\n",
            "        [2631,    1,   99,    4],\n",
            "        [2631,    1,  584,    4]], device='cuda:0')\n",
            "  i, step : 0 3 After slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 0 3 After slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 0 3 After slicing - top_k_scores.size() : torch.Size([3, 1])\n",
            "  i, step : 0 3 After slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[8],\n",
            "        [4],\n",
            "        [4]], device='cuda:0')\n",
            "*****************\n",
            "  i, step : 0 4 top_k_scores, top_k_words : tensor([-3.5668, -5.4006, -5.7144], device='cuda:0', grad_fn=<TopkBackward>) tensor([   1, 5634, 2781], device='cuda:0')\n",
            "  i, step : 0 4 prev_word_inds : tensor([0, 2, 1], device='cuda:0') next_word_inds : tensor([  1, 368, 148], device='cuda:0')\n",
            "  i, step : 0 4 Before cat - seqs.size() : torch.Size([3, 4]) seqs before cat : tensor([[2631,    1,   99,    8],\n",
            "        [2631,    1,   99,    4],\n",
            "        [2631,    1,  584,    4]], device='cuda:0')\n",
            "  i, step : 0 4 Before cat - seqs.size() : torch.Size([3, 5]) seqs After cat : tensor([[2631,    1,   99,    8,    1],\n",
            "        [2631,    1,  584,    4,  368],\n",
            "        [2631,    1,   99,    4,  148]], device='cuda:0')\n",
            "  i, step : 0 4 incomplete_inds : [0, 1, 2]\n",
            "  i, step : 0 4 set(range(len(next_word_inds))) & set(incomplete_inds) : {0, 1, 2} {0, 1, 2}\n",
            "  i, step : 0 4 complete_inds : []\n",
            "  i, step : 0 4  k after minus : 3\n",
            "  i, step : 0 4 Before slicing - seqs.size(), seqs: torch.Size([3, 5]) tensor([[2631,    1,   99,    8,    1],\n",
            "        [2631,    1,  584,    4,  368],\n",
            "        [2631,    1,   99,    4,  148]], device='cuda:0')\n",
            "  i, step : 0 4 Before slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 0 4 Before slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 0 4 Before slicing - top_k_scores.size() : torch.Size([3])\n",
            "  i, step : 0 4 Before slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[8],\n",
            "        [4],\n",
            "        [4]], device='cuda:0')\n",
            "  i, step : 0 4 After slicing - seqs.size(), seqs: torch.Size([3, 5]) tensor([[2631,    1,   99,    8,    1],\n",
            "        [2631,    1,  584,    4,  368],\n",
            "        [2631,    1,   99,    4,  148]], device='cuda:0')\n",
            "  i, step : 0 4 After slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 0 4 After slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 0 4 After slicing - top_k_scores.size() : torch.Size([3, 1])\n",
            "  i, step : 0 4 After slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[  1],\n",
            "        [368],\n",
            "        [148]], device='cuda:0')\n",
            "*****************\n",
            "  i, step : 0 5 top_k_scores, top_k_words : tensor([-5.1937, -5.5679, -5.6440], device='cuda:0', grad_fn=<TopkBackward>) tensor([ 91, 133,  72], device='cuda:0')\n",
            "  i, step : 0 5 prev_word_inds : tensor([0, 0, 0], device='cuda:0') next_word_inds : tensor([ 91, 133,  72], device='cuda:0')\n",
            "  i, step : 0 5 Before cat - seqs.size() : torch.Size([3, 5]) seqs before cat : tensor([[2631,    1,   99,    8,    1],\n",
            "        [2631,    1,  584,    4,  368],\n",
            "        [2631,    1,   99,    4,  148]], device='cuda:0')\n",
            "  i, step : 0 5 Before cat - seqs.size() : torch.Size([3, 6]) seqs After cat : tensor([[2631,    1,   99,    8,    1,   91],\n",
            "        [2631,    1,   99,    8,    1,  133],\n",
            "        [2631,    1,   99,    8,    1,   72]], device='cuda:0')\n",
            "  i, step : 0 5 incomplete_inds : [0, 1, 2]\n",
            "  i, step : 0 5 set(range(len(next_word_inds))) & set(incomplete_inds) : {0, 1, 2} {0, 1, 2}\n",
            "  i, step : 0 5 complete_inds : []\n",
            "  i, step : 0 5  k after minus : 3\n",
            "  i, step : 0 5 Before slicing - seqs.size(), seqs: torch.Size([3, 6]) tensor([[2631,    1,   99,    8,    1,   91],\n",
            "        [2631,    1,   99,    8,    1,  133],\n",
            "        [2631,    1,   99,    8,    1,   72]], device='cuda:0')\n",
            "  i, step : 0 5 Before slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 0 5 Before slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 0 5 Before slicing - top_k_scores.size() : torch.Size([3])\n",
            "  i, step : 0 5 Before slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[  1],\n",
            "        [368],\n",
            "        [148]], device='cuda:0')\n",
            "  i, step : 0 5 After slicing - seqs.size(), seqs: torch.Size([3, 6]) tensor([[2631,    1,   99,    8,    1,   91],\n",
            "        [2631,    1,   99,    8,    1,  133],\n",
            "        [2631,    1,   99,    8,    1,   72]], device='cuda:0')\n",
            "  i, step : 0 5 After slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 0 5 After slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 0 5 After slicing - top_k_scores.size() : torch.Size([3, 1])\n",
            "  i, step : 0 5 After slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[ 91],\n",
            "        [133],\n",
            "        [ 72]], device='cuda:0')\n",
            "*****************\n",
            "  i, step : 0 6 top_k_scores, top_k_words : tensor([-5.7129, -6.1770, -6.2468], device='cuda:0', grad_fn=<TopkBackward>) tensor([  56, 2689, 5322], device='cuda:0')\n",
            "  i, step : 0 6 prev_word_inds : tensor([0, 1, 2], device='cuda:0') next_word_inds : tensor([56, 56, 56], device='cuda:0')\n",
            "  i, step : 0 6 Before cat - seqs.size() : torch.Size([3, 6]) seqs before cat : tensor([[2631,    1,   99,    8,    1,   91],\n",
            "        [2631,    1,   99,    8,    1,  133],\n",
            "        [2631,    1,   99,    8,    1,   72]], device='cuda:0')\n",
            "  i, step : 0 6 Before cat - seqs.size() : torch.Size([3, 7]) seqs After cat : tensor([[2631,    1,   99,    8,    1,   91,   56],\n",
            "        [2631,    1,   99,    8,    1,  133,   56],\n",
            "        [2631,    1,   99,    8,    1,   72,   56]], device='cuda:0')\n",
            "  i, step : 0 6 incomplete_inds : [0, 1, 2]\n",
            "  i, step : 0 6 set(range(len(next_word_inds))) & set(incomplete_inds) : {0, 1, 2} {0, 1, 2}\n",
            "  i, step : 0 6 complete_inds : []\n",
            "  i, step : 0 6  k after minus : 3\n",
            "  i, step : 0 6 Before slicing - seqs.size(), seqs: torch.Size([3, 7]) tensor([[2631,    1,   99,    8,    1,   91,   56],\n",
            "        [2631,    1,   99,    8,    1,  133,   56],\n",
            "        [2631,    1,   99,    8,    1,   72,   56]], device='cuda:0')\n",
            "  i, step : 0 6 Before slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 0 6 Before slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 0 6 Before slicing - top_k_scores.size() : torch.Size([3])\n",
            "  i, step : 0 6 Before slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[ 91],\n",
            "        [133],\n",
            "        [ 72]], device='cuda:0')\n",
            "  i, step : 0 6 After slicing - seqs.size(), seqs: torch.Size([3, 7]) tensor([[2631,    1,   99,    8,    1,   91,   56],\n",
            "        [2631,    1,   99,    8,    1,  133,   56],\n",
            "        [2631,    1,   99,    8,    1,   72,   56]], device='cuda:0')\n",
            "  i, step : 0 6 After slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 0 6 After slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 0 6 After slicing - top_k_scores.size() : torch.Size([3, 1])\n",
            "  i, step : 0 6 After slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[56],\n",
            "        [56],\n",
            "        [56]], device='cuda:0')\n",
            "*****************\n",
            "  i, step : 0 7 top_k_scores, top_k_words : tensor([-6.7012, -7.0832, -7.3562], device='cuda:0', grad_fn=<TopkBackward>) tensor([   4, 2637, 5270], device='cuda:0')\n",
            "  i, step : 0 7 prev_word_inds : tensor([0, 1, 2], device='cuda:0') next_word_inds : tensor([4, 4, 4], device='cuda:0')\n",
            "  i, step : 0 7 Before cat - seqs.size() : torch.Size([3, 7]) seqs before cat : tensor([[2631,    1,   99,    8,    1,   91,   56],\n",
            "        [2631,    1,   99,    8,    1,  133,   56],\n",
            "        [2631,    1,   99,    8,    1,   72,   56]], device='cuda:0')\n",
            "  i, step : 0 7 Before cat - seqs.size() : torch.Size([3, 8]) seqs After cat : tensor([[2631,    1,   99,    8,    1,   91,   56,    4],\n",
            "        [2631,    1,   99,    8,    1,  133,   56,    4],\n",
            "        [2631,    1,   99,    8,    1,   72,   56,    4]], device='cuda:0')\n",
            "  i, step : 0 7 incomplete_inds : [0, 1, 2]\n",
            "  i, step : 0 7 set(range(len(next_word_inds))) & set(incomplete_inds) : {0, 1, 2} {0, 1, 2}\n",
            "  i, step : 0 7 complete_inds : []\n",
            "  i, step : 0 7  k after minus : 3\n",
            "  i, step : 0 7 Before slicing - seqs.size(), seqs: torch.Size([3, 8]) tensor([[2631,    1,   99,    8,    1,   91,   56,    4],\n",
            "        [2631,    1,   99,    8,    1,  133,   56,    4],\n",
            "        [2631,    1,   99,    8,    1,   72,   56,    4]], device='cuda:0')\n",
            "  i, step : 0 7 Before slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 0 7 Before slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 0 7 Before slicing - top_k_scores.size() : torch.Size([3])\n",
            "  i, step : 0 7 Before slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[56],\n",
            "        [56],\n",
            "        [56]], device='cuda:0')\n",
            "  i, step : 0 7 After slicing - seqs.size(), seqs: torch.Size([3, 8]) tensor([[2631,    1,   99,    8,    1,   91,   56,    4],\n",
            "        [2631,    1,   99,    8,    1,  133,   56,    4],\n",
            "        [2631,    1,   99,    8,    1,   72,   56,    4]], device='cuda:0')\n",
            "  i, step : 0 7 After slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 0 7 After slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 0 7 After slicing - top_k_scores.size() : torch.Size([3, 1])\n",
            "  i, step : 0 7 After slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[4],\n",
            "        [4],\n",
            "        [4]], device='cuda:0')\n",
            "*****************\n",
            "  i, step : 0 8 top_k_scores, top_k_words : tensor([-8.8498, -9.2912, -9.4017], device='cuda:0', grad_fn=<TopkBackward>) tensor([ 368, 3001, 5634], device='cuda:0')\n",
            "  i, step : 0 8 prev_word_inds : tensor([0, 1, 2], device='cuda:0') next_word_inds : tensor([368, 368, 368], device='cuda:0')\n",
            "  i, step : 0 8 Before cat - seqs.size() : torch.Size([3, 8]) seqs before cat : tensor([[2631,    1,   99,    8,    1,   91,   56,    4],\n",
            "        [2631,    1,   99,    8,    1,  133,   56,    4],\n",
            "        [2631,    1,   99,    8,    1,   72,   56,    4]], device='cuda:0')\n",
            "  i, step : 0 8 Before cat - seqs.size() : torch.Size([3, 9]) seqs After cat : tensor([[2631,    1,   99,    8,    1,   91,   56,    4,  368],\n",
            "        [2631,    1,   99,    8,    1,  133,   56,    4,  368],\n",
            "        [2631,    1,   99,    8,    1,   72,   56,    4,  368]],\n",
            "       device='cuda:0')\n",
            "  i, step : 0 8 incomplete_inds : [0, 1, 2]\n",
            "  i, step : 0 8 set(range(len(next_word_inds))) & set(incomplete_inds) : {0, 1, 2} {0, 1, 2}\n",
            "  i, step : 0 8 complete_inds : []\n",
            "  i, step : 0 8  k after minus : 3\n",
            "  i, step : 0 8 Before slicing - seqs.size(), seqs: torch.Size([3, 9]) tensor([[2631,    1,   99,    8,    1,   91,   56,    4,  368],\n",
            "        [2631,    1,   99,    8,    1,  133,   56,    4,  368],\n",
            "        [2631,    1,   99,    8,    1,   72,   56,    4,  368]],\n",
            "       device='cuda:0')\n",
            "  i, step : 0 8 Before slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 0 8 Before slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 0 8 Before slicing - top_k_scores.size() : torch.Size([3])\n",
            "  i, step : 0 8 Before slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[4],\n",
            "        [4],\n",
            "        [4]], device='cuda:0')\n",
            "  i, step : 0 8 After slicing - seqs.size(), seqs: torch.Size([3, 9]) tensor([[2631,    1,   99,    8,    1,   91,   56,    4,  368],\n",
            "        [2631,    1,   99,    8,    1,  133,   56,    4,  368],\n",
            "        [2631,    1,   99,    8,    1,   72,   56,    4,  368]],\n",
            "       device='cuda:0')\n",
            "  i, step : 0 8 After slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 0 8 After slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 0 8 After slicing - top_k_scores.size() : torch.Size([3, 1])\n",
            "  i, step : 0 8 After slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[368],\n",
            "        [368],\n",
            "        [368]], device='cuda:0')\n",
            "*****************\n",
            "  i, step : 0 9 top_k_scores, top_k_words : tensor([-10.3975, -10.8266, -10.8556], device='cuda:0', grad_fn=<TopkBackward>) tensor([  67,  158, 2700], device='cuda:0')\n",
            "  i, step : 0 9 prev_word_inds : tensor([0, 0, 1], device='cuda:0') next_word_inds : tensor([ 67, 158,  67], device='cuda:0')\n",
            "  i, step : 0 9 Before cat - seqs.size() : torch.Size([3, 9]) seqs before cat : "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEVALUATING AT BEAM SIZE 3:   0%|          | 1/5000 [00:00<51:28,  1.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([[2631,    1,   99,    8,    1,   91,   56,    4,  368],\n",
            "        [2631,    1,   99,    8,    1,  133,   56,    4,  368],\n",
            "        [2631,    1,   99,    8,    1,   72,   56,    4,  368]],\n",
            "       device='cuda:0')\n",
            "  i, step : 0 9 Before cat - seqs.size() : torch.Size([3, 10]) seqs After cat : tensor([[2631,    1,   99,    8,    1,   91,   56,    4,  368,   67],\n",
            "        [2631,    1,   99,    8,    1,   91,   56,    4,  368,  158],\n",
            "        [2631,    1,   99,    8,    1,  133,   56,    4,  368,   67]],\n",
            "       device='cuda:0')\n",
            "  i, step : 0 9 incomplete_inds : [0, 1, 2]\n",
            "  i, step : 0 9 set(range(len(next_word_inds))) & set(incomplete_inds) : {0, 1, 2} {0, 1, 2}\n",
            "  i, step : 0 9 complete_inds : []\n",
            "  i, step : 0 9  k after minus : 3\n",
            "  i, step : 0 9 Before slicing - seqs.size(), seqs: torch.Size([3, 10]) tensor([[2631,    1,   99,    8,    1,   91,   56,    4,  368,   67],\n",
            "        [2631,    1,   99,    8,    1,   91,   56,    4,  368,  158],\n",
            "        [2631,    1,   99,    8,    1,  133,   56,    4,  368,   67]],\n",
            "       device='cuda:0')\n",
            "  i, step : 0 9 Before slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 0 9 Before slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 0 9 Before slicing - top_k_scores.size() : torch.Size([3])\n",
            "  i, step : 0 9 Before slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[368],\n",
            "        [368],\n",
            "        [368]], device='cuda:0')\n",
            "  i, step : 0 9 After slicing - seqs.size(), seqs: torch.Size([3, 10]) tensor([[2631,    1,   99,    8,    1,   91,   56,    4,  368,   67],\n",
            "        [2631,    1,   99,    8,    1,   91,   56,    4,  368,  158],\n",
            "        [2631,    1,   99,    8,    1,  133,   56,    4,  368,   67]],\n",
            "       device='cuda:0')\n",
            "  i, step : 0 9 After slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 0 9 After slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 0 9 After slicing - top_k_scores.size() : torch.Size([3, 1])\n",
            "  i, step : 0 9 After slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[ 67],\n",
            "        [158],\n",
            "        [ 67]], device='cuda:0')\n",
            "*****************\n",
            "  i, step : 0 10 top_k_scores, top_k_words : tensor([-10.6017, -11.0304, -11.0645], device='cuda:0', grad_fn=<TopkBackward>) tensor([   1, 2634, 5267], device='cuda:0')\n",
            "  i, step : 0 10 prev_word_inds : tensor([0, 1, 2], device='cuda:0') next_word_inds : tensor([1, 1, 1], device='cuda:0')\n",
            "  i, step : 0 10 Before cat - seqs.size() : torch.Size([3, 10]) seqs before cat : tensor([[2631,    1,   99,    8,    1,   91,   56,    4,  368,   67],\n",
            "        [2631,    1,   99,    8,    1,   91,   56,    4,  368,  158],\n",
            "        [2631,    1,   99,    8,    1,  133,   56,    4,  368,   67]],\n",
            "       device='cuda:0')\n",
            "  i, step : 0 10 Before cat - seqs.size() : torch.Size([3, 11]) seqs After cat : tensor([[2631,    1,   99,    8,    1,   91,   56,    4,  368,   67,    1],\n",
            "        [2631,    1,   99,    8,    1,   91,   56,    4,  368,  158,    1],\n",
            "        [2631,    1,   99,    8,    1,  133,   56,    4,  368,   67,    1]],\n",
            "       device='cuda:0')\n",
            "  i, step : 0 10 incomplete_inds : [0, 1, 2]\n",
            "  i, step : 0 10 set(range(len(next_word_inds))) & set(incomplete_inds) : {0, 1, 2} {0, 1, 2}\n",
            "  i, step : 0 10 complete_inds : []\n",
            "  i, step : 0 10  k after minus : 3\n",
            "  i, step : 0 10 Before slicing - seqs.size(), seqs: torch.Size([3, 11]) tensor([[2631,    1,   99,    8,    1,   91,   56,    4,  368,   67,    1],\n",
            "        [2631,    1,   99,    8,    1,   91,   56,    4,  368,  158,    1],\n",
            "        [2631,    1,   99,    8,    1,  133,   56,    4,  368,   67,    1]],\n",
            "       device='cuda:0')\n",
            "  i, step : 0 10 Before slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 0 10 Before slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 0 10 Before slicing - top_k_scores.size() : torch.Size([3])\n",
            "  i, step : 0 10 Before slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[ 67],\n",
            "        [158],\n",
            "        [ 67]], device='cuda:0')\n",
            "  i, step : 0 10 After slicing - seqs.size(), seqs: torch.Size([3, 11]) tensor([[2631,    1,   99,    8,    1,   91,   56,    4,  368,   67,    1],\n",
            "        [2631,    1,   99,    8,    1,   91,   56,    4,  368,  158,    1],\n",
            "        [2631,    1,   99,    8,    1,  133,   56,    4,  368,   67,    1]],\n",
            "       device='cuda:0')\n",
            "  i, step : 0 10 After slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 0 10 After slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 0 10 After slicing - top_k_scores.size() : torch.Size([3, 1])\n",
            "  i, step : 0 10 After slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[1],\n",
            "        [1],\n",
            "        [1]], device='cuda:0')\n",
            "*****************\n",
            "  i, step : 0 11 top_k_scores, top_k_words : tensor([-12.4347, -12.7513, -12.8729], device='cuda:0', grad_fn=<TopkBackward>) tensor([ 354, 2987, 5620], device='cuda:0')\n",
            "  i, step : 0 11 prev_word_inds : tensor([0, 1, 2], device='cuda:0') next_word_inds : tensor([354, 354, 354], device='cuda:0')\n",
            "  i, step : 0 11 Before cat - seqs.size() : torch.Size([3, 11]) seqs before cat : tensor([[2631,    1,   99,    8,    1,   91,   56,    4,  368,   67,    1],\n",
            "        [2631,    1,   99,    8,    1,   91,   56,    4,  368,  158,    1],\n",
            "        [2631,    1,   99,    8,    1,  133,   56,    4,  368,   67,    1]],\n",
            "       device='cuda:0')\n",
            "  i, step : 0 11 Before cat - seqs.size() : torch.Size([3, 12]) seqs After cat : tensor([[2631,    1,   99,    8,    1,   91,   56,    4,  368,   67,    1,  354],\n",
            "        [2631,    1,   99,    8,    1,   91,   56,    4,  368,  158,    1,  354],\n",
            "        [2631,    1,   99,    8,    1,  133,   56,    4,  368,   67,    1,  354]],\n",
            "       device='cuda:0')\n",
            "  i, step : 0 11 incomplete_inds : [0, 1, 2]\n",
            "  i, step : 0 11 set(range(len(next_word_inds))) & set(incomplete_inds) : {0, 1, 2} {0, 1, 2}\n",
            "  i, step : 0 11 complete_inds : []\n",
            "  i, step : 0 11  k after minus : 3\n",
            "  i, step : 0 11 Before slicing - seqs.size(), seqs: torch.Size([3, 12]) tensor([[2631,    1,   99,    8,    1,   91,   56,    4,  368,   67,    1,  354],\n",
            "        [2631,    1,   99,    8,    1,   91,   56,    4,  368,  158,    1,  354],\n",
            "        [2631,    1,   99,    8,    1,  133,   56,    4,  368,   67,    1,  354]],\n",
            "       device='cuda:0')\n",
            "  i, step : 0 11 Before slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 0 11 Before slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 0 11 Before slicing - top_k_scores.size() : torch.Size([3])\n",
            "  i, step : 0 11 Before slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[1],\n",
            "        [1],\n",
            "        [1]], device='cuda:0')\n",
            "  i, step : 0 11 After slicing - seqs.size(), seqs: torch.Size([3, 12]) tensor([[2631,    1,   99,    8,    1,   91,   56,    4,  368,   67,    1,  354],\n",
            "        [2631,    1,   99,    8,    1,   91,   56,    4,  368,  158,    1,  354],\n",
            "        [2631,    1,   99,    8,    1,  133,   56,    4,  368,   67,    1,  354]],\n",
            "       device='cuda:0')\n",
            "  i, step : 0 11 After slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 0 11 After slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 0 11 After slicing - top_k_scores.size() : torch.Size([3, 1])\n",
            "  i, step : 0 11 After slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[354],\n",
            "        [354],\n",
            "        [354]], device='cuda:0')\n",
            "*****************\n",
            "  i, step : 0 12 top_k_scores, top_k_words : tensor([-12.9422, -13.2754, -13.3660], device='cuda:0', grad_fn=<TopkBackward>) tensor([2632, 5265, 7898], device='cuda:0')\n",
            "  i, step : 0 12 prev_word_inds : tensor([0, 1, 2], device='cuda:0') next_word_inds : tensor([2632, 2632, 2632], device='cuda:0')\n",
            "  i, step : 0 12 Before cat - seqs.size() : torch.Size([3, 12]) seqs before cat : tensor([[2631,    1,   99,    8,    1,   91,   56,    4,  368,   67,    1,  354],\n",
            "        [2631,    1,   99,    8,    1,   91,   56,    4,  368,  158,    1,  354],\n",
            "        [2631,    1,   99,    8,    1,  133,   56,    4,  368,   67,    1,  354]],\n",
            "       device='cuda:0')\n",
            "  i, step : 0 12 Before cat - seqs.size() : torch.Size([3, 13]) seqs After cat : tensor([[2631,    1,   99,    8,    1,   91,   56,    4,  368,   67,    1,  354,\n",
            "         2632],\n",
            "        [2631,    1,   99,    8,    1,   91,   56,    4,  368,  158,    1,  354,\n",
            "         2632],\n",
            "        [2631,    1,   99,    8,    1,  133,   56,    4,  368,   67,    1,  354,\n",
            "         2632]], device='cuda:0')\n",
            "  i, step : 0 12 incomplete_inds : []\n",
            "  i, step : 0 12 set(range(len(next_word_inds))) & set(incomplete_inds) : {0, 1, 2} set()\n",
            "  i, step : 0 12 complete_inds : [0, 1, 2]\n",
            "  i, step : 0 12 complete_seqs : [[2631, 1, 99, 8, 1, 91, 56, 4, 368, 67, 1, 354, 2632], [2631, 1, 99, 8, 1, 91, 56, 4, 368, 158, 1, 354, 2632], [2631, 1, 99, 8, 1, 133, 56, 4, 368, 67, 1, 354, 2632]]\n",
            "  i, step : 0 12 complete_seqs_scores : [tensor(-12.9422, device='cuda:0', grad_fn=<SelectBackward>), tensor(-13.2754, device='cuda:0', grad_fn=<SelectBackward>), tensor(-13.3660, device='cuda:0', grad_fn=<SelectBackward>)]\n",
            "  i, step : 0 12  k before minus : 3\n",
            "  i, step : 0 12  k after minus : 0\n",
            "  i, step : 0 12 k reached zero , breaking ***\n",
            "i : 0  Max Index - j : 0\n",
            "i : 0 seq : [2631, 1, 99, 8, 1, 91, 56, 4, 368, 67, 1, 354, 2632]\n",
            "****** i ******* : 1 k : 3\n",
            "  i, step : 1 1 top_k_scores, top_k_words : tensor([-0.2896, -2.1558, -2.9235], device='cuda:0', grad_fn=<TopkBackward>) tensor([ 1, 14,  9], device='cuda:0')\n",
            "  i, step : 1 1 prev_word_inds : tensor([0, 0, 0], device='cuda:0') next_word_inds : tensor([ 1, 14,  9], device='cuda:0')\n",
            "  i, step : 1 1 Before cat - seqs.size() : torch.Size([3, 1]) seqs before cat : tensor([[2631],\n",
            "        [2631],\n",
            "        [2631]], device='cuda:0')\n",
            "  i, step : 1 1 Before cat - seqs.size() : torch.Size([3, 2]) seqs After cat : tensor([[2631,    1],\n",
            "        [2631,   14],\n",
            "        [2631,    9]], device='cuda:0')\n",
            "  i, step : 1 1 incomplete_inds : [0, 1, 2]\n",
            "  i, step : 1 1 set(range(len(next_word_inds))) & set(incomplete_inds) : {0, 1, 2} {0, 1, 2}\n",
            "  i, step : 1 1 complete_inds : []\n",
            "  i, step : 1 1  k after minus : 3\n",
            "  i, step : 1 1 Before slicing - seqs.size(), seqs: torch.Size([3, 2]) tensor([[2631,    1],\n",
            "        [2631,   14],\n",
            "        [2631,    9]], device='cuda:0')\n",
            "  i, step : 1 1 Before slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 1 1 Before slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 1 1 Before slicing - top_k_scores.size() : torch.Size([3])\n",
            "  i, step : 1 1 Before slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[2631],\n",
            "        [2631],\n",
            "        [2631]], device='cuda:0')\n",
            "  i, step : 1 1 After slicing - seqs.size(), seqs: torch.Size([3, 2]) tensor([[2631,    1],\n",
            "        [2631,   14],\n",
            "        [2631,    9]], device='cuda:0')\n",
            "  i, step : 1 1 After slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 1 1 After slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 1 1 After slicing - top_k_scores.size() : torch.Size([3, 1])\n",
            "  i, step : 1 1 After slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[ 1],\n",
            "        [14],\n",
            "        [ 9]], device='cuda:0')\n",
            "*****************\n",
            "  i, step : 1 2 top_k_scores, top_k_words : tensor([-1.1663, -2.2254, -2.3012], device='cuda:0', grad_fn=<TopkBackward>) tensor([ 3,  2, 12], device='cuda:0')\n",
            "  i, step : 1 2 prev_word_inds : tensor([0, 0, 0], device='cuda:0') next_word_inds : tensor([ 3,  2, 12], device='cuda:0')\n",
            "  i, step : 1 2 Before cat - seqs.size() : torch.Size([3, 2]) seqs before cat : tensor([[2631,    1],\n",
            "        [2631,   14],\n",
            "        [2631,    9]], device='cuda:0')\n",
            "  i, step : 1 2 Before cat - seqs.size() : torch.Size([3, 3]) seqs After cat : tensor([[2631,    1,    3],\n",
            "        [2631,    1,    2],\n",
            "        [2631,    1,   12]], device='cuda:0')\n",
            "  i, step : 1 2 incomplete_inds : [0, 1, 2]\n",
            "  i, step : 1 2 set(range(len(next_word_inds))) & set(incomplete_inds) : {0, 1, 2} {0, 1, 2}\n",
            "  i, step : 1 2 complete_inds : []\n",
            "  i, step : 1 2  k after minus : 3\n",
            "  i, step : 1 2 Before slicing - seqs.size(), seqs: torch.Size([3, 3]) tensor([[2631,    1,    3],\n",
            "        [2631,    1,    2],\n",
            "        [2631,    1,   12]], device='cuda:0')\n",
            "  i, step : 1 2 Before slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 1 2 Before slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 1 2 Before slicing - top_k_scores.size() : torch.Size([3])\n",
            "  i, step : 1 2 Before slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[ 1],\n",
            "        [14],\n",
            "        [ 9]], device='cuda:0')\n",
            "  i, step : 1 2 After slicing - seqs.size(), seqs: torch.Size([3, 3]) tensor([[2631,    1,    3],\n",
            "        [2631,    1,    2],\n",
            "        [2631,    1,   12]], device='cuda:0')\n",
            "  i, step : 1 2 After slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 1 2 After slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 1 2 After slicing - top_k_scores.size() : torch.Size([3, 1])\n",
            "  i, step : 1 2 After slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[ 3],\n",
            "        [ 2],\n",
            "        [12]], device='cuda:0')\n",
            "*****************\n",
            "  i, step : 1 3 top_k_scores, top_k_words : tensor([-2.2673, -2.3361, -3.0318], device='cuda:0', grad_fn=<TopkBackward>) tensor([2636, 5269,  117], device='cuda:0')\n",
            "  i, step : 1 3 prev_word_inds : tensor([1, 2, 0], device='cuda:0') next_word_inds : tensor([  3,   3, 117], device='cuda:0')\n",
            "  i, step : 1 3 Before cat - seqs.size() : torch.Size([3, 3]) seqs before cat : tensor([[2631,    1,    3],\n",
            "        [2631,    1,    2],\n",
            "        [2631,    1,   12]], device='cuda:0')\n",
            "  i, step : 1 3 Before cat - seqs.size() : torch.Size([3, 4]) seqs After cat : tensor([[2631,    1,    2,    3],\n",
            "        [2631,    1,   12,    3],\n",
            "        [2631,    1,    3,  117]], device='cuda:0')\n",
            "  i, step : 1 3 incomplete_inds : [0, 1, 2]\n",
            "  i, step : 1 3 set(range(len(next_word_inds))) & set(incomplete_inds) : {0, 1, 2} {0, 1, 2}\n",
            "  i, step : 1 3 complete_inds : []\n",
            "  i, step : 1 3  k after minus : 3\n",
            "  i, step : 1 3 Before slicing - seqs.size(), seqs: torch.Size([3, 4]) tensor([[2631,    1,    2,    3],\n",
            "        [2631,    1,   12,    3],\n",
            "        [2631,    1,    3,  117]], device='cuda:0')\n",
            "  i, step : 1 3 Before slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 1 3 Before slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 1 3 Before slicing - top_k_scores.size() : torch.Size([3])\n",
            "  i, step : 1 3 Before slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[ 3],\n",
            "        [ 2],\n",
            "        [12]], device='cuda:0')\n",
            "  i, step : 1 3 After slicing - seqs.size(), seqs: torch.Size([3, 4]) tensor([[2631,    1,    2,    3],\n",
            "        [2631,    1,   12,    3],\n",
            "        [2631,    1,    3,  117]], device='cuda:0')\n",
            "  i, step : 1 3 After slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 1 3 After slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 1 3 After slicing - top_k_scores.size() : torch.Size([3, 1])\n",
            "  i, step : 1 3 After slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[  3],\n",
            "        [  3],\n",
            "        [117]], device='cuda:0')\n",
            "*****************\n",
            "  i, step : 1 4 top_k_scores, top_k_words : tensor([-3.8216, -3.8343, -3.9512], device='cuda:0', grad_fn=<TopkBackward>) tensor([2637,    4, 5279], device='cuda:0')\n",
            "  i, step : 1 4 prev_word_inds : tensor([1, 0, 2], device='cuda:0') next_word_inds : tensor([ 4,  4, 13], device='cuda:0')\n",
            "  i, step : 1 4 Before cat - seqs.size() : torch.Size([3, 4]) seqs before cat : tensor([[2631,    1,    2,    3],\n",
            "        [2631,    1,   12,    3],\n",
            "        [2631,    1,    3,  117]], device='cuda:0')\n",
            "  i, step : 1 4 Before cat - seqs.size() : torch.Size([3, 5]) seqs After cat : tensor([[2631,    1,   12,    3,    4],\n",
            "        [2631,    1,    2,    3,    4],\n",
            "        [2631,    1,    3,  117,   13]], device='cuda:0')\n",
            "  i, step : 1 4 incomplete_inds : [0, 1, 2]\n",
            "  i, step : 1 4 set(range(len(next_word_inds))) & set(incomplete_inds) : {0, 1, 2} {0, 1, 2}\n",
            "  i, step : 1 4 complete_inds : []\n",
            "  i, step : 1 4  k after minus : 3\n",
            "  i, step : 1 4 Before slicing - seqs.size(), seqs: torch.Size([3, 5]) tensor([[2631,    1,   12,    3,    4],\n",
            "        [2631,    1,    2,    3,    4],\n",
            "        [2631,    1,    3,  117,   13]], device='cuda:0')\n",
            "  i, step : 1 4 Before slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 1 4 Before slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 1 4 Before slicing - top_k_scores.size() : torch.Size([3])\n",
            "  i, step : 1 4 Before slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[  3],\n",
            "        [  3],\n",
            "        [117]], device='cuda:0')\n",
            "  i, step : 1 4 After slicing - seqs.size(), seqs: torch.Size([3, 5]) tensor([[2631,    1,   12,    3,    4],\n",
            "        [2631,    1,    2,    3,    4],\n",
            "        [2631,    1,    3,  117,   13]], device='cuda:0')\n",
            "  i, step : 1 4 After slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 1 4 After slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 1 4 After slicing - top_k_scores.size() : torch.Size([3, 1])\n",
            "  i, step : 1 4 After slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[ 4],\n",
            "        [ 4],\n",
            "        [13]], device='cuda:0')\n",
            "*****************\n",
            "  i, step : 1 5 top_k_scores, top_k_words : tensor([-4.7033, -4.8102, -4.9031], device='cuda:0', grad_fn=<TopkBackward>) tensor([5267, 5275, 2638], device='cuda:0')\n",
            "  i, step : 1 5 prev_word_inds : tensor([2, 2, 1], device='cuda:0') next_word_inds : tensor([1, 9, 5], device='cuda:0')\n",
            "  i, step : 1 5 Before cat - seqs.size() : torch.Size([3, 5]) seqs before cat : tensor([[2631,    1,   12,    3,    4],\n",
            "        [2631,    1,    2,    3,    4],\n",
            "        [2631,    1,    3,  117,   13]], device='cuda:0')\n",
            "  i, step : 1 5 Before cat - seqs.size() : torch.Size([3, 6]) seqs After cat : tensor([[2631,    1,    3,  117,   13,    1],\n",
            "        [2631,    1,    3,  117,   13,    9],\n",
            "        [2631,    1,    2,    3,    4,    5]], device='cuda:0')\n",
            "  i, step : 1 5 incomplete_inds : [0, 1, 2]\n",
            "  i, step : 1 5 set(range(len(next_word_inds))) & set(incomplete_inds) : {0, 1, 2} {0, 1, 2}\n",
            "  i, step : 1 5 complete_inds : []\n",
            "  i, step : 1 5  k after minus : 3\n",
            "  i, step : 1 5 Before slicing - seqs.size(), seqs: torch.Size([3, 6]) tensor([[2631,    1,    3,  117,   13,    1],\n",
            "        [2631,    1,    3,  117,   13,    9],\n",
            "        [2631,    1,    2,    3,    4,    5]], device='cuda:0')\n",
            "  i, step : 1 5 Before slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 1 5 Before slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 1 5 Before slicing - top_k_scores.size() : torch.Size([3])\n",
            "  i, step : 1 5 Before slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[ 4],\n",
            "        [ 4],\n",
            "        [13]], device='cuda:0')\n",
            "  i, step : 1 5 After slicing - seqs.size(), seqs: torch.Size([3, 6]) tensor([[2631,    1,    3,  117,   13,    1],\n",
            "        [2631,    1,    3,  117,   13,    9],\n",
            "        [2631,    1,    2,    3,    4,    5]], device='cuda:0')\n",
            "  i, step : 1 5 After slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 1 5 After slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 1 5 After slicing - top_k_scores.size() : torch.Size([3, 1])\n",
            "  i, step : 1 5 After slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[1],\n",
            "        [9],\n",
            "        [5]], device='cuda:0')\n",
            "*****************\n",
            "  i, step : 1 6 top_k_scores, top_k_words : tensor([-5.5293, -5.9082, -6.5020], device='cuda:0', grad_fn=<TopkBackward>) tensor([3031, 5279,  392], device='cuda:0')\n",
            "  i, step : 1 6 prev_word_inds : tensor([1, 2, 0], device='cuda:0') next_word_inds : tensor([398,  13, 392], device='cuda:0')\n",
            "  i, step : 1 6 Before cat - seqs.size() : torch.Size([3, 6]) seqs before cat : tensor([[2631,    1,    3,  117,   13,    1],\n",
            "        [2631,    1,    3,  117,   13,    9],\n",
            "        [2631,    1,    2,    3,    4,    5]], device='cuda:0')\n",
            "  i, step : 1 6 Before cat - seqs.size() : torch.Size([3, 7]) seqs After cat : tensor([[2631,    1,    3,  117,   13,    9,  398],\n",
            "        [2631,    1,    2,    3,    4,    5,   13],\n",
            "        [2631,    1,    3,  117,   13,    1,  392]], device='cuda:0')\n",
            "  i, step : 1 6 incomplete_inds : [0, 1, 2]\n",
            "  i, step : 1 6 set(range(len(next_word_inds))) & set(incomplete_inds) : {0, 1, 2} {0, 1, 2}\n",
            "  i, step : 1 6 complete_inds : []\n",
            "  i, step : 1 6  k after minus : 3\n",
            "  i, step : 1 6 Before slicing - seqs.size(), seqs: torch.Size([3, 7]) tensor([[2631,    1,    3,  117,   13,    9,  398],\n",
            "        [2631,    1,    2,    3,    4,    5,   13],\n",
            "        [2631,    1,    3,  117,   13,    1,  392]], device='cuda:0')\n",
            "  i, step : 1 6 Before slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 1 6 Before slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 1 6 Before slicing - top_k_scores.size() : torch.Size([3])\n",
            "  i, step : 1 6 Before slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[1],\n",
            "        [9],\n",
            "        [5]], device='cuda:0')\n",
            "  i, step : 1 6 After slicing - seqs.size(), seqs: torch.Size([3, 7]) tensor([[2631,    1,    3,  117,   13,    9,  398],\n",
            "        [2631,    1,    2,    3,    4,    5,   13],\n",
            "        [2631,    1,    3,  117,   13,    1,  392]], device='cuda:0')\n",
            "  i, step : 1 6 After slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 1 6 After slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 1 6 After slicing - top_k_scores.size() : torch.Size([3, 1])\n",
            "  i, step : 1 6 After slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[398],\n",
            "        [ 13],\n",
            "        [392]], device='cuda:0')\n",
            "*****************\n",
            "  i, step : 1 7 top_k_scores, top_k_words : tensor([-5.6654, -6.5182, -6.6516], device='cuda:0', grad_fn=<TopkBackward>) tensor([2632, 2634, 5488], device='cuda:0')\n",
            "  i, step : 1 7 prev_word_inds : tensor([0, 1, 2], device='cuda:0') next_word_inds : tensor([2632,    1,  222], device='cuda:0')\n",
            "  i, step : 1 7 Before cat - seqs.size() : torch.Size([3, 7]) seqs before cat : tensor([[2631,    1,    3,  117,   13,    9,  398],\n",
            "        [2631,    1,    2,    3,    4,    5,   13],\n",
            "        [2631,    1,    3,  117,   13,    1,  392]], device='cuda:0')\n",
            "  i, step : 1 7 Before cat - seqs.size() : torch.Size([3, 8]) seqs After cat : tensor([[2631,    1,    3,  117,   13,    9,  398, 2632],\n",
            "        [2631,    1,    2,    3,    4,    5,   13,    1],\n",
            "        [2631,    1,    3,  117,   13,    1,  392,  222]], device='cuda:0')\n",
            "  i, step : 1 7 incomplete_inds : [1, 2]\n",
            "  i, step : 1 7 set(range(len(next_word_inds))) & set(incomplete_inds) : {0, 1, 2} {1, 2}\n",
            "  i, step : 1 7 complete_inds : [0]\n",
            "  i, step : 1 7 complete_seqs : [[2631, 1, 3, 117, 13, 9, 398, 2632]]\n",
            "  i, step : 1 7 complete_seqs_scores : [tensor(-5.6654, device='cuda:0', grad_fn=<SelectBackward>)]\n",
            "  i, step : 1 7  k before minus : 3\n",
            "  i, step : 1 7  k after minus : 2\n",
            "  i, step : 1 7 Before slicing - seqs.size(), seqs: torch.Size([3, 8]) tensor([[2631,    1,    3,  117,   13,    9,  398, 2632],\n",
            "        [2631,    1,    2,    3,    4,    5,   13,    1],\n",
            "        [2631,    1,    3,  117,   13,    1,  392,  222]], device='cuda:0')\n",
            "  i, step : 1 7 Before slicing - h.size() : torch.Size([3, 512]) c.size() : torch.Size([3, 512])\n",
            "  i, step : 1 7 Before slicing - encoder_out.size() : torch.Size([3, 196, 2048])\n",
            "  i, step : 1 7 Before slicing - top_k_scores.size() : torch.Size([3])\n",
            "  i, step : 1 7 Before slicing - size() & k_prev_words : torch.Size([3, 1]) tensor([[398],\n",
            "        [ 13],\n",
            "        [392]], device='cuda:0')\n",
            "  i, step : 1 7 After slicing - seqs.size(), seqs: torch.Size([2, 8]) tensor([[2631,    1,    2,    3,    4,    5,   13,    1],\n",
            "        [2631,    1,    3,  117,   13,    1,  392,  222]], device='cuda:0')\n",
            "  i, step : 1 7 After slicing - h.size() : torch.Size([2, 512]) c.size() : torch.Size([2, 512])\n",
            "  i, step : 1 7 After slicing - encoder_out.size() : torch.Size([2, 196, 2048])\n",
            "  i, step : 1 7 After slicing - top_k_scores.size() : torch.Size([2, 1])\n",
            "  i, step : 1 7 After slicing - size() & k_prev_words : torch.Size([2, 1]) tensor([[  1],\n",
            "        [222]], device='cuda:0')\n",
            "*****************\n",
            "  i, step : 1 8 top_k_scores, top_k_words : tensor([-6.8259, -8.2292], device='cuda:0', grad_fn=<TopkBackward>) tensor([5265,  392], device='cuda:0')\n",
            "  i, step : 1 8 prev_word_inds : tensor([1, 0], device='cuda:0') next_word_inds : tensor([2632,  392], device='cuda:0')\n",
            "  i, step : 1 8 Before cat - seqs.size() : torch.Size([2, 8]) seqs before cat : tensor([[2631,    1,    2,    3,    4,    5,   13,    1],\n",
            "        [2631,    1,    3,  117,   13,    1,  392,  222]], device='cuda:0')\n",
            "  i, step : 1 8 Before cat - seqs.size() : torch.Size([2, 9]) seqs After cat : tensor([[2631,    1,    3,  117,   13,    1,  392,  222, 2632],\n",
            "        [2631,    1,    2,    3,    4,    5,   13,    1,  392]],\n",
            "       device='cuda:0')\n",
            "  i, step : 1 8 incomplete_inds : [1]\n",
            "  i, step : 1 8 set(range(len(next_word_inds))) & set(incomplete_inds) : {0, 1} {1}\n",
            "  i, step : 1 8 complete_inds : [0]\n",
            "  i, step : 1 8 complete_seqs : [[2631, 1, 3, 117, 13, 9, 398, 2632], [2631, 1, 3, 117, 13, 1, 392, 222, 2632]]\n",
            "  i, step : 1 8 complete_seqs_scores : [tensor(-5.6654, device='cuda:0', grad_fn=<SelectBackward>), tensor(-6.8259, device='cuda:0', grad_fn=<SelectBackward>)]\n",
            "  i, step : 1 8  k before minus : 2\n",
            "  i, step : 1 8  k after minus : 1\n",
            "  i, step : 1 8 Before slicing - seqs.size(), seqs: torch.Size([2, 9]) tensor([[2631,    1,    3,  117,   13,    1,  392,  222, 2632],\n",
            "        [2631,    1,    2,    3,    4,    5,   13,    1,  392]],\n",
            "       device='cuda:0')\n",
            "  i, step : 1 8 Before slicing - h.size() : torch.Size([2, 512]) c.size() : torch.Size([2, 512])\n",
            "  i, step : 1 8 Before slicing - encoder_out.size() : torch.Size([2, 196, 2048])\n",
            "  i, step : 1 8 Before slicing - top_k_scores.size() : torch.Size([2])\n",
            "  i, step : 1 8 Before slicing - size() & k_prev_words : torch.Size([2, 1]) tensor([[  1],\n",
            "        [222]], device='cuda:0')\n",
            "  i, step : 1 8 After slicing - seqs.size(), seqs: torch.Size([1, 9]) tensor([[2631,    1,    2,    3,    4,    5,   13,    1,  392]],\n",
            "       device='cuda:0')\n",
            "  i, step : 1 8 After slicing - h.size() : torch.Size([1, 512]) c.size() : torch.Size([1, 512])\n",
            "  i, step : 1 8 After slicing - encoder_out.size() : torch.Size([1, 196, 2048])\n",
            "  i, step : 1 8 After slicing - top_k_scores.size() : torch.Size([1, 1])\n",
            "  i, step : 1 8 After slicing - size() & k_prev_words : torch.Size([1, 1]) tensor([[392]], device='cuda:0')\n",
            "*****************\n",
            "  i, step : 1 9 top_k_scores, top_k_words : tensor([-8.3846], device='cuda:0', grad_fn=<TopkBackward>) tensor([222], device='cuda:0')\n",
            "  i, step : 1 9 prev_word_inds : tensor([0], device='cuda:0') next_word_inds : tensor([222], device='cuda:0')\n",
            "  i, step : 1 9 Before cat - seqs.size() : torch.Size([1, 9]) seqs before cat : tensor([[2631,    1,    2,    3,    4,    5,   13,    1,  392]],\n",
            "       device='cuda:0')\n",
            "  i, step : 1 9 Before cat - seqs.size() : torch.Size([1, 10]) seqs After cat : tensor([[2631,    1,    2,    3,    4,    5,   13,    1,  392,  222]],\n",
            "       device='cuda:0')\n",
            "  i, step : 1 9 incomplete_inds : [0]\n",
            "  i, step : 1 9 set(range(len(next_word_inds))) & set(incomplete_inds) : {0} {0}\n",
            "  i, step : 1 9 complete_inds : []\n",
            "  i, step : 1 9  k after minus : 1\n",
            "  i, step : 1 9 Before slicing - seqs.size(), seqs: torch.Size([1, 10]) tensor([[2631,    1,    2,    3,    4,    5,   13,    1,  392,  222]],\n",
            "       device='cuda:0')\n",
            "  i, step : 1 9 Before slicing - h.size() : torch.Size([1, 512]) c.size() : torch.Size([1, 512])\n",
            "  i, step : 1 9 Before slicing - encoder_out.size() : torch.Size([1, 196, 2048])\n",
            "  i, step : 1 9 Before slicing - top_k_scores.size() : torch.Size([1])\n",
            "  i, step : 1 9 Before slicing - size() & k_prev_words : torch.Size([1, 1]) tensor([[392]], device='cuda:0')\n",
            "  i, step : 1 9 After slicing - seqs.size(), seqs: torch.Size([1, 10]) tensor([[2631,    1,    2,    3,    4,    5,   13,    1,  392,  222]],\n",
            "       device='cuda:0')\n",
            "  i, step : 1 9 After slicing - h.size() : torch.Size([1, 512]) c.size() : torch.Size([1, 512])\n",
            "  i, step : 1 9 After slicing - encoder_out.size() : torch.Size([1, 196, 2048])\n",
            "  i, step : 1 9 After slicing - top_k_scores.size() : torch.Size([1, 1])\n",
            "  i, step : 1 9 After slicing - size() & k_prev_words : torch.Size([1, 1]) tensor([[222]], device='cuda:0')\n",
            "*****************\n",
            "  i, step : 1 10 top_k_scores, top_k_words : tensor([-8.5411], device='cuda:0', grad_fn=<TopkBackward>) tensor([2632], device='cuda:0')\n",
            "  i, step : 1 10 prev_word_inds : tensor([0], device='cuda:0') next_word_inds : tensor([2632], device='cuda:0')\n",
            "  i, step : 1 10 Before cat - seqs.size() : torch.Size([1, 10]) seqs before cat : tensor([[2631,    1,    2,    3,    4,    5,   13,    1,  392,  222]],\n",
            "       device='cuda:0')\n",
            "  i, step : 1 10 Before cat - seqs.size() : torch.Size([1, 11]) seqs After cat : "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATING AT BEAM SIZE 3:   0%|          | 4/5000 [00:01<35:06,  2.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([[2631,    1,    2,    3,    4,    5,   13,    1,  392,  222, 2632]],\n",
            "       device='cuda:0')\n",
            "  i, step : 1 10 incomplete_inds : []\n",
            "  i, step : 1 10 set(range(len(next_word_inds))) & set(incomplete_inds) : {0} set()\n",
            "  i, step : 1 10 complete_inds : [0]\n",
            "  i, step : 1 10 complete_seqs : [[2631, 1, 3, 117, 13, 9, 398, 2632], [2631, 1, 3, 117, 13, 1, 392, 222, 2632], [2631, 1, 2, 3, 4, 5, 13, 1, 392, 222, 2632]]\n",
            "  i, step : 1 10 complete_seqs_scores : [tensor(-5.6654, device='cuda:0', grad_fn=<SelectBackward>), tensor(-6.8259, device='cuda:0', grad_fn=<SelectBackward>), tensor(-8.5411, device='cuda:0', grad_fn=<SelectBackward>)]\n",
            "  i, step : 1 10  k before minus : 1\n",
            "  i, step : 1 10  k after minus : 0\n",
            "  i, step : 1 10 k reached zero , breaking ***\n",
            "i : 1  Max Index - j : 0\n",
            "i : 1 seq : [2631, 1, 3, 117, 13, 9, 398, 2632]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATING AT BEAM SIZE 3: 100%|██████████| 5000/5000 [03:17<00:00, 25.36it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "BLEU-4 score @ beam size of 3 is 0.2111.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsAmWzkMNmoZ",
        "outputId": "f7ffb095-b872-493a-b090-91e78e4bae80"
      },
      "source": [
        "x = torch.rand(3,4,2)\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.7775, 0.0470],\n",
              "         [0.7418, 0.5029],\n",
              "         [0.0276, 0.7762],\n",
              "         [0.8285, 0.5989]],\n",
              "\n",
              "        [[0.2478, 0.3601],\n",
              "         [0.1465, 0.4953],\n",
              "         [0.5763, 0.5703],\n",
              "         [0.2093, 0.9217]],\n",
              "\n",
              "        [[0.5986, 0.6990],\n",
              "         [0.1599, 0.9496],\n",
              "         [0.0229, 0.8896],\n",
              "         [0.6324, 0.6696]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXwazaPhtpC2"
      },
      "source": [
        "!cp '/content/BEST_checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar' '/content/gdrive/MyDrive/EVA4P2_S12_ImageCaptioning'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ppsJwhNvY-d"
      },
      "source": [
        "!cp '/content/BEST_checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pt' '/content/gdrive/MyDrive/EVA4P2_S12_ImageCaptioning'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vwW93yevlCy"
      },
      "source": [
        "!cp '/content/data_output/WORDMAP_flickr8k_5_cap_per_img_5_min_word_freq.json' '/content/gdrive/MyDrive/EVA4P2_S12_ImageCaptioning'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-erplLWnvrt7"
      },
      "source": [
        "torch.save({\n",
        "    \"encoder\": encoder.state_dict(),\n",
        "    \"decoder\": decoder.state_dict()\n",
        "}, \"TEST_checkpoint_flickr8k_5_cap_per_img_5_min_word_freq_state_dict.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXZAWJHoLWo7"
      },
      "source": [
        "encoder_script = torch.jit.script(encoder.to(\"cpu\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9W9576LLdXN"
      },
      "source": [
        "from typing import List\n",
        "\n",
        "class DecoderWithAttention2(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        attention_dim,\n",
        "        embed_dim,\n",
        "        decoder_dim,\n",
        "        vocab_size,\n",
        "        encoder_dim=2048,\n",
        "        dropout=0.5,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param attention_dim: size of attention network\n",
        "        :param embed_dim: embedding size\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param vocab_size: size of vocabulary\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param dropout: dropout\n",
        "        \"\"\"\n",
        "        super(DecoderWithAttention2, self).__init__()\n",
        "\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.attention_dim = attention_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attention = Attention(\n",
        "            encoder_dim, decoder_dim, attention_dim\n",
        "        )  # attention network\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
        "        self.dropout = nn.Dropout(p=self.dropout)\n",
        "        self.decode_step = nn.LSTMCell(\n",
        "            embed_dim + encoder_dim, decoder_dim, bias=True\n",
        "        )  # decoding LSTMCell\n",
        "        self.init_h = nn.Linear(\n",
        "            encoder_dim, decoder_dim\n",
        "        )  # linear layer to find initial hidden state of LSTMCell\n",
        "        self.init_c = nn.Linear(\n",
        "            encoder_dim, decoder_dim\n",
        "        )  # linear layer to find initial cell state of LSTMCell\n",
        "        self.f_beta = nn.Linear(\n",
        "            decoder_dim, encoder_dim\n",
        "        )  # linear layer to create a sigmoid-activated gate\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(\n",
        "            decoder_dim, vocab_size\n",
        "        )  # linear layer to find scores over vocabulary\n",
        "        self.init_weights()  # initialize some layers with the uniform distribution\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
        "        \"\"\"\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def load_pretrained_embeddings(self, embeddings):\n",
        "        \"\"\"\n",
        "        Loads embedding layer with pre-trained embeddings.\n",
        "\n",
        "        :param embeddings: pre-trained embeddings\n",
        "        \"\"\"\n",
        "        self.embedding.weight = nn.Parameter(embeddings)\n",
        "\n",
        "    def fine_tune_embeddings(self, fine_tune=True):\n",
        "        \"\"\"\n",
        "        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n",
        "\n",
        "        :param fine_tune: Allow?\n",
        "        \"\"\"\n",
        "        for p in self.embedding.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "\n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        \"\"\"\n",
        "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
        "\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :return: hidden state, cell state\n",
        "        \"\"\"\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)\n",
        "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n",
        "        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
        "        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
        "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = encoder_out.size(0)\n",
        "        encoder_dim = encoder_out.size(-1)\n",
        "        vocab_size = self.vocab_size\n",
        "\n",
        "        # Flatten image\n",
        "        encoder_out = encoder_out.view(\n",
        "            batch_size, -1, encoder_dim\n",
        "        )  # (batch_size, num_pixels, encoder_dim)\n",
        "        num_pixels = encoder_out.size(1)\n",
        "\n",
        "        # Sort input data by decreasing lengths; why? apparent below\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(\n",
        "            dim=0, descending=True\n",
        "        )\n",
        "        encoder_out = encoder_out[sort_ind]\n",
        "        encoded_captions = encoded_captions[sort_ind]\n",
        "\n",
        "        # Embedding\n",
        "        embeddings = self.embedding(\n",
        "            encoded_captions\n",
        "        )  # (batch_size, max_caption_length, embed_dim)\n",
        "\n",
        "        # Initialize LSTM state\n",
        "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
        "\n",
        "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
        "        # So, decoding lengths are actual lengths - 1\n",
        "        decode_lengths: List[int] = (caption_lengths - 1).tolist()\n",
        "\n",
        "        # Create tensors to hold word predicion scores and alphas\n",
        "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size)\n",
        "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels)\n",
        "\n",
        "        # At each time-step, decode by\n",
        "        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n",
        "        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
        "        for t in range(max(decode_lengths)):\n",
        "            batch_size_t = torch.sum(torch.tensor([l > t for l in decode_lengths])).item()\n",
        "            attention_weighted_encoding, alpha = self.attention(\n",
        "                encoder_out[:batch_size_t], h[:batch_size_t]\n",
        "            )\n",
        "            gate = self.sigmoid(\n",
        "                self.f_beta(h[:batch_size_t])\n",
        "            )  # gating scalar, (batch_size_t, encoder_dim)\n",
        "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
        "            h, c = self.decode_step(\n",
        "                torch.cat(\n",
        "                    [embeddings[:batch_size_t, t, :], attention_weighted_encoding],\n",
        "                    dim=1,\n",
        "                ),\n",
        "                (h[:batch_size_t], c[:batch_size_t]),\n",
        "            )  # (batch_size_t, decoder_dim)\n",
        "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
        "            predictions[:batch_size_t, t, :] = preds\n",
        "            alphas[:batch_size_t, t, :] = alpha\n",
        "\n",
        "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-pQIqYNLm2z"
      },
      "source": [
        "decoder2 = DecoderWithAttention2(\n",
        "        attention_dim=attention_dim,\n",
        "        embed_dim=emb_dim,\n",
        "        decoder_dim=decoder_dim,\n",
        "        vocab_size=len(word_map),\n",
        "        dropout=dropout,\n",
        "        encoder_dim=2048\n",
        "    ).to(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlRod4bkLhwF",
        "outputId": "e99c03e1-f31f-436a-a4fe-29686be59667"
      },
      "source": [
        "decoder2.load_state_dict(decoder.state_dict())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "DVdkTTdUMl6B",
        "outputId": "6d2bbde0-0eab-455d-f337-f0c124db1983"
      },
      "source": [
        "decoder2_script = torch.jit.script(decoder2.to(\"cpu\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-048e5dc17cee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecoder2_script\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/jit/__init__.py\u001b[0m in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb)\u001b[0m\n\u001b[1;32m   1259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recursive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_script_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recursive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_methods_to_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m     \u001b[0mqualified_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_qualified_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module\u001b[0;34m(nn_module, stubs_fn, share_types)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mconcrete_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcrete_type_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_script_module_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_script_module_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;31m# Compile methods if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconcrete_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconcrete_type_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethods_compiled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0mcreate_methods_from_stubs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcrete_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstubs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_emit_module_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpp_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mconcrete_type_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethods_compiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcrete_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/jit/_recursive.py\u001b[0m in \u001b[0;36mcreate_methods_from_stubs\u001b[0;34m(concrete_type, stubs)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0mrcbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolution_callback\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstubs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0mdefaults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_default_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_method\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstubs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m     \u001b[0mconcrete_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_methods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_script_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstubs_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshare_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: \n\nforward(__torch__.Attention self, Tensor encoder_out, Tensor decoder_hidden, Tensor px) -> ((Tensor, Tensor)):\nArgument px not provided.\n:\n  File \"<ipython-input-27-94f8651f3cf7>\", line 143\n        for t in range(max(decode_lengths)):\n            batch_size_t = torch.sum(torch.tensor([l > t for l in decode_lengths])).item()\n            attention_weighted_encoding, alpha = self.attention(\n                                                 ~~~~~~~~~~~~~~ <--- HERE\n                encoder_out[:batch_size_t], h[:batch_size_t]\n            )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDAkh4syLjxd"
      },
      "source": [
        "encoder_script.save(\"Test_flickr8k_caption.encoder.scripted.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "yXihxDebMcx3",
        "outputId": "f88a8b05-ed9d-4d1e-c4cb-8e7388dc77d0"
      },
      "source": [
        "decoder2_script.save(\"Test_flickr8k_caption.decoder.scripted.pt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-525ca2b6ef82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecoder2_script\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test_flickr8k_caption.decoder.scripted.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'decoder2_script' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3T86EqcMgFc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
