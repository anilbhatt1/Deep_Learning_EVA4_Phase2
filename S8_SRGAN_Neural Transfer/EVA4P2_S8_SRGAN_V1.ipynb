{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA4P2_S8_SRGAN_V1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00e4c9805a2d433a9010773c5de0d42c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f30076fad6784520bdd4ce3176cb2bf1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_33b1d5e353a1403ba53fab5dd2dd791e",
              "IPY_MODEL_91553c0a400b4ff8aa18833dc1dfa3d6"
            ]
          }
        },
        "f30076fad6784520bdd4ce3176cb2bf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "33b1d5e353a1403ba53fab5dd2dd791e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_00b2660b537c47fdb13de450469a7662",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 553433881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 553433881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ddef859f7fd647b6bb7d63dc70759653"
          }
        },
        "91553c0a400b4ff8aa18833dc1dfa3d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2589788d631b4aee8da1b2e7c986b252",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 528M/528M [01:37&lt;00:00, 5.70MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5bcd2cfb1a2f4c4892af5114ee1d2a08"
          }
        },
        "00b2660b537c47fdb13de450469a7662": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ddef859f7fd647b6bb7d63dc70759653": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2589788d631b4aee8da1b2e7c986b252": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5bcd2cfb1a2f4c4892af5114ee1d2a08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anilbhatt1/Deep_Learning_EVA4_Phase2/blob/master/EVA4P2_S8_SRGAN_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlGk2nuq4ZCr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "c7f01aa5-4094-425f-cdbe-8177bd681606"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Oct 11 07:24:41 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95gzlYvniAPz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d6039efd-b617-42e8-a9a7-fe181126da82"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYrGOUqlURIQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "155cdf13-b647-4be9-b970-d253a0307c36"
      },
      "source": [
        "!pip install torch==1.5.1+cu92 torchvision==0.6.1+cu92 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.5.1+cu92 in /usr/local/lib/python3.6/dist-packages (1.5.1+cu92)\n",
            "Requirement already satisfied: torchvision==0.6.1+cu92 in /usr/local/lib/python3.6/dist-packages (0.6.1+cu92)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu92) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu92) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu92) (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teyHqGVbqdfJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "3c85936d-e35a-4380-b678-5e0cd13de779"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import zipfile\n",
        "from zipfile import ZipFile\n",
        "from pathlib import Path\n",
        "from time import time\n",
        "from datetime import datetime \n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from tqdm import tqdm_notebook\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import pkgutil\n",
        "import importlib\n",
        "import os\n",
        "import random\n",
        "%matplotlib inline\n",
        "%config IPCompleter.greedy=True\n",
        "%reload_ext autoreload\n",
        "import io\n",
        "import skimage\n",
        "from skimage.transform import resize\n",
        "from itertools import groupby\n",
        "from tqdm.auto import tqdm\n",
        "import gc\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "print('Pytorch version:', torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "Pytorch version: 1.5.1+cu92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1hKcTcaxib-"
      },
      "source": [
        "CROP_SIZE      = 44\n",
        "UPSCALE_FACTOR = 4\n",
        "NUM_EPOCHS     = 6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTUUUfh-yfz5"
      },
      "source": [
        "from os import listdir\n",
        "from os.path import join\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision.transforms import Compose, RandomCrop, ToTensor, ToPILImage, CenterCrop, Resize\n",
        "\n",
        "def is_image_file(filename):\n",
        "    return any(filename.endswith(extension) for extension in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'])\n",
        "\n",
        "def calculate_valid_crop_size(crop_size, upscale_factor):\n",
        "    return crop_size - (crop_size % upscale_factor)\n",
        "\n",
        "def plain_transform():\n",
        "    return Compose([\n",
        "        ToTensor(),\n",
        "    ])\n",
        "\n",
        "def train_hr_transform(crop_size):\n",
        "    return Compose([\n",
        "        RandomCrop(crop_size),\n",
        "        ToTensor(),\n",
        "    ])\n",
        "\n",
        "def train_lr_transform(crop_size, upscale_factor):\n",
        "    return Compose([\n",
        "        ToPILImage(),\n",
        "        Resize(crop_size // upscale_factor, interpolation=Image.BICUBIC),\n",
        "        ToTensor()\n",
        "    ])\n",
        "\n",
        "def display_transform():\n",
        "    return Compose([\n",
        "        ToPILImage(),\n",
        "        Resize(400),\n",
        "        CenterCrop(400),\n",
        "        ToTensor()\n",
        "    ])\n",
        "\n",
        "# Images are read from directory.    \n",
        "# 44, 4 -> parameters passed to 'calculate_crop_size' function. Both parameters are configured.\n",
        "# Example : Image Input Size -> (127, 224)\n",
        "# crop_size = (44 - 44%4) = 44\n",
        "# hr_image.size -> (44,44) ->  i.e. crop size we calculated. We are cropping the hr_image based on crop size from input image.\n",
        "# lr_image.size -> 44 // 4 = 11 -> (11,11)\n",
        "# Note : Train data prep is different from validation data prep. In train data, hr_img is significantly down-sized. This is to reduce the training time.\n",
        "\n",
        "class TrainDatasetFromFolder(Dataset):\n",
        "    def __init__(self, dataset_dir, crop_size, upscale_factor):\n",
        "        super(TrainDatasetFromFolder, self).__init__()\n",
        "        self.image_filenames = [join(dataset_dir, x) for x in listdir(dataset_dir) if is_image_file(x)]\n",
        "        crop_size = calculate_valid_crop_size(crop_size, upscale_factor)\n",
        "        self.plain_transform = plain_transform()\n",
        "        self.hr_transform = train_hr_transform(crop_size)\n",
        "        self.lr_transform = train_lr_transform(crop_size, upscale_factor)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        train_image = self.plain_transform(Image.open(self.image_filenames[index]))\n",
        "        hr_image = self.hr_transform(Image.open(self.image_filenames[index]))\n",
        "        lr_image = self.lr_transform(hr_image)\n",
        "        return lr_image, hr_image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "class ValDatasetFromFolder(Dataset):\n",
        "    def __init__(self, dataset_dir, upscale_factor):\n",
        "        super(ValDatasetFromFolder, self).__init__()\n",
        "        self.upscale_factor = upscale_factor\n",
        "        self.image_filenames = [join(dataset_dir, x) for x in listdir(dataset_dir) if is_image_file(x)]\n",
        "\n",
        "# hr_image -> Original image we are giving from validation dataset. We calculate crop size & then again modify same 'hr_image' by CenterCrop from hr_image we started with.\n",
        "# lr_image -> This is reduced version of original image supplied. Dimension will be crop size//upscale factor\n",
        "# hr_restore_img -> This is merely resizing the lr_image to make it same size as hr_image. This will be the lr image considered for loss calculations\n",
        "\n",
        "# Example 1\n",
        "#   Orig Image size -> (224, 150)\n",
        "#   150, 4 -> parameters passed to 'calculate_crop_size' function i.e. minimum of original image dimension & upscale factor that we set (in this case 4)\n",
        "#   crop_size = (150 - 150%4) = 148\n",
        "#   lr_scale -> 148/4 = 37 i.e. lr_image size will (37, 37)\n",
        "#   hr_scale -> 148, so hr_image size will be (148,148) i.e. crop size we calculated\n",
        "#   hr_restore_img_size = (148, 148) because this is mere resizing of lr_image\n",
        "\n",
        "# Example 2 \n",
        "#   Orig Image size -> (224, 224)\n",
        "#   224, 4 -> parameters passed to 'calculate_crop_size' function  i.e. minimum of original image dimension & upscale factor that we set (in this case 4)\n",
        "#   crop_size = (224 - 224%4) = 224\n",
        "#   lr_scale -> 224/4 = 56 i.e. lr_image size will (56, 56)\n",
        "#   hr_scale -> 224, so hr_image size will be (224,224) i.e. crop size we calculated\n",
        "#   hr_restore_img_size = (224, 224) because this is mere resizing of lr_image\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        hr_image = Image.open(self.image_filenames[index])\n",
        "        w, h = hr_image.size\n",
        "        crop_size = calculate_valid_crop_size(min(w, h), self.upscale_factor)\n",
        "        lr_scale = Resize(crop_size // self.upscale_factor, interpolation=Image.BICUBIC)\n",
        "        hr_scale = Resize(crop_size, interpolation=Image.BICUBIC)\n",
        "        hr_image = CenterCrop(crop_size)(hr_image)\n",
        "        lr_image = lr_scale(hr_image)\n",
        "        hr_restore_img = hr_scale(lr_image)\n",
        "        return ToTensor()(lr_image), ToTensor()(hr_restore_img), ToTensor()(hr_image)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bppZLAPV0afF"
      },
      "source": [
        "train_set    = TrainDatasetFromFolder('/content/gdrive/My Drive/EVA4P2_S8_Data/train_data', crop_size=CROP_SIZE, upscale_factor=UPSCALE_FACTOR)\n",
        "val_set      = ValDatasetFromFolder('/content/gdrive/My Drive/EVA4P2_S8_Data/valid_data', upscale_factor=UPSCALE_FACTOR)\n",
        "train_loader = DataLoader(dataset=train_set, num_workers=4, batch_size=64, shuffle=True)\n",
        "val_loader   = DataLoader(dataset=val_set, num_workers=4, batch_size=1, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRUzFfdy1Le2"
      },
      "source": [
        "import math\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, scale_factor):\n",
        "        upsample_block_num = int(math.log(scale_factor, 2))\n",
        "\n",
        "        super(Generator, self).__init__()\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=9, padding=4),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "        self.block2 = ResidualBlock(64)\n",
        "        self.block3 = ResidualBlock(64)\n",
        "        self.block4 = ResidualBlock(64)\n",
        "        self.block5 = ResidualBlock(64)\n",
        "        self.block6 = ResidualBlock(64)\n",
        "        self.block7 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64)\n",
        "        )\n",
        "        block8 = [UpsampleBLock(64, 2) for _ in range(upsample_block_num)]\n",
        "        block8.append(nn.Conv2d(64, 3, kernel_size=9, padding=4))\n",
        "        self.block8 = nn.Sequential(*block8)\n",
        "\n",
        "    def forward(self, x):\n",
        "        block1 = self.block1(x)\n",
        "        block2 = self.block2(block1)\n",
        "        block3 = self.block3(block2)\n",
        "        block4 = self.block4(block3)\n",
        "        block5 = self.block5(block4)\n",
        "        block6 = self.block6(block5)\n",
        "        block7 = self.block7(block6)\n",
        "        block8 = self.block8(block1 + block7)\n",
        "\n",
        "        return (torch.tanh(block8) + 1) / 2\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(512, 1024, kernel_size=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(1024, 1, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        return torch.sigmoid(self.net(x).view(batch_size))\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(channels)\n",
        "        self.prelu = nn.PReLU()\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.conv1(x)\n",
        "        residual = self.bn1(residual)\n",
        "        residual = self.prelu(residual)\n",
        "        residual = self.conv2(residual)\n",
        "        residual = self.bn2(residual)\n",
        "\n",
        "        return x + residual\n",
        "\n",
        "class UpsampleBLock(nn.Module):\n",
        "    def __init__(self, in_channels, up_scale):\n",
        "        super(UpsampleBLock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, in_channels * up_scale ** 2, kernel_size=3, padding=1)\n",
        "        self.pixel_shuffle = nn.PixelShuffle(up_scale)\n",
        "        self.prelu = nn.PReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.pixel_shuffle(x)\n",
        "        x = self.prelu(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXvxj2YM1dl0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "04289e00-dabf-493b-c7a7-7b85b428960c"
      },
      "source": [
        "netG = Generator(UPSCALE_FACTOR)\n",
        "print('# generator parameters:', sum(param.numel() for param in netG.parameters()))\n",
        "netD = Discriminator()\n",
        "print('# discriminator parameters:', sum(param.numel() for param in netD.parameters()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# generator parameters: 734219\n",
            "# discriminator parameters: 5215425\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf_2wtmC3C__"
      },
      "source": [
        "from torchvision.models.vgg import vgg16\n",
        "\n",
        "class GeneratorLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GeneratorLoss, self).__init__()\n",
        "        vgg = vgg16(pretrained=True)\n",
        "        loss_network = nn.Sequential(*list(vgg.features)[:31]).eval()\n",
        "        for param in loss_network.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.loss_network = loss_network\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "        self.tv_loss = TVLoss()\n",
        "\n",
        "    def forward(self, out_labels, out_images, target_images):\n",
        "        # Adversarial Loss -> Calculates difference between 1 and value returned by discriminator (fake_out) after evaluating the fake img generated by generator\n",
        "        adversarial_loss = torch.mean(1 - out_labels)\n",
        "        # Perception Loss -> This is VGG loss between fake_img (sr) and real_img(hr)\n",
        "        perception_loss = self.mse_loss(self.loss_network(out_images), self.loss_network(target_images))\n",
        "        # Image Loss -> This is MSE loss between fake img and real_img\n",
        "        image_loss = self.mse_loss(out_images, target_images)\n",
        "        # TV Loss  -> Total Variation Loss\n",
        "        tv_loss = self.tv_loss(out_images)\n",
        "        return image_loss + 0.001 * adversarial_loss + 0.006 * perception_loss + 2e-8 * tv_loss\n",
        "\n",
        "# TVLoss -> Total Variation Loss\n",
        "# The total variation is the sum of the absolute differences for neighboring pixel-values in the input images. This measures how much noise is in the images.\n",
        "# TV loss is getting fake_img generated(sr_img) as input\n",
        "\n",
        "class TVLoss(nn.Module):\n",
        "    def __init__(self, tv_loss_weight=1):\n",
        "        super(TVLoss, self).__init__()\n",
        "        self.tv_loss_weight = tv_loss_weight\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size()[0]\n",
        "        h_x = x.size()[2]\n",
        "        w_x = x.size()[3]\n",
        "        count_h = self.tensor_size(x[:, :, 1:, :])                          # If sr_img size is (64, 3, 44, 44), we are passing tensor as (64,3,43,44) i.e discarding first row. This is to facilitate vertical grad calc.\n",
        "        count_w = self.tensor_size(x[:, :, :, 1:])                          # If sr_img size is (64, 3, 44, 44), we are passing tensor as (64,3,44,43) i.e discarding first column. This is to facilitate horizontal grad calc.\n",
        "        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()  # Difference of neighbouring pixel values using rows i.e. Calculating vertical gradient.(Refer EVA4-P1-S1)\n",
        "        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()  # Difference of neighbouring pixel values using columns i.e. calculation horizontal gradient.\n",
        "        return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def tensor_size(t):     \n",
        "        return t.size()[1] * t.size()[2] * t.size()[3]        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t40n2hpW3Ssx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "00e4c9805a2d433a9010773c5de0d42c",
            "f30076fad6784520bdd4ce3176cb2bf1",
            "33b1d5e353a1403ba53fab5dd2dd791e",
            "91553c0a400b4ff8aa18833dc1dfa3d6",
            "00b2660b537c47fdb13de450469a7662",
            "ddef859f7fd647b6bb7d63dc70759653",
            "2589788d631b4aee8da1b2e7c986b252",
            "5bcd2cfb1a2f4c4892af5114ee1d2a08"
          ]
        },
        "outputId": "4840ea06-9f47-46de-80de-7aaae24a1470"
      },
      "source": [
        "generator_criterion = GeneratorLoss()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/checkpoints/vgg16-397923af.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00e4c9805a2d433a9010773c5de0d42c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=553433881.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmVMdUM-3Znv"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "   netG.cuda()\n",
        "   netD.cuda()\n",
        "   generator_criterion.cuda()\n",
        "\n",
        "optimizerG = optim.Adam(netG.parameters())\n",
        "optimizerD = optim.Adam(netD.parameters())\n",
        "\n",
        "results = {'d_loss': [], 'g_loss': [], 'd_score': [], 'g_score': [], 'psnr': [], 'ssim': []}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub0p1SonxY6N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "6a2c0639-fa1b-4780-ea2b-64a829d793da"
      },
      "source": [
        "summary(netD, input_size=(3, 128, 128))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 128, 128]           1,792\n",
            "         LeakyReLU-2         [-1, 64, 128, 128]               0\n",
            "            Conv2d-3           [-1, 64, 64, 64]          36,928\n",
            "       BatchNorm2d-4           [-1, 64, 64, 64]             128\n",
            "         LeakyReLU-5           [-1, 64, 64, 64]               0\n",
            "            Conv2d-6          [-1, 128, 64, 64]          73,856\n",
            "       BatchNorm2d-7          [-1, 128, 64, 64]             256\n",
            "         LeakyReLU-8          [-1, 128, 64, 64]               0\n",
            "            Conv2d-9          [-1, 128, 32, 32]         147,584\n",
            "      BatchNorm2d-10          [-1, 128, 32, 32]             256\n",
            "        LeakyReLU-11          [-1, 128, 32, 32]               0\n",
            "           Conv2d-12          [-1, 256, 32, 32]         295,168\n",
            "      BatchNorm2d-13          [-1, 256, 32, 32]             512\n",
            "        LeakyReLU-14          [-1, 256, 32, 32]               0\n",
            "           Conv2d-15          [-1, 256, 16, 16]         590,080\n",
            "      BatchNorm2d-16          [-1, 256, 16, 16]             512\n",
            "        LeakyReLU-17          [-1, 256, 16, 16]               0\n",
            "           Conv2d-18          [-1, 512, 16, 16]       1,180,160\n",
            "      BatchNorm2d-19          [-1, 512, 16, 16]           1,024\n",
            "        LeakyReLU-20          [-1, 512, 16, 16]               0\n",
            "           Conv2d-21            [-1, 512, 8, 8]       2,359,808\n",
            "      BatchNorm2d-22            [-1, 512, 8, 8]           1,024\n",
            "        LeakyReLU-23            [-1, 512, 8, 8]               0\n",
            "AdaptiveAvgPool2d-24            [-1, 512, 1, 1]               0\n",
            "           Conv2d-25           [-1, 1024, 1, 1]         525,312\n",
            "        LeakyReLU-26           [-1, 1024, 1, 1]               0\n",
            "           Conv2d-27              [-1, 1, 1, 1]           1,025\n",
            "================================================================\n",
            "Total params: 5,215,425\n",
            "Trainable params: 5,215,425\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.19\n",
            "Forward/backward pass size (MB): 48.27\n",
            "Params size (MB): 19.90\n",
            "Estimated Total Size (MB): 68.35\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNP0WqWRxcyK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "outputId": "7c967ca7-6d06-40dc-da1e-8defbcc0e22d"
      },
      "source": [
        "summary(netG, input_size=(3, 128, 128))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 128, 128]          15,616\n",
            "             PReLU-2         [-1, 64, 128, 128]               1\n",
            "            Conv2d-3         [-1, 64, 128, 128]          36,928\n",
            "       BatchNorm2d-4         [-1, 64, 128, 128]             128\n",
            "             PReLU-5         [-1, 64, 128, 128]               1\n",
            "            Conv2d-6         [-1, 64, 128, 128]          36,928\n",
            "       BatchNorm2d-7         [-1, 64, 128, 128]             128\n",
            "     ResidualBlock-8         [-1, 64, 128, 128]               0\n",
            "            Conv2d-9         [-1, 64, 128, 128]          36,928\n",
            "      BatchNorm2d-10         [-1, 64, 128, 128]             128\n",
            "            PReLU-11         [-1, 64, 128, 128]               1\n",
            "           Conv2d-12         [-1, 64, 128, 128]          36,928\n",
            "      BatchNorm2d-13         [-1, 64, 128, 128]             128\n",
            "    ResidualBlock-14         [-1, 64, 128, 128]               0\n",
            "           Conv2d-15         [-1, 64, 128, 128]          36,928\n",
            "      BatchNorm2d-16         [-1, 64, 128, 128]             128\n",
            "            PReLU-17         [-1, 64, 128, 128]               1\n",
            "           Conv2d-18         [-1, 64, 128, 128]          36,928\n",
            "      BatchNorm2d-19         [-1, 64, 128, 128]             128\n",
            "    ResidualBlock-20         [-1, 64, 128, 128]               0\n",
            "           Conv2d-21         [-1, 64, 128, 128]          36,928\n",
            "      BatchNorm2d-22         [-1, 64, 128, 128]             128\n",
            "            PReLU-23         [-1, 64, 128, 128]               1\n",
            "           Conv2d-24         [-1, 64, 128, 128]          36,928\n",
            "      BatchNorm2d-25         [-1, 64, 128, 128]             128\n",
            "    ResidualBlock-26         [-1, 64, 128, 128]               0\n",
            "           Conv2d-27         [-1, 64, 128, 128]          36,928\n",
            "      BatchNorm2d-28         [-1, 64, 128, 128]             128\n",
            "            PReLU-29         [-1, 64, 128, 128]               1\n",
            "           Conv2d-30         [-1, 64, 128, 128]          36,928\n",
            "      BatchNorm2d-31         [-1, 64, 128, 128]             128\n",
            "    ResidualBlock-32         [-1, 64, 128, 128]               0\n",
            "           Conv2d-33         [-1, 64, 128, 128]          36,928\n",
            "      BatchNorm2d-34         [-1, 64, 128, 128]             128\n",
            "           Conv2d-35        [-1, 256, 128, 128]         147,712\n",
            "     PixelShuffle-36         [-1, 64, 256, 256]               0\n",
            "            PReLU-37         [-1, 64, 256, 256]               1\n",
            "    UpsampleBLock-38         [-1, 64, 256, 256]               0\n",
            "           Conv2d-39        [-1, 256, 256, 256]         147,712\n",
            "     PixelShuffle-40         [-1, 64, 512, 512]               0\n",
            "            PReLU-41         [-1, 64, 512, 512]               1\n",
            "    UpsampleBLock-42         [-1, 64, 512, 512]               0\n",
            "           Conv2d-43          [-1, 3, 512, 512]          15,555\n",
            "================================================================\n",
            "Total params: 734,219\n",
            "Trainable params: 734,219\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.19\n",
            "Forward/backward pass size (MB): 918.00\n",
            "Params size (MB): 2.80\n",
            "Estimated Total Size (MB): 920.99\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZaNxfJp4Dqj"
      },
      "source": [
        "from math import exp\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def gaussian(window_size, sigma):    # Creates a gaussian tensor of size 11 eg: torch.Size([11])\n",
        "    gauss = torch.Tensor([exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2)) for x in range(window_size)])\n",
        "    return gauss / gauss.sum()\n",
        "\n",
        "def create_window(window_size, channel):\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)  # Adds one more dimension to gaussian tensor - torch.Size([11, 1])\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)   # mm is matrix multiplication. Also adds 2 more dimensions - torch.Size([1, 1, 11, 11])\n",
        "\n",
        "    # expand     -> Returns a new view of the self tensor with singleton dimensions expanded to a larger size\n",
        "    # contiguous -> It is like transpose but with seperate memory\n",
        "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous()) # Changes window size as torch.Size([3, 1, 11, 11])                                                                                     \n",
        "    return window\n",
        "\n",
        "# Example used to explain comments below: img1.size (sr or Fake) -> torch.Size([1, 3, 144, 144]), img2.size (hr or GT) -> torch.Size([1, 3, 144, 144])\n",
        "\n",
        "def _ssim(img1, img2, window, window_size, channel, size_average=True):    \n",
        "    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)   # Conv2d parms: torch.Size([1, 3, 144, 144]), torch.Size([3, 1, 11, 11]), padding= 5, groups = 3\n",
        "                                                                             # conv2d returns mu1 & mu2 of size torch.Size([1, 3, 144, 144])    \n",
        "    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n",
        "\n",
        "    mu1_sq = mu1.pow(2)       # mu1_sq and mu2_sq sizes : torch.Size([1, 3, 144, 144])\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "\n",
        "    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq  # sigma1_sq.size: torch.Size([1, 3, 144, 144])\n",
        "    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq  # sigma2_sq.size: torch.Size([1, 3, 144, 144])\n",
        "    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2\n",
        "    \n",
        "    C1 = 0.01 ** 2\n",
        "    C2 = 0.03 ** 2\n",
        "\n",
        "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))  # ssim_map.size : torch.Size([1, 3, 144, 144])\n",
        "\n",
        "    if size_average:\n",
        "        return ssim_map.mean()\n",
        "    else:\n",
        "        return ssim_map.mean(1).mean(1).mean(1)\n",
        "\n",
        "class SSIM(torch.nn.Module):\n",
        "    def __init__(self, window_size=11, size_average=True):\n",
        "        super(SSIM, self).__init__()\n",
        "        self.window_size = window_size\n",
        "        self.size_average = size_average\n",
        "        self.channel = 1\n",
        "        self.window = create_window(window_size, self.channel)\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        (_, channel, _, _) = img1.size()\n",
        "\n",
        "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
        "            window = self.window\n",
        "        else:\n",
        "            window = create_window(self.window_size, channel)\n",
        "\n",
        "            if img1.is_cuda:\n",
        "                window = window.cuda(img1.get_device())\n",
        "            window = window.type_as(img1)\n",
        "\n",
        "            self.window = window\n",
        "            self.channel = channel\n",
        "\n",
        "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
        "\n",
        "def ssim(img1, img2, window_size=11, size_average=True):  # This is the function that is called while training which calls -> create_window -> _ssim (img1 is sr, img2 is hr/GT)\n",
        "    (_, channel, _, _) = img1.size()\n",
        "    window = create_window(window_size, channel)\n",
        "    if img1.is_cuda:\n",
        "        window = window.cuda(img1.get_device())\n",
        "    window = window.type_as(img1)   # type_as -> Returns this tensor cast to the type of the given tensor.\n",
        "\n",
        "    return _ssim(img1, img2, window, window_size, channel, size_average)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs2QF-fQ5jfI"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "from math import log10\n",
        "\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.utils as utils\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "#import pytorch_ssim\n",
        "#from data_utils import TrainDatasetFromFolder, ValDatasetFromFolder, display_transform\n",
        "#from loss import GeneratorLoss\n",
        "#from model import Generator, Discriminator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jSJzXQALjkg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c9812bae-1530-4ebd-fe84-fb27d2d2dda6"
      },
      "source": [
        "netD_prev_save = f'/content/gdrive/My Drive/EVA4P2_S8_Data/Weights/netD_5_20201011082115.pt'\n",
        "netD.load_state_dict(torch.load(netD_prev_save))\n",
        "netG_prev_save = f'/content/gdrive/My Drive/EVA4P2_S8_Data/Weights/netG_5_20201011082115.pt'\n",
        "netG.load_state_dict(torch.load(netG_prev_save))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly72tKxN56O9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "292b5db4-6557-430a-ecc7-baed03fb47b4"
      },
      "source": [
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "\n",
        "        running_results = {'batch_sizes': 0, 'd_loss': 0, 'g_loss': 0, 'd_score': 0, 'g_score': 0}\n",
        "        print('epoch',epoch)\n",
        "    \n",
        "        netG.train()\n",
        "        netD.train()\n",
        "        for data, target in train_loader:    # data is lr image, target is hr image. eg: lr image (64,3,11,11) & hr image (64,3,44,44)\n",
        "            g_update_first = True\n",
        "            batch_size = data.size(0)\n",
        "            running_results['batch_sizes'] += batch_size\n",
        "    \n",
        "            ############################\n",
        "            # (1) Update D network: maximize D(x)-1-D(G(z))\n",
        "            ###########################\n",
        "            real_img = Variable(target)   # We are using variable to ensure back-propagation reaches the input hr image\n",
        "            if torch.cuda.is_available():\n",
        "                real_img = real_img.cuda()\n",
        "            z = Variable(data)           # We are using variable to ensure back-propagation reaches the input lr image \n",
        "            if torch.cuda.is_available():\n",
        "                z = z.cuda()\n",
        "            fake_img = netG(z)          # lr img is the input to generator which will create an sr image\n",
        "    \n",
        "            netD.zero_grad()\n",
        "            real_out = netD(real_img).mean()\n",
        "            fake_out = netD(fake_img).mean()\n",
        "            d_loss = 1 - real_out + fake_out\n",
        "            d_loss.backward(retain_graph=True)\n",
        "            optimizerD.step()\n",
        "    \n",
        "            ############################\n",
        "            # (2) Update G network: minimize 1-D(G(z)) + Perception Loss + Image Loss + TV Loss\n",
        "            ###########################\n",
        "            netG.zero_grad()\n",
        "            ############################################################\n",
        "            ## The two lines below are added to prevent runetime error! ##\n",
        "            fake_img = netG(z)\n",
        "            fake_out = netD(fake_img).mean()\n",
        "            ############################################################\n",
        "            g_loss = generator_criterion(fake_out, fake_img, real_img)    # fake_img will be same size as real_img (i.e. hr_img used for training) eg: (64,3,44,44)\n",
        "            g_loss.backward()\n",
        "            \n",
        "            fake_img = netG(z)\n",
        "            fake_out = netD(fake_img).mean()\n",
        "            \n",
        "            \n",
        "            optimizerG.step()\n",
        "\n",
        "            # loss for current batch before optimization \n",
        "            running_results['g_loss']  += g_loss.item() * batch_size\n",
        "            running_results['d_loss']  += d_loss.item() * batch_size\n",
        "            running_results['d_score'] += real_out.item() * batch_size\n",
        "            running_results['g_score'] += fake_out.item() * batch_size\n",
        "\n",
        "        if epoch % 3 == 0:\n",
        "            loss_d  = running_results['d_loss'] / running_results['batch_sizes']\n",
        "            loss_g  = running_results['g_loss'] / running_results['batch_sizes']\n",
        "            d_score = running_results['d_score'] / running_results['batch_sizes']\n",
        "            g_score = running_results['g_score'] / running_results['batch_sizes']\n",
        "            print(f'Training ~ Epoch - {epoch}/{NUM_EPOCHS}, Loss_D: {loss_d:.4f}, Loss_G: {loss_g:.4f}, D(x) Score: {d_score:.4f}, D(G(z)) Score: {g_score:.4f}')            \n",
        "    \n",
        "        netG.eval()\n",
        "        out_path     = f'/content/gdrive/My Drive/EVA4P2_S8_Data/Results/'\n",
        "        path_name_wt = f'/content/gdrive/My Drive/EVA4P2_S8_Data/Weights/'\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            valing_results = {'mse': 0, 'ssims': 0, 'psnr': 0, 'ssim': 0, 'batch_sizes': 0}\n",
        "            val_images = []\n",
        "\n",
        "            # Image sizes of validation differs from training. Refer TrainDatasetFromFolder class for more details.\n",
        "            # val_lr -> Low resolution image, val_hr_restore -> Resized version of Low resolution image, val_hr -> Cropped version of original image\n",
        "\n",
        "            for val_lr, val_hr_restore, val_hr in val_loader:\n",
        "                batch_size = val_lr.size(0)\n",
        "                valing_results['batch_sizes'] += batch_size\n",
        "                lr = val_lr\n",
        "                hr = val_hr\n",
        "                if torch.cuda.is_available():\n",
        "                    lr = lr.cuda()\n",
        "                    hr = hr.cuda()\n",
        "                sr = netG(lr)\n",
        "        \n",
        "                batch_mse = ((sr - hr) ** 2).data.mean()\n",
        "                valing_results['mse'] += batch_mse * batch_size\n",
        "                #batch_ssim = pytorch_ssim.ssim(sr, hr).item()\n",
        "                batch_ssim = ssim(sr, hr).item()\n",
        "                valing_results['ssims'] += batch_ssim * batch_size\n",
        "                valing_results['psnr'] = 10 * log10((hr.max()**2) / (valing_results['mse'] / valing_results['batch_sizes']))\n",
        "                valing_results['ssim'] = valing_results['ssims'] / valing_results['batch_sizes']\n",
        "        \n",
        "                # This is for display, left will be having LR image (hr_restore is a resized version of LR img only)\n",
        "                #                    middle will be having HR image (this is the original validation image - ground truth)\n",
        "                #                     right will be having SR image (this is the generated img, our aim is to make this better than GT)\n",
        "                val_images.extend(\n",
        "                    [display_transform()(val_hr_restore.squeeze(0)), display_transform()(hr.data.cpu().squeeze(0)),\n",
        "                     display_transform()(sr.data.cpu().squeeze(0))])\n",
        "            \n",
        "            val_images = torch.stack(val_images)                             # Concatenates sequence of tensors. All tensors need to be of the same size. \n",
        "                                                                             # eg: If val dataset has 20 images, then [60, 3, 400, 400] i.e. 20 + 20 + 20 = 60\n",
        "            val_images = torch.chunk(val_images, val_images.size(0) // 15)   # Splits a tensor into a specific number of chunks. \n",
        "                                                                             # We are displaying 5 images, 5 rows - each row having LR, HR, SR\n",
        "            \n",
        "            if epoch % 5 == 0:\n",
        "                psnr_val = valing_results['psnr']\n",
        "                ssim_val = valing_results['ssim']\n",
        "                print(f'Validation ~ Epoch - {epoch}/{NUM_EPOCHS}, PSNR: {psnr_val:.4f}, SSIM: {ssim_val:.4f}')\n",
        "\n",
        "                for image in val_images:\n",
        "                    t = datetime.now()\n",
        "                    time_stamp = t.strftime(\"%Y\")+t.strftime(\"%m\")+t.strftime(\"%d\")+t.strftime(\"%H\")+t.strftime(\"%M\")+t.strftime(\"%S\")                \n",
        "                    image = utils.make_grid(image, nrow=3, padding=5)\n",
        "                    #utils.save_image(image, out_path + 'epoch_%d_index_%d.png' % (epoch, index), padding=5)\n",
        "                    utils.save_image(image, f'{out_path}SRGAN_{epoch}_{time_stamp}.png', padding=5)\n",
        "    \n",
        "        # save model parameters\n",
        "        #torch.save(netG.state_dict(), 'epochs/netG_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))\n",
        "        #torch.save(netD.state_dict(), 'epochs/netD_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))\n",
        "        # save loss\\scores\\psnr\\ssim\n",
        "        results['d_loss'].append(running_results['d_loss'] / running_results['batch_sizes'])\n",
        "        results['g_loss'].append(running_results['g_loss'] / running_results['batch_sizes'])\n",
        "        results['d_score'].append(running_results['d_score'] / running_results['batch_sizes'])\n",
        "        results['g_score'].append(running_results['g_score'] / running_results['batch_sizes'])\n",
        "        results['psnr'].append(valing_results['psnr'])\n",
        "        results['ssim'].append(valing_results['ssim'])\n",
        "    \n",
        "        if epoch % 10 == 0:\n",
        "            out_path = f'/content/gdrive/My Drive/EVA4P2_S8_Data/Statistics/'\n",
        "            data_frame = pd.DataFrame(\n",
        "                data={'Loss_D': results['d_loss'], 'Loss_G': results['g_loss'], 'Score_D': results['d_score'],\n",
        "                      'Score_G': results['g_score'], 'PSNR': results['psnr'], 'SSIM': results['ssim']})\n",
        "                #index=range(1, epoch + 1))\n",
        "            data_frame.to_csv(out_path + 'srf_' + str(UPSCALE_FACTOR) + '_train_results.csv', index_label='Epoch')\n",
        "\n",
        "        ### Keep the model in Gpu & Save the model values in intermittent epochs\n",
        "        if epoch % 5 == 0 or epoch == (NUM_EPOCHS-1):\n",
        "            t = datetime.now()\n",
        "            time_stamp = t.strftime(\"%Y\")+t.strftime(\"%m\")+t.strftime(\"%d\")+t.strftime(\"%H\")+t.strftime(\"%M\")+t.strftime(\"%S\")         \n",
        "            torch.save(netG.state_dict(),f'{path_name_wt}netG_{epoch}_{time_stamp}.pt')\n",
        "            torch.save(netD.state_dict(),f'{path_name_wt}netD_{epoch}_{time_stamp}.pt')                              \n",
        "            print(f'GPU models saved in epoch {epoch}/{NUM_EPOCHS}')\n",
        "\n",
        "        ### Convert the model to CPU & save the model values on final epoch    \n",
        "        if epoch == (NUM_EPOCHS-1):              \n",
        "            t = datetime.now()\n",
        "            time_stamp = t.strftime(\"%Y\")+t.strftime(\"%m\")+t.strftime(\"%d\")+t.strftime(\"%H\")+t.strftime(\"%M\")+t.strftime(\"%S\")\n",
        "            netG.eval()\n",
        "            netD.eval()\n",
        "            netG.to('cpu')\n",
        "            netD.to('cpu')\n",
        "            traced_netD = torch.jit.trace(netD,torch.randn(1,3,128,128))      \n",
        "            traced_netD.save(f'{path_name_wt}netD_CPU_{epoch}_{time_stamp}.pt')\n",
        "            traced_netG = torch.jit.trace(netG,torch.randn(1,3,128,128))      \n",
        "            traced_netG.save(f'{path_name_wt}netG_CPU_{epoch}_{time_stamp}.pt')             \n",
        "            print(f' **** CPU models Saved in epoch:{epoch+1}/{NUM_EPOCHS}')\n",
        "            netG.cuda()\n",
        "            netD.cuda()     "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1\n",
            "epoch 2\n",
            "epoch 3\n",
            "Training ~ Epoch - 3/6, Loss_D: 1.0074, Loss_G: 0.0046, D(x) Score: 0.6409, D(G(z)) Score: 0.6465\n",
            "epoch 4\n",
            "epoch 5\n",
            "Validation ~ Epoch - 5/6, PSNR: 25.4787, SSIM: 0.8367\n",
            "GPU models saved in epoch 5/6\n",
            " **** CPU models Saved in epoch:6/6\n",
            "epoch 6\n",
            "Training ~ Epoch - 6/6, Loss_D: 1.0037, Loss_G: 0.0043, D(x) Score: 0.8359, D(G(z)) Score: 0.8416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzmNlmjonxBr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
