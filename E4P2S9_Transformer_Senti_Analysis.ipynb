{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "E4P2S9_Transformer_Senti_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNfoCLYqbHMJVxShjtGpz+3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "616e94fff3aa45c58ec7f8b0f682902a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0542f4f9464248829e50bc06ee711350",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f91fb2c489bb49ebb31bd4538b8e860d",
              "IPY_MODEL_d35210521e064fe5acdf4c9d7f444750"
            ]
          }
        },
        "0542f4f9464248829e50bc06ee711350": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f91fb2c489bb49ebb31bd4538b8e860d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a53206958ed04b05875c8db311e4f68c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fdf5d1a9c0af40a793568378a254c46f"
          }
        },
        "d35210521e064fe5acdf4c9d7f444750": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1a5438b429054c85941c50942c0c9495",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 301kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9951d894f8c2480ca8a81a4bed454ee5"
          }
        },
        "a53206958ed04b05875c8db311e4f68c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fdf5d1a9c0af40a793568378a254c46f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1a5438b429054c85941c50942c0c9495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9951d894f8c2480ca8a81a4bed454ee5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "626d9488d2ff403aa7b784cbfc3a72ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_374e20761abf4f77bc44ff4206d014ee",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_27e8f871f8214bce8c8f88361f28deb5",
              "IPY_MODEL_9f4fe23050c144e6bc44c3f8c1f9d92f"
            ]
          }
        },
        "374e20761abf4f77bc44ff4206d014ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "27e8f871f8214bce8c8f88361f28deb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d26df2ab20284ac7a76c51aad4b6030e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ea17ebb854b949b6932dd942f5737b5e"
          }
        },
        "9f4fe23050c144e6bc44c3f8c1f9d92f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_46f702b96659497f9e7d56656d0ca21b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 531B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b64d83a33d2b4ec8a7affce25902144f"
          }
        },
        "d26df2ab20284ac7a76c51aad4b6030e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ea17ebb854b949b6932dd942f5737b5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "46f702b96659497f9e7d56656d0ca21b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b64d83a33d2b4ec8a7affce25902144f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f963bc76ae5d461ea7335e5e9c531ea6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ace4796f7cf14700a38e5875baa72334",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b02baf7cd0034cc1bbff3b51a32dfd63",
              "IPY_MODEL_939afcb88bbe4b01b5b82a7a2b0ae455"
            ]
          }
        },
        "ace4796f7cf14700a38e5875baa72334": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b02baf7cd0034cc1bbff3b51a32dfd63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d8177289533e422b8b28c1900874538f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c20611bf11044d1c8c4a81843ecc3279"
          }
        },
        "939afcb88bbe4b01b5b82a7a2b0ae455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2a239557ac7e43c18424ed302f0bf0eb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:09&lt;00:00, 45.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5eadb1662b4c4a97bfd4989b7a6d0f27"
          }
        },
        "d8177289533e422b8b28c1900874538f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c20611bf11044d1c8c4a81843ecc3279": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2a239557ac7e43c18424ed302f0bf0eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5eadb1662b4c4a97bfd4989b7a6d0f27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anilbhatt1/Deep_Learning_EVA4_Phase2/blob/master/E4P2S9_Transformer_Senti_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZfgro3lIRIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49b90735-7fa9-480a-871b-a2a5271e4a20"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Nov 25 11:38:11 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P8    29W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRY7_5mnWrdS"
      },
      "source": [
        "# Source : https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/6%20-%20Transformers%20for%20Sentiment%20Analysis.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xms0-m7HHClI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31ccade2-d636-4884-d8c5-22d0a15e89fc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gzqkl_z_Gzst",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97ba9067-42a2-4c10-f182-20dc84561a49"
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "!pip install transformers\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)                      # We are using seed to ensure that we get similar data while splitting train & test data\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.3MB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 38.1MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.9MB 45.1MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 43.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=c876e764d88252c130998bcc35927bfb9cf10da03fd706c4723e169e0b38e486\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy39UkEpMkA6"
      },
      "source": [
        "#### We are using transformers here. Transformers are already trained with specific vocabulary. Hence we also need to train with exact vocabulary and tokenization for our IMDB example. Fortunately, each transformer comes with its own tokenization method & these tokenization are available for us to use. We are using BERT model here and its tokenization is 'BertTokenizer' which is available for us to use. Hence, we will use BertTokenizer as our tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6DENjQDNRML",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "616e94fff3aa45c58ec7f8b0f682902a",
            "0542f4f9464248829e50bc06ee711350",
            "f91fb2c489bb49ebb31bd4538b8e860d",
            "d35210521e064fe5acdf4c9d7f444750",
            "a53206958ed04b05875c8db311e4f68c",
            "fdf5d1a9c0af40a793568378a254c46f",
            "1a5438b429054c85941c50942c0c9495",
            "9951d894f8c2480ca8a81a4bed454ee5"
          ]
        },
        "outputId": "46b6a1a5-28df-4737-e6ef-e92f3ecaa19e"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  #BERT ignores casing i.e. everything is lower-case. This is specified by 'bert-base-uncased'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "616e94fff3aa45c58ec7f8b0f682902a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLL1u28qN5Uz"
      },
      "source": [
        "##### We will be using the vocabulary present in BERT. We can understand how many unique words are present in this vocabulary as below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UL0UnSzOFLy",
        "outputId": "b06aab09-bd0b-42d9-986a-e3f82bebe057"
      },
      "source": [
        "len(tokenizer.vocab)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30522"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFa2KxffOkv7"
      },
      "source": [
        "#### Example of tokenization using BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg7ZzFUAOooa",
        "outputId": "d839a49f-e79d-4ef1-b9ea-998758276a45"
      },
      "source": [
        "tokens = tokenizer.tokenize(\"Hey How are You, Man  ?\")\n",
        "tokens"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hey', 'how', 'are', 'you', ',', 'man', '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqxfisdQO7cS"
      },
      "source": [
        "#### Example of numericalizing the tokens (i.e. converting tokens into indexes present in vocabulary) using BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eBMRRCGPGKC",
        "outputId": "32568581-ac8e-43a8-f989-8ab7e27c1ed1"
      },
      "source": [
        "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
        "indexes"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4931, 2129, 2024, 2017, 1010, 2158, 1029]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-x_uJrlS8Od"
      },
      "source": [
        "##### Transformers are trained with special tokens at begining and end, also with pad and unknown token. We can get the index of them as shown below. Please note that we will use these indexes later while field definition ('txt')."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwulwO3STSss",
        "outputId": "86983b1e-0a82-4d09-d81f-1e2da1a6c12b"
      },
      "source": [
        "init_token = tokenizer.cls_token\n",
        "eos_token  = tokenizer.sep_token\n",
        "pad_token  = tokenizer.pad_token\n",
        "unk_token  = tokenizer.unk_token\n",
        "print(init_token, eos_token, pad_token, unk_token)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] [SEP] [PAD] [UNK]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1CoaFb8Tw98",
        "outputId": "a077031b-96f6-4761-828f-49970a197fc6"
      },
      "source": [
        "init_token_idx = tokenizer.cls_token_id\n",
        "eos_token_idx  = tokenizer.sep_token_id\n",
        "pad_token_idx  = tokenizer.pad_token_id\n",
        "unk_token_idx  = tokenizer.unk_token_id\n",
        "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101 102 0 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocvHGOKtVguF"
      },
      "source": [
        "#### Transformers are trained for fixed length sequences. It cant handle sequences more than this fixed length. In this example with BERT 512 is the fixed length. Also, previosuly we used 'spacy' for tokenizing, but in this case we will define a function for the same which will also manage to keep the token size within 512 limit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3ThChhkVSzW",
        "outputId": "179430a6-f7ae-4201-8a9c-c89b93fa77fa"
      },
      "source": [
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
        "print(max_input_length)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o25etcU9WJ1o"
      },
      "source": [
        "def tokenize_and_cut(sentence):\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    tokens = tokens[:max_input_length - 2]  # -2 to give room for special tokens at begining and end\n",
        "    return tokens"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpJXwMCUZjIM"
      },
      "source": [
        "#### Defining fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi73zUWuMb5z"
      },
      "source": [
        "from torchtext import data\n",
        "\n",
        "txt = data.Field(batch_first = True,  # Transformers need batch dimension to come first\n",
        "                 use_vocab = False,   # This is to tell torchtext that vocabulary will be taken care by ourselves  i.e we wont do txt.build_vocab    \n",
        "                 tokenize = tokenize_and_cut, # Using our customized tokenization function instead of 'spacy'\n",
        "                 preprocessing = tokenizer.convert_tokens_to_ids, # For numericalization of tokens\n",
        "                 init_token = init_token_idx, # Defining special tokens interms of their index values (bcoz this line comes after numericalization)\n",
        "                 eos_token  = eos_token_idx,\n",
        "                 pad_token  = pad_token_idx,\n",
        "                 unk_token  = unk_token_idx)\n",
        "\n",
        "lbl = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jHe24iKZnrb"
      },
      "source": [
        "#### Creating train and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0gVNoP_HAgj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08918608-98da-4ac1-8e43-067f53f7a182"
      },
      "source": [
        "from torchtext import datasets\n",
        "import random\n",
        "train_data, test_data = datasets.IMDB.splits(txt, lbl)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84.1M/84.1M [00:07<00:00, 11.2MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAZOiCWjZkfY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b2c4767-6f49-4f56-c553-78ccc3ca600b"
      },
      "source": [
        "print('Length of train_data:',len(train_data), 'Type:', type(train_data))\n",
        "print('Length of test_data :',len(test_data), 'Type:', type(test_data))\n",
        "print(train_data.fields)\n",
        "print(test_data.fields)\n",
        "print(vars(train_data.examples[0]))   # vars -> Built-in function, with an argument equivalent to object.dict. \n",
        "print(vars(test_data.examples[0])) \n",
        "print(vars(train_data[-1]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of train_data: 25000 Type: torchtext.datasets.imdb.IMDB\n",
            "Length of test_data : 25000 Type: torchtext.datasets.imdb.IMDB\n",
            "{'text': <torchtext.data.field.Field object at 0x7f2553021c88>, 'label': <torchtext.data.field.LabelField object at 0x7f25530219e8>}\n",
            "{'text': <torchtext.data.field.Field object at 0x7f2553021c88>, 'label': <torchtext.data.field.LabelField object at 0x7f25530219e8>}\n",
            "{'text': [1045, 4635, 2023, 1996, 2190, 1997, 1996, 1062, 29459, 3127, 13068, 2015, 1012, 1996, 10990, 3315, 3556, 9909, 8595, 2000, 2019, 10990, 3898, 2377, 1012, 2045, 2003, 2019, 6581, 4637, 3459, 1998, 6547, 12700, 2008, 2097, 2562, 2017, 16986, 2127, 1996, 2345, 3127, 1012, 7305, 21681, 2515, 1037, 2986, 3105, 2004, 2123, 5277, 1998, 2010, 11477, 13059, 1062, 29459, 1012, 2197, 1010, 2021, 5121, 2025, 2560, 1010, 2003, 1996, 2307, 9855, 2136, 1997, 9809, 1998, 2394, 1012], 'label': 'pos'}\n",
            "{'text': [2798, 1000, 9610, 2278, 1000, 4341, 2003, 7078, 27547, 2004, 1996, 7082, 2266, 1997, 1996, 7873, 2155, 5627, 2000, 19919, 2114, 1037, 20067, 2027, 2387, 4028, 1037, 14460, 1998, 2019, 28694, 1012, 2002, 4061, 2012, 7087, 2448, 1999, 1996, 2942, 2162, 1998, 2010, 16419, 2964, 3216, 2152, 1010, 2130, 2044, 2010, 2365, 1011, 1999, 1011, 2375, 2003, 7854, 1998, 2028, 1997, 2010, 7631, 1005, 1055, 2003, 11364, 2011, 1996, 6080, 1010, 24439, 2035, 1996, 2060, 2372, 1012, 3571, 1997, 2010, 7631, 1005, 1055, 2331, 2003, 2053, 8016, 1010, 2002, 2758, 1012, 2002, 2876, 1005, 1056, 2215, 2010, 7631, 2542, 1999, 1037, 2406, 2448, 2011, 20067, 2015, 4312, 1012, 1996, 4736, 2090, 8388, 4611, 1998, 3167, 3808, 2003, 5533, 2188, 9249, 1999, 2023, 7436, 1011, 4222, 2466, 1012, 4787, 15876, 7106, 2003, 2036, 1037, 3233, 5833, 2004, 1996, 2524, 4439, 2212, 4905, 8701, 1996, 2155, 2007, 2566, 9103, 2854, 2065, 2027, 2123, 1005, 1056, 2067, 2039, 2037, 8720, 1997, 1996, 6359, 1999, 2457, 1012, 1996, 2717, 1997, 1996, 3459, 1010, 2164, 1996, 22889, 5243, 9096, 6359, 1010, 6798, 4297, 2063, 1010, 2024, 2035, 6581, 1010, 1998, 1996, 2143, 2003, 10245, 8197, 2135, 2856, 2011, 2520, 1037, 1012, 2092, 2386, 1012, 2045, 1005, 1055, 2036, 2204, 23873, 1010, 2004, 4341, 17144, 2074, 2004, 1996, 3979, 2003, 2055, 2000, 4088, 1012], 'label': 'pos'}\n",
            "{'text': [1996, 5436, 2001, 21425, 1010, 1998, 3554, 2007, 4409, 4152, 2214, 1010, 2021, 2023, 2003, 1037, 13366, 14776, 3185, 2000, 2298, 2012, 2065, 2017, 2031, 1037, 2659, 26264, 1998, 2123, 1005, 1056, 2428, 2729, 2055, 2613, 5691, 1012, 1045, 2052, 2203, 5313, 3351, 1999, 2995, 2396, 5691, 1010, 2066, 1005, 26706, 1005, 1010, 1005, 2242, 2055, 2984, 1005, 1010, 1005, 3449, 3814, 5428, 1005, 1010, 2030, 1005, 2474, 11937, 4226, 4360, 1005, 1012], 'label': 'neg'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMfyfIBqcM6b"
      },
      "source": [
        "#### Unlike in previous examples, here data is organized in terms of indexes. If we wish to see them as words we can use tokenizer.convert_ids_to_tokens as shown below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTY5NjlpcrZ7",
        "outputId": "3761dbc0-47a5-4fbf-84ef-78afa4cdc77e"
      },
      "source": [
        "example = tokenizer.convert_ids_to_tokens(vars(train_data.examples[0])['text'])\n",
        "\n",
        "print(example)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'rank', 'this', 'the', 'best', 'of', 'the', 'z', '##orro', 'chapter', '##play', '##s', '.', 'the', 'exciting', 'musical', 'score', 'adds', 'punch', 'to', 'an', 'exciting', 'screen', 'play', '.', 'there', 'is', 'an', 'excellent', 'supporting', 'cast', 'and', 'mystery', 'villain', 'that', 'will', 'keep', 'you', 'guessing', 'until', 'the', 'final', 'chapter', '.', 'reed', 'hadley', 'does', 'a', 'fine', 'job', 'as', 'don', 'diego', 'and', 'his', 'alter', 'ego', 'z', '##orro', '.', 'last', ',', 'but', 'certainly', 'not', 'least', ',', 'is', 'the', 'great', 'directing', 'team', 'of', 'whitney', 'and', 'english', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMXj2X1ucgy0"
      },
      "source": [
        "##### Split the train_data further into train_data & valid_data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaHsrWT5Zp7H"
      },
      "source": [
        "import random\n",
        "train_data, valid_data = train_data.split(split_ratio=0.8,random_state=random.seed(SEED))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1O4-C2laOjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45da3b2b-05f5-4fc8-eaeb-1e04bd7e1d24"
      },
      "source": [
        "print('Length of train_data:',len(train_data))\n",
        "print('Length of valid_data:',len(valid_data))\n",
        "print('Length of test_data:',len(test_data))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of train_data: 20000\n",
            "Length of valid_data: 5000\n",
            "Length of test_data: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft0wEgy-enP_"
      },
      "source": [
        "### Building vocabulary. We already build txt vocab early ie we used whatever is used for training BERT as is the norm. However, we still need to build the vocab for lbl as below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG03sdvzcF0t"
      },
      "source": [
        "lbl.build_vocab(train_data)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1QuekVUgB8X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11e3cda5-3e9f-471d-c73c-1af5656d69bc"
      },
      "source": [
        "print('Unique words in lbl:',len(lbl.vocab))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words in lbl: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Krw_X-IdlkVA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8cf81b3-bda3-44e7-e378-9f46f2ad14bf"
      },
      "source": [
        "print(lbl.vocab.stoi)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<function _default_unk_index at 0x7f2591708158>, {'neg': 0, 'pos': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIT5qqb-nlfR"
      },
      "source": [
        "#### Creating iterator. Using buckeiterator that will return batch of examples where each example is of similar length, minimizing the amount of padding per example. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lepEYL-mFj1"
      },
      "source": [
        "batch_size = 128\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train_data, valid_data, test_data), \n",
        "                                                                            batch_size = batch_size,\n",
        "                                                                            device = device)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYdDMiCrdv5-"
      },
      "source": [
        "#### Building the model. We'll load the pre-trained model, making sure to load the same model as we did for tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "626d9488d2ff403aa7b784cbfc3a72ef",
            "374e20761abf4f77bc44ff4206d014ee",
            "27e8f871f8214bce8c8f88361f28deb5",
            "9f4fe23050c144e6bc44c3f8c1f9d92f",
            "d26df2ab20284ac7a76c51aad4b6030e",
            "ea17ebb854b949b6932dd942f5737b5e",
            "46f702b96659497f9e7d56656d0ca21b",
            "b64d83a33d2b4ec8a7affce25902144f",
            "f963bc76ae5d461ea7335e5e9c531ea6",
            "ace4796f7cf14700a38e5875baa72334",
            "b02baf7cd0034cc1bbff3b51a32dfd63",
            "939afcb88bbe4b01b5b82a7a2b0ae455",
            "d8177289533e422b8b28c1900874538f",
            "c20611bf11044d1c8c4a81843ecc3279",
            "2a239557ac7e43c18424ed302f0bf0eb",
            "5eadb1662b4c4a97bfd4989b7a6d0f27"
          ]
        },
        "id": "lpoLjrBNd8oN",
        "outputId": "054ec5e9-c78c-480c-da9a-7502e2909f3b"
      },
      "source": [
        "from transformers import BertModel\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "626d9488d2ff403aa7b784cbfc3a72ef",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f963bc76ae5d461ea7335e5e9c531ea6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1w6mPaNmMFj"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BERTGRUSentiment(nn.Module):\n",
        "     def __init__(self, bert, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
        "         super().__init__()\n",
        "         self.bert     = bert\n",
        "         embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "         self.rnn      = nn.GRU(embedding_dim,\n",
        "                                hidden_dim,\n",
        "                                num_layers = n_layers,\n",
        "                                bidirectional = bidirectional,\n",
        "                                batch_first = True,\n",
        "                                dropout = 0 if n_layers < 2 else dropout)\n",
        "         self.out     = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "         self.dropout = nn.Dropout(dropout)\n",
        "     \n",
        "     def forward(self, text):  \n",
        "         #text = [batch size, sentence len] --> eg: [64, 1150] means 64 sentences with 1150 words each. Length of sentence will vary for each batch.\n",
        "         #we gave 'batch_first = True' while data pre-processing, hence comes with batch_dimension first\n",
        "         with torch.no_grad():\n",
        "             embedded = self.bert(text)[0]\n",
        "         #embedded = [batch size, sentence len, emb dim] --> [64, 1150, 100] adding one more dimension for embed dimension\n",
        "\n",
        "         _, hidden = self.rnn(embedded)\n",
        "         #hidden = [n layers * n directions, batch size, emb dim] \n",
        "\n",
        "         if self.rnn.bidirectional:\n",
        "             hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "         else:\n",
        "             hidden = self.dropout(hidden[-1,:,:])\n",
        "         #hidden = [batch size, hid dim]        \n",
        "\n",
        "         output = self.out(hidden)\n",
        "         #output = [batch size, out dim]\n",
        "         \n",
        "         return output "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFy2JQ7K-tqM"
      },
      "source": [
        "hidden_dim    = 256\n",
        "output_dim    = 1            # pos(1) or neg(0) for sentiment analysis \n",
        "n_layers      = 2\n",
        "bidirectional = True\n",
        "dropout       = 0.25\n",
        "\n",
        "model = BERTGRUSentiment(bert, hidden_dim, output_dim, n_layers, bidirectional, dropout)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhnP4fLmCDNE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f60fa16-085d-4141-df80-c3f7c4c07555"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 112,241,409 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q_rIXhMhPjQ"
      },
      "source": [
        "for name, param in model.named_parameters(): \n",
        "    if name.startswith('bert'):\n",
        "        param.requires_grad = False"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gY3aZEMhcak",
        "outputId": "d592f225-3594-461e-aa6e-7f0b7bb920ce"
      },
      "source": [
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 2,759,169 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bmZFDrcifYM",
        "outputId": "32735fcc-c4a4-4fc4-a4f7-0565f5b7067a"
      },
      "source": [
        "for name, param in model.named_parameters(): \n",
        "    if param.requires_grad:\n",
        "        print(name)   "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rnn.weight_ih_l0\n",
            "rnn.weight_hh_l0\n",
            "rnn.bias_ih_l0\n",
            "rnn.bias_hh_l0\n",
            "rnn.weight_ih_l0_reverse\n",
            "rnn.weight_hh_l0_reverse\n",
            "rnn.bias_ih_l0_reverse\n",
            "rnn.bias_hh_l0_reverse\n",
            "rnn.weight_ih_l1\n",
            "rnn.weight_hh_l1\n",
            "rnn.bias_ih_l1\n",
            "rnn.bias_hh_l1\n",
            "rnn.weight_ih_l1_reverse\n",
            "rnn.weight_hh_l1_reverse\n",
            "rnn.bias_ih_l1_reverse\n",
            "rnn.bias_hh_l1_reverse\n",
            "out.weight\n",
            "out.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UljmhyjpN76a"
      },
      "source": [
        "##### SGD updates all parameters with the same learning rate and choosing this learning rate can be tricky. Adam adapts the learning rate for each parameter, giving parameters that are updated more frequently lower learning rates and parameters that are updated infrequently higher learning rates. Hence using Adam in this example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w48qWZBmCP3Z"
      },
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szGGXaLPCeeA"
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86KjUqMACyWs"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc  = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for idx, batch in enumerate(iterator):       \n",
        "        optimizer.zero_grad()      \n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        acc  = binary_accuracy(predictions, batch.label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc  += acc.item()    \n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)   "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L92GvYX0FAQk"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc  = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(iterator):      \n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            acc  = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc  += acc.item() \n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)      "
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjElrmOwFmNx"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8vyUS5SF1po",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed721507-30c9-4858-a484-3b44ce24b7c5"
      },
      "source": [
        "n_epochs = 5\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    \n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), '/content/gdrive/My Drive/EVA4P2_S9/E4P2_S9_Transformer_Senti_Analysis.pt')\n",
        "       \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 17m 29s\n",
            "\tTrain Loss: 0.264 | Train Acc: 89.29%\n",
            "\t Val. Loss: 0.241 |  Val. Acc: 90.98%\n",
            "Epoch: 02 | Epoch Time: 17m 29s\n",
            "\tTrain Loss: 0.231 | Train Acc: 90.85%\n",
            "\t Val. Loss: 0.200 |  Val. Acc: 92.50%\n",
            "Epoch: 03 | Epoch Time: 17m 30s\n",
            "\tTrain Loss: 0.205 | Train Acc: 91.96%\n",
            "\t Val. Loss: 0.194 |  Val. Acc: 92.52%\n",
            "Epoch: 04 | Epoch Time: 17m 26s\n",
            "\tTrain Loss: 0.177 | Train Acc: 93.32%\n",
            "\t Val. Loss: 0.246 |  Val. Acc: 90.08%\n",
            "Epoch: 05 | Epoch Time: 17m 27s\n",
            "\tTrain Loss: 0.151 | Train Acc: 94.19%\n",
            "\t Val. Loss: 0.220 |  Val. Acc: 91.97%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMr0n_TtHi4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c68e8a8b-96da-4f0a-a0ca-32423654b87d"
      },
      "source": [
        "model.load_state_dict(torch.load('/content/gdrive/My Drive/EVA4P2_S9/E4P2_S9_Transformer_Senti_Analysis.pt'))\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.192 | Test Acc: 92.22%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAODnTYV1SfQ"
      },
      "source": [
        "##### Giving user input to get the sentiment back. Please note that we trained on movie review comments, hence input also should be similar text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwMKdilKafhF"
      },
      "source": [
        "def predict_sentiment(model, tokenizer, sentence): \n",
        "    model.eval()\n",
        "    tokens  = tokenizer.tokenize(sentence)\n",
        "    tokens  = tokens[:max_input_length - 2]\n",
        "    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
        "    tensor  = torch.LongTensor(indexed).to(device)  # converts 'indexed' which is a Python list into a PyTorch tensor\n",
        "    tensor  = tensor.unsqueeze(0)                   # adding batch dimension to feed it to GPU\n",
        "    preds   = torch.sigmoid(model(tensor))\n",
        "    return preds.item()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fA0GgPD3zot",
        "outputId": "24029be4-ac32-419b-d8a4-213638be2540"
      },
      "source": [
        "predict_sentiment(model, tokenizer, \"This film is terrible\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03443735092878342"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YubQTSRUxma"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}